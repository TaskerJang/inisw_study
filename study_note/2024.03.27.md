## 1. zotero => download citation => 논문 관리 => logsiq과 연동 가능

## 2. 연표 만들기 => 논문 관리

## 3. reshaping data => 분석하기 어려운 데이터를 분석하기 용이하게 바꾸기

## 4. Tidy Data => 분석에 적합한 데이터!

## 5. csv 파일은 스키마가 없다 => 불편함이 매우 크다.

## 6. molten data => 쉽게 데이터의 값을 수정 가능한 상태로 만들기 => melt 명령어 사용하기!

## 7. 미싱데이터 처리 => melt(data.columns[:7])=> 의미있는 칼럼 고정 시키기!

## 8. dropna => 미싱데이터 칼럼 삭제!

## 9. var_name & value_name => 칼럼 명과 값을 바꾸기 가능!

## 10. extract => pat 

## 11. 정규식으로 추출하기 => 매우 편리

## 12. 3차원 데이터의 2차원화 => 정규표현식 사용!

## 13. 입력이 되어 있지 않은 데이터 처리하기 => 정규표현식으로 정리 => 매우 편리

## 14. stack과 unstack => set_index => 칼럼 변경하기

## 15. stack , unstack => 인덱스가 2개 이상인 곳에서 사용 가능 => multi-index

## 16. unstack(숫자) => 어떤 것을 칼럼으로 바꿀 것인지 인덱스(숫자) 선택 가능

## 17. stack => 칼럼을 인덱스로 바꿔준다.

## 18. aggregation 테크닉

## 19. split-apply-combine 

## 20. split 과 group의 차이!

## 21. r의 apply는 map과 같이 뛰어난 성능을 발휘한다.

## 22. plot.bar(stacked= true)=> stack 그래프 생성

## 23. .transform(lambda x:x+1) => 값을 1씩 추가해주기

## 24. 입력(input)받아서 동적으로 테이블 통계치를 생성하는 것도 가능하다.

```python

Preprocessing;전처리
Tokenizing => Tokens={음절,어절,단어,형태소,어근,접사,어간,어미,구,...,ngram,품사...}
           => 왜? feature, 한정된 자원, 한정된 데이터, 한정된 모델 세상을 이해해야 함(모델링)
Zipf => 빈도의 순으로 나열 = 순위의 역순이랑 동일, 가장 많이 나온 단어는 그 다음 단어의 두배
        고빈도단어(문장부호,접사,조사,...); 상위 n개의 단어가 대부분 차지
        저빈도단어(고유명사,신조어,비속어,오탈자,...); 토큰 후보들 중 대부분을 차지하는 너무 많음
Heaps => 문서에 나타난 고유한 단어들은 전체 단어의 수와 일정한 관계

Tokenizing 기법
NLTK -> sent_tokenize, word_tokenize => regex_tokenize, TweetTokenize
한글 -> Morpheme Analyzer, POS, nouns, ... 품사표(tagset)
       => key 못찾음; Out Of Vocaburaly(OOV) = [UNK]
Entropy, Perplexity(Cohesion) => 조건부확률 => 어간
Stemming(Stem;어간); 어간+어미; 있다|있었다|있었고|있었으며.... 있+_______
                    ed, y, ies, s, ... => Porter
Lemmatization(Lemma;표제어); 어근/원형, 날다 + 나/는... => 날/는...
                            is was be ... => be

SPM - WPM - BPE(*)
; Sentence-piece Model(문장;분절-띄어쓰기) => O
      ; Word-piece Model(어절;분절) => 중국어,일본어...
            ; Bytes Pair Encoding(*)

Normalization
대소문자, 약어(D.C => ?, LA, Los ang... =>), Stopwords, ...
from konlpy.tag import Hannanum, Kkma, Komoran, Okt

ma = [Hannanum(), Kkma(), Komoran(), Okt()]
for m in ma:
    print(m.pos('하늘을 나는 새'))
[('하늘', 'N'), ('을', 'J'), ('나', 'N'), ('는', 'J'), ('새', 'M')]
[('하늘', 'NNG'), ('을', 'JKO'), ('날', 'VV'), ('는', 'ETD'), ('새', 'NNG')]
[('하늘', 'NNG'), ('을', 'JKO'), ('나', 'NP'), ('는', 'JX'), ('새', 'MM')]
[('하늘', 'Noun'), ('을', 'Josa'), ('나', 'Noun'), ('는', 'Josa'), ('새', 'Noun')]
# 뉴스 수집기
from requests import get
from requests.compat import urljoin
from bs4 import BeautifulSoup
import re

urls = ['https://news.naver.com']
seens = []
path = './news/'

while urls:
    url = urls.pop(0)
    
    resp = get(url)
    seens.append(url)
    
    if not re.search(r'text\/html', resp.headers['content-type']):
        continue
        
    dom = BeautifulSoup(resp.text, 'html.parser')
    
    # 1번 - 뉴스 메뉴
    for a in dom.select('.Nlnb_menu_list > li > a[href]')[1:7]:
        nurl = urljoin(url, a.attrs['href'])
        if nurl not in urls and nurl not in seens:
            urls.append(nurl)
            
    # 2번 - 헤드라인
    for a in dom.select('[id^="_SECTION_HEADLINE_LIST"] .sa_text_title[href]'):
        nurl = urljoin(url, a.attrs['href'])
        if nurl not in urls and nurl not in seens:
            urls.append(nurl)
            
    # 3번 - 뉴스본문
    c = dom.select_one('#contents')
    if c:
        file = re.search(r'(\d{8,})$', url).group(1)
        with open(f'{path}{file}.txt', 'w', encoding='utf8') as f:
            f.write(c.get_text())
from os import listdir

def fileids(path):
    return list(map(lambda f:path + ('' if path[-1] == '/' else '/')+f, listdir(path)))
corpus = list()
for file in fileids('news'):
    with open(file, 'r', encoding='utf8') as f:
        corpus.append(
            re.sub(r'^\s+|\s+$', '',
                   re.sub(r'\s+', ' ',
                          re.sub(r'\sCopyright.+', ' ',
                                 re.sub(r'[\xa0-\xff]', ' ', f.read())))))
from nltk.text import Text
from nltk.tokenize import word_tokenize
from nltk.stem import PorterStemmer
import matplotlib.pyplot as plt
PorterStemmer().stem('play'),\
PorterStemmer().stem('played'),\
PorterStemmer().stem('playing'),\
PorterStemmer().stem('was')
('play', 'play', 'play', 'wa')
tObj = list()

for d in corpus:
    if len(tObj) == 0:
        tObj.append(Text(word_tokenize(d)).vocab())
    else:
        tObj.append(tObj[-1] + Text(word_tokenize(d)).vocab())
tObj[-1].N(), tObj[-1].B()
(32428, 10753)
kkmtObj = list()

for d in corpus:
    if len(kkmtObj) == 0:
        kkmtObj.append(Text(ma[1].morphs(d)).vocab())
    else:
        kkmtObj.append(kkmtObj[-1] + Text(ma[1].morphs(d)).vocab())
kkmtObj[-1].N(), kkmtObj[-1].B()
(59421, 5823)
n = 50
plt.plot([1/i for i in range(1,n+1)], c='k')
plt.plot([r[1]/tObj[-1].get(tObj[-1].max())
          for r in tObj[-1].most_common(n)], c='b')
plt.plot([r[1]/kkmtObj[-1].get(kkmtObj[-1].max())
          for r in kkmtObj[-1].most_common(n)], c='r')
[<matplotlib.lines.Line2D at 0x29f701010>]

k = 12
b = .56
plt.plot([k.B() for k in kkmtObj], c='k')
plt.plot([12*k.N()**b for k in kkmtObj], c='r')
[<matplotlib.lines.Line2D at 0x29ecbac10>]

# 띄어쓰기
def ngram(s, n=2, t=True): # t=True;어절, False;음절
    result = []
    
    if not t:
        s = list(s)
        
    for i in range(len(s)-(n-1)):
        result.append(tuple(s[i:i+n])) # 튜플로 수정
        
    return result
gram1 = list()
gram2 = list()
gram3 = list()
gram4 = list()

for c in corpus:
    gram1.extend(ngram(c, n=1, t=False))
    gram2.extend(ngram(c, n=2, t=False))
    gram3.extend(ngram(c, n=3, t=False))
    gram4.extend(ngram(c, n=4, t=False))
from collections import Counter

c1 = Counter(gram1)
c2 = Counter(gram2)
c3 = Counter(gram3)
c4 = Counter(gram4)
n = 50
plt.plot([1/i for i in range(1,n+1)], c='k')
plt.plot([i[1]/max(c1.values()) for i in c1.most_common(n)], c='r')
plt.plot([i[1]/max(c2.values()) for i in c2.most_common(n)], c='g')
plt.plot([i[1]/max(c3.values()) for i in c3.most_common(n)], c='b')
plt.plot([i[1]/max(c4.values()) for i in c4.most_common(n)])
[<matplotlib.lines.Line2D at 0x2a4a52510>]

from nltk.tokenize import sent_tokenize

re.sub(r'\s', '', sent_tokenize(corpus[0])[0])
'초고금리급전대출‘사기주의보’금융감독원은최근불법대부업자가최대수천만원의대출실행을빌미로초고금리불법거래를강요하고살인적인이자를빼돌린뒤잠적하는사례가발생하고있다며26일소비자주의경보를발령했다.'
fileids('news')[0]
'news/0003918245.txt'
q = re.sub(r'\s', '', sent_tokenize(corpus[0])[0])

# => 1:Counter => P(B|A)=P(A,B)/P(A)
# => 2:Counter => P(C|A,B)=P(A,B,C)/P(A,B)
cdict = {1:c1, 2:c2, 3:c3, 4:c4}
n = 2
s = ''

for c in range(len(q)-(n-1)): # ABCDE(5) - (0 ... 5) => A, B, C, D, E
                              #          - (0 ... 4) => AB, BC, CD, DE
                              # 분모                  => ABC, BCD, CDE
    k = tuple(q[c:c+n])       # k = (A,), (A,B), (A,B,C),
    print(k)
    base = cdict[n].get(k, 1) # OOV;
    result = dict()           # n=1, k=(A,), {(A,?):Prob, ...}
    for b in cdict[n+1].keys(): # 분자
        if b[:n] == k: # OOV  # (A,?) = (A,)
            result[b] = cdict[n+1].get(b)/base
#             print(b, c2.get(b)/base)
    print(sorted(result.items(), key=lambda r:r[1], reverse=True)[0])
    best = sorted(result.items(), key=lambda r:r[1], reverse=True)[0]
('초', '고')
(('초', '고', '금'), 0.75)
('고', '금')
(('고', '금', '리'), 1.0)
('금', '리')
(('금', '리', ' '), 0.75)
('리', '급')
---------------------------------------------------------------------------
IndexError                                Traceback (most recent call last)
Cell In[127], line 20
     18             result[b] = cdict[n+1].get(b)/base
     19 #             print(b, c2.get(b)/base)
---> 20     print(sorted(result.items(), key=lambda r:r[1], reverse=True)[0])
     21     best = sorted(result.items(), key=lambda r:r[1], reverse=True)[0]

IndexError: list index out of range
q = re.sub(r'\s', '', sent_tokenize(corpus[0])[0])

# => 1:Counter => P(B|A)=P(A,B)/P(A)
# => 2:Counter => P(C|A,B)=P(A,B,C)/P(A,B)
cdict = {1:c1, 2:c2, 3:c3, 4:c4}
n = 2
s = list()          # 결과
k = tuple(q[:n])    # 분모; n=1, (A,),     n=2, (A,B) [:2]
s.append(k)         # 추가


for c in range(len(q)-(n-1)):
    k = k[-n:]                # n=1, ('A', ' ', 'B') => ('B')   n=2, (' ','C')
#     print(k)
    base = cdict[n].get(k, 1) # OOV;
    result = dict()           # n=1, k=(A,), {(A,?):Prob, ...}
    for b in cdict[n+1].keys(): # 분자
        if b[:n] == k: # OOV  # (A,?) = (A,)
            result[b] = cdict[n+1].get(b)/base
#             print(b, c2.get(b)/base)
#     print(sorted(result.items(), key=lambda r:r[1], reverse=True)[0])
    best = sorted(result.items(), key=lambda r:r[1], reverse=True)[0]
    # 다음글자 ' '일대
    if best[0][-1] == ' ':
        k += tuple(best[0][-1]) # n=1, (A,' '),    n=2, ('A','B',' ')
        s.append(tuple(' ',))   #       0      1              0           2
    k += tuple(q[c+n])          # n=1, (A,' ','B')     n=2, ('A','B',' ','C')
    s.append(k[-1:])            # 
---------------------------------------------------------------------------
IndexError                                Traceback (most recent call last)
Cell In[163], line 22
     19             result[b] = cdict[n+1].get(b)/base
     20 #             print(b, c2.get(b)/base)
     21 #     print(sorted(result.items(), key=lambda r:r[1], reverse=True)[0])
---> 22     best = sorted(result.items(), key=lambda r:r[1], reverse=True)[0]
     23     # 다음글자 ' '일대
     24     if best[0][-1] == ' ':

IndexError: list index out of range
''.join([''.join(i) for i in s])
'초고금리 급전대출 ‘사기 주의 보’ 금융 감독원은 최근 불법 대부업자가 최대 수 천만원 의대 출실'
sent_tokenize(corpus[0])[0]
'초고금리 급전 대출 ‘사기 주의보’ 금융감독원은 최근 불법 대부업자가 최대 수천만원의 대출 실행을 빌미로 초고금리 불법 거래를 강요하고 살인적인 이자를 빼돌린 뒤 잠적하는 사례가 발생하고 있다며 26일 소비자 주의경보를 발령했다.'
# Entropy(불확실정도) => log
# 8개의 공 => 공 1개가 무게가 다름
# log2 => 4 4 => 2 2 => 1 1 => 2^3 = 8 log2^3 => 3
# log3 => 3 3 3 => 1 1 1 => log2^3 = 2 
50% 50% => p(A=true) = .5 => 
|          *
---------1-----X
|     *
|  *
|*
|*      log
|*

|*
|*
|*
| *
|    *      -log=error= (실제정답 - 모델정답) <= error
---------1-----X
|           *

|
|
|
|   *
| *   *      P*-log
-*-----*-1-----X
|   + => 불확실성 높은 상태 => BCE(Binomial Cross Entropy)  => Loss(Error)

- p * log p => 0 or 1
Entropy=-P(A=true)*log(P(A=true))+-P(A=false)*log(P(A=false))
       =-sigma(P(A=case)log(P(A=case)))
단어, 초고금리
     초장
     초
     초??????
     있다
     있었다
     있었고
     있는데
    [있]+?
     초고+금
     어근[L]+접사[R]
Branch Entropy
P(다음글자|처음부터 바로 이전글자)
    ?   | A, B, C
=> P(A,B,C,?)     freq(A,B,C,?) => 4gram
   ----------  =  -------------
    P(A,B,C)        freq(A,B,C) => 3gram
def findKey(gram, key):
    k = tuple(key)
    return list(filter(lambda g:g[:len(k)] == k, gram.keys()))
from math import log
key = '초'
base = c1.get(tuple(key))
be = 0.0

for k in findKey(c2, key):
    p = c2.get(k)/base
    be += -p*log(p)
    print('{} = {}/{} = {}'.format(k, c2.get(k), base, p))
print(key, be)
('초', '고') = 4/37 = 0.10810810810810811
('초', '대') = 2/37 = 0.05405405405405406
('초', '긴') = 2/37 = 0.05405405405405406
('초', '콜') = 1/37 = 0.02702702702702703
('초', ' ') = 10/37 = 0.2702702702702703
('초', '로') = 4/37 = 0.10810810810810811
('초', '미') = 1/37 = 0.02702702702702703
('초', '구') = 1/37 = 0.02702702702702703
('초', '개') = 1/37 = 0.02702702702702703
('초', '기') = 1/37 = 0.02702702702702703
('초', '점') = 2/37 = 0.05405405405405406
('초', '반') = 1/37 = 0.02702702702702703
('초', '음') = 1/37 = 0.02702702702702703
('초', '특') = 1/37 = 0.02702702702702703
('초', "'") = 1/37 = 0.02702702702702703
('초', '청') = 1/37 = 0.02702702702702703
('초', '임') = 2/37 = 0.05405405405405406
('초', '과') = 1/37 = 0.02702702702702703
초 2.5389886352555995
key = '고금리'
base = c3.get(tuple(key))
be = 0.0

for k in findKey(c4, key):
    p = c4.get(k)/base
    be += -p*log(p)
    print('{} = {}/{} = {}'.format(k, c4.get(k), base, p))
print(key, be)
('고', '금', '리', ' ') = 3/4 = 0.75
('고', '금', '리', '로') = 1/4 = 0.25
고금리 0.5623351446188083
초    고    금   리
2.5  0.5   0   .5
*
   *           *
          *
=> [알잘딱깔센]/하게 => 
    창덕궁 달빛기행
ma[1].morphs('창덕궁 달빛기행')
['창덕궁', '달빛', '기행']
```