## 1. inplace => mutable 처럼 재할당 없이 사용하겠다는 것!

## 2. columns => axis = 1이 숨겨져 있는 뜻. axis = 0 => row

## 3. index => 재할당 => 메모리를 추가로 사용

## 4. drop => 데이터 버리기. axis를 꼭 표기해줘야 한다. (`column =` 을 사용하면 안써도 됨)

## 5. 이름에 특수문자 또는 공백 => '.'으로 데이터를 관리할 수 없다.

## 6. value_count => 종류 수 추출

## 7. describe => 값의 범위 및 통계치 추출 => 데이터의 오류 파악

## 8. map => func + dict => 데이터를 보다 깔끔하게 처리할 수 있다.

## 9. split => 문자열 분리

## 10. isin => value_counts 와 결합하여 사용하기!

## 11. duplicated => 기계학습 vs 데이터분석

## 12. 중복 표현 방식은 여러가지 => keep 등의 옵션을 사용한다.

## 13. drop_duplicate => 

## 14. type casting

## 15. coerce

## 16. as => 데이터 타입 변환

## 17. 탐색적 데이터 분석 => 숫자와 그래프를 이용해 데이터를 얻는다.

## 18. boxplot() => 그래프 생성

## 19. include = 'category'=> 카테고리로 통계치를 계산

## 20. x축이 인덱스 => plot.bar => 막대 그래프

## 21. ylim => 좌표축 조정

## 22. 데이터.['칼럼명'] => 새로운 칼럼 생성

## 23. stack => 아래로 연결

## 24. concatenate => 옆으로 연결

## 25. hstack vs vstack vs dstack

## 26. r_ vs c_ => row 와 column으로 붙인다.

## 27. column_stack => 

## 28. ignore_index => 칼럼 연결시 인덱스를 자연스럽게 연결

## 29. split axis => 등분 한 뒤 아래로 이어붙이기

## 30. 데이터 수집시 수집하는 시간을 꼭 적어야 한다.

```python
Loading [MathJax]/jax/output/CommonHTML/fonts/TeX/fontdata.js
P => MLE, MAP(Bayes) => p=theta optimization => derivation
PAC learning => Model(error 수반) < error(^-N)
P(A) => 조건부(conditional),결합(joint)
P(B=결과|A=원인) => P(A,B) / P(A)
P(A|B) => P(A,B) / P(B)
P(B|A) => P(A=원인|B=결과)P(B) / P(A)
=> P(A,B) = P(B|A)P(A)
=> sum_a P(B) = P(A=true, B) + P(A=false, B); marginalization, summing out
P(A={},B={},C={},D={},E={}) => P(B,C,D,E|A)P(A)
                            => P(E|A,B,C,D,E)P(A,B,C,D,E)
                                             P(E|A,B,C,D)P(A,B,C,D) ... P(B|A)P(A)
1st Markov Assumption       => P(E|D)P(D|C)P(C|B)P(B|A)P(A) => N-gram(2-gram)
                                                  --------
                                                   P(A,B)   P(A)=freq(a/N)
                                                  freq(A,B)/freq(A)
N-gram = Language Model => s,s,... P(s) 계산가능
                           s를 생성

Preprocessing => tokenizing => sentence => tokens(word, regexp, tweet tokenizer)
                             splitlines(X)        split(기호)     +     P, MA(konlpy)
from konlpy.tag import Hannanum, Kkma, Komoran, Okt

# core                api
# Java(JVM) - Jpype - Python
han = Hannanum()
kkm = Kkma()
kom = Komoran()
okt = Okt()
OpenJDK 64-Bit Server VM warning: Attempt to protect stack guard pages failed.
OpenJDK 64-Bit Server VM warning: Attempt to deallocate stack guard pages failed.
len(han.tagset), len(kkm.tagset), len(kom.tagset), len(okt.tagset)
(29, 67, 45, 19)
from nltk.tokenize import treebank
from nltk.tag import pos_tag
from nltk.help import upenn_tagset
upenn_tagset('NN')
NN: noun, common, singular or mass
    common-carrier cabbage knuckle-duster Casino afghan shed thermostat
    investment slide humour falloff slick wind hyena override subhumanity
    machinist ...
from nltk.corpus import gutenberg

emma = gutenberg.open(gutenberg.fileids()[0]).read()
from nltk.text import Text
from nltk.tokenize import word_tokenize

t1 = Text(word_tokenize(emma))

tokenizer = treebank.TreebankWordTokenizer()
t2 = Text(tokenizer.tokenize(emma))
t1.vocab().B(), t2.vocab().B()
(8406, 9852)
list(zip(t1.vocab().most_common(20), t2.vocab().most_common(20)))
[((',', 12016), (',', 12016)),
 (('.', 6355), ('to', 5109)),
 (('to', 5125), ('the', 4842)),
 (('the', 4844), ('and', 4652)),
 (('and', 4653), ('of', 4250)),
 (('of', 4272), ("''", 3725)),
 (('I', 3177), ('I', 3164)),
 (('--', 3100), ('--', 3100)),
 (('a', 3001), ('a', 3001)),
 (("''", 2452), ('was', 2375)),
 (('was', 2383), (';', 2353)),
 (('her', 2360), ('her', 2223)),
 ((';', 2353), ('not', 2218)),
 (('not', 2242), ('in', 2089)),
 (('in', 2103), ('be', 1942)),
 (('it', 2103), ('it', 1899)),
 (('be', 1965), ('she', 1765)),
 (('she', 1774), ('that', 1718)),
 (('``', 1735), ('had', 1600)),
 (('that', 1729), ('you', 1563))]
data = '아버지가 방에들어 가신다.'
han.pos(data)
[('아버지', 'N'), ('가', 'J'), ('방에들어', 'N'), ('가', 'P'), ('시ㄴ다', 'E'), ('.', 'S')]
kkm.pos(data)
[('아버지', 'NNG'),
 ('가', 'JKS'),
 ('방', 'NNG'),
 ('에', 'JKM'),
 ('듣', 'VV'),
 ('어', 'ECD'),
 ('가시', 'VV'),
 ('ㄴ다', 'EFN'),
 ('.', 'SF')]
kom.pos(data)
[('아버지', 'NNG'),
 ('가', 'JKS'),
 ('방', 'NNG'),
 ('에', 'JKB'),
 ('들', 'VV'),
 ('어', 'EC'),
 ('가', 'VV'),
 ('시', 'EP'),
 ('ㄴ다', 'EF'),
 ('.', 'SF')]
okt.pos(data)
[('아버지', 'Noun'),
 ('가', 'Josa'),
 ('방', 'Noun'),
 ('에', 'Josa'),
 ('들어', 'Verb'),
 ('가신다', 'Verb'),
 ('.', 'Punctuation')]
han.pos('알잘딱깔센')
[('알잘딱깔센', 'N')]
kkm.pos('알잘딱깔센')
[('알', 'NNG'),
 ('잘', 'MAG'),
 ('딱', 'MAG'),
 ('깔', 'VV'),
 ('ㄹ', 'ETD'),
 ('세', 'VV'),
 ('ㄴ', 'ETD')]
kom.pos('알잘딱깔센')
[('알', 'NNG'),
 ('잘', 'MAG'),
 ('딱', 'MAG'),
 ('깔', 'VV'),
 ('ㄹ', 'ETM'),
 ('센', 'NNP')]
okt.pos('알잘딱깔센')
[('알잘', 'Verb'), ('딱', 'VerbPrefix'), ('깔', 'Verb'), ('센', 'Verb')]
t1.vocab().most_common(50)
[(',', 12016),
 ('.', 6355),
 ('to', 5125),
 ('the', 4844),
 ('and', 4653),
 ('of', 4272),
 ('I', 3177),
 ('--', 3100),
 ('a', 3001),
 ("''", 2452),
 ('was', 2383),
 ('her', 2360),
 (';', 2353),
 ('not', 2242),
 ('in', 2103),
 ('it', 2103),
 ('be', 1965),
 ('she', 1774),
 ('``', 1735),
 ('that', 1729),
 ('you', 1664),
 ('had', 1605),
 ('as', 1387),
 ('he', 1365),
 ('for', 1320),
 ('have', 1301),
 ('is', 1221),
 ('with', 1185),
 ('very', 1151),
 ('but', 1148),
 ('Mr.', 1091),
 ('his', 1084),
 ('!', 1063),
 ('at', 996),
 ('so', 918),
 ("'s", 866),
 ('Emma', 855),
 ('all', 831),
 ('could', 824),
 ('would', 813),
 ('been', 755),
 ('him', 748),
 ('on', 674),
 ('Mrs.', 668),
 ('any', 651),
 ('?', 621),
 ('my', 619),
 ('no', 616),
 ('Miss', 592),
 ('were', 590)]
import matplotlib.pyplot as plt
plt.plot([1/i for i in range(1,51)])
plt.plot([r[1]/12016 for r in t1.vocab().most_common(50)])
plt.plot([r[1]/12016 for r in t2.vocab().most_common(50)])
[<matplotlib.lines.Line2D at 0x29954b150>]

from konlpy.corpus import kolaw, kobill
law = kolaw.open(kolaw.fileids()[0]).read()
from nltk.tokenize import regexp_tokenize

kt1 = Text(law.split())
kt2 = Text(word_tokenize(law))
kt3 = Text(regexp_tokenize(law, r'\b\w+\b'))
kt4 = Text(pos_tag(word_tokenize(law)))
kt5 = Text(han.morphs(law))
kt6 = Text(kkm.morphs(law))
kt7 = Text(kom.morphs(law))
kt8 = Text(okt.morphs(law))
kt9 = Text(han.pos(law))
kt10 = Text(kkm.pos(law))
kt11 = Text(kom.pos(law))
kt12 = Text(okt.pos(law))
n = 50
plt.plot([1/i for i in range(1,n+1)], c='k')
plt.plot([r[1]/kt1.count(kt1.vocab().max()) for r in kt1.vocab().most_common(n)], c='r')
plt.plot([r[1]/kt2.count(kt2.vocab().max()) for r in kt2.vocab().most_common(n)], c='g')
plt.plot([r[1]/kt3.count(kt3.vocab().max()) for r in kt3.vocab().most_common(n)], c='b')
plt.plot([r[1]/kt4.count(kt4.vocab().max()) for r in kt4.vocab().most_common(n)], c='c')
[<matplotlib.lines.Line2D at 0x29c494490>]

n = 50
plt.plot([1/i for i in range(1,n+1)], c='k')
plt.plot([r[1]/kt1.count(kt1.vocab().max()) for r in kt1.vocab().most_common(n)], c='k')
plt.plot([r[1]/kt4.count(kt4.vocab().max()) for r in kt4.vocab().most_common(n)], c='r')
plt.plot([r[1]/kt5.count(kt5.vocab().max()) for r in kt5.vocab().most_common(n)])
plt.plot([r[1]/kt6.count(kt6.vocab().max()) for r in kt6.vocab().most_common(n)], c='b')
plt.plot([r[1]/kt7.count(kt7.vocab().max()) for r in kt7.vocab().most_common(n)])
plt.plot([r[1]/kt8.count(kt8.vocab().max()) for r in kt8.vocab().most_common(n)])
[<matplotlib.lines.Line2D at 0x29cad2190>]

t1.vocab().N(), t1.vocab().B(),\
kt1.vocab().N(), kt1.vocab().B()
(191785, 8406, 4178, 2029)
n = 50
plt.plot([1/i for i in range(1,n+1)], c='k')
plt.plot([r[1]/kt1.count(kt1.vocab().max()) for r in kt1.vocab().most_common(n)], c='k')
plt.plot([r[1]/kt4.count(kt4.vocab().max()) for r in kt4.vocab().most_common(n)], c='r')
plt.plot([r[1]/kt9.count(kt9.vocab().max()) for r in kt9.vocab().most_common(n)])
plt.plot([r[1]/kt10.count(kt10.vocab().max()) for r in kt10.vocab().most_common(n)], c='r')
plt.plot([r[1]/kt11.count(kt11.vocab().max()) for r in kt11.vocab().most_common(n)],)
plt.plot([r[1]/kt12.count(kt12.vocab().max()) for r in kt12.vocab().most_common(n)])
[<matplotlib.lines.Line2D at 0x29cd0b2d0>]

list(zip(kt6.vocab().most_common(10), kt10.vocab().most_common(10)))
[(('의', 532), (('의', 'JKG'), 532)),
 (('하', 457), (('.', 'SF'), 359)),
 (('.', 359), (('하', 'XSV'), 350)),
 (('에', 328), (('에', 'JKM'), 328)),
 (('는', 281), (('ㄴ다', 'EFN'), 243)),
 (('ㄴ다', 243), (('ㄴ', 'ETD'), 234)),
 (('ㄴ', 234), (('을', 'JKO'), 211)),
 (('을', 232), (('은', 'JX'), 182)),
 (('은', 195), (('는', 'JX'), 180)),
 (('이', 192), (('저', 'NP'), 155))]
t1.vocab().N(), t1.vocab().B()
(191785, 8406)
n = 50
s1 = 0.0
s2 = 0.0

for r in t1.vocab().most_common(n):
    s1 += r[1]
    s2 += r[1]/t1.vocab().N()
    
    print(r[0], r[1]/t1.vocab().N(), t1.vocab().freq(r[0]))
s1, s2
, 0.06265349219177725 0.06265349219177725
. 0.03313606382146675 0.03313606382146675
to 0.026722632114086087 0.026722632114086087
the 0.025257449748416195 0.025257449748416195
and 0.024261542873530256 0.024261542873530256
of 0.022274943295878195 0.022274943295878195
I 0.016565424824673464 0.016565424824673464
-- 0.016163933571447194 0.016163933571447194
a 0.015647730531584848 0.015647730531584848
'' 0.012785150037802747 0.012785150037802747
was 0.012425372161535051 0.012425372161535051
her 0.012305446202779154 0.012305446202779154
; 0.012268946997940402 0.012268946997940402
not 0.011690173892640196 0.011690173892640196
in 0.010965403967984982 0.010965403967984982
it 0.010965403967984982 0.010965403967984982
be 0.010245848215449592 0.010245848215449592
she 0.009249941340563651 0.009249941340563651
`` 0.009046588627890607 0.009046588627890607
that 0.009015303595171676 0.009015303595171676
you 0.008676382407383268 0.008676382407383268
had 0.00836874625231379 0.00836874625231379
as 0.007232056730192664 0.007232056730192664
he 0.0071173449435565864 0.0071173449435565864
for 0.0068827071981646115 0.0068827071981646115
have 0.006783637927887999 0.006783637927887999
is 0.006366504158302266 0.006366504158302266
with 0.006178793961988685 0.006178793961988685
very 0.006001512109914748 0.006001512109914748
but 0.0059858695935552835 0.0059858695935552835
Mr. 0.005688661782725448 0.005688661782725448
his 0.0056521625778866965 0.0056521625778866965
! 0.005542664963370441 0.005542664963370441
at 0.0051933154313423885 0.0051933154313423885
so 0.004786610005996298 0.004786610005996298
's 0.004515473055765571 0.004515473055765571
Emma 0.004458117162447532 0.004458117162447532
all 0.004332977031571812 0.004332977031571812
could 0.00429647782673306 0.00429647782673306
would 0.004239121933415022 0.004239121933415022
been 0.0039366999504653645 0.0039366999504653645
him 0.0039002007456266133 0.0039002007456266133
on 0.003514352008759809 0.003514352008759809
Mrs. 0.003483066976040879 0.003483066976040879
any 0.0033944260500039106 0.0033944260500039106
? 0.0032380008864092602 0.0032380008864092602
my 0.003227572542169617 0.003227572542169617
no 0.003211930025810152 0.003211930025810152
Miss 0.0030867898949344316 0.0030867898949344316
were 0.0030763615506947885 0.0030763615506947885
(98964.0, 0.5160153296660321)
n = 7300
s1 = 0.0
s2 = 0.0

for r in t1.vocab().most_common()[::-1][:n]:
    s1 += r[1]
    s2 += r[1]/t1.vocab().N()
s1, s2
(20345.0, 0.1060823317777741)
n = 30
s1 = 0.0
s2 = 0.0

for r in kt6.vocab().most_common(n):
    s1 += r[1]
    s2 += r[1]/kt6.vocab().N()
    
    print(r[0], r[1]/kt6.vocab().N(), kt6.vocab().freq(r[0]))
s1, s2
의 0.05291952650949965 0.05291952650949965
하 0.04545906694519049 0.04545906694519049
. 0.03571073311449319 0.03571073311449319
에 0.03262707649457873 0.03262707649457873
는 0.02795185516761166 0.02795185516761166
ㄴ다 0.024171888988361683 0.024171888988361683
ㄴ 0.023276633840644583 0.023276633840644583
을 0.023077688252263005 0.023077688252263005
은 0.01939719486720382 0.01939719486720382
이 0.019098776484631454 0.019098776484631454
저 0.015418283099572267 0.015418283099572267
여 0.014821446334427535 0.014821446334427535
· 0.01442355515766438 0.01442355515766438
ㄹ 0.014025663980901224 0.014025663980901224
조 0.01352830000994728 0.01352830000994728
를 0.01342882721575649 0.01342882721575649
법률 0.012036208097085447 0.012036208097085447
되 0.011240425743559136 0.011240425743559136
, 0.01004675221326967 0.01004675221326967
있 0.009847806624888093 0.009847806624888093
다 0.008753605888789416 0.008753605888789416
정하 0.008753605888789416 0.008753605888789416
수 0.008753605888789416 0.008753605888789416
대통령 0.008355714712026261 0.008355714712026261
의하 0.008256241917835472 0.008256241917835472
과 0.008156769123644683 0.008156769123644683
① 0.007758877946881528 0.007758877946881528
② 0.007758877946881528 0.007758877946881528
국가 0.007261513975927584 0.007261513975927584
헌법 0.006863622799164429 0.006863622799164429
(5159.0, 0.5131801452302795)
n = 800
s1 = 0.0
s2 = 0.0

for r in kt6.vocab().most_common()[::-1][:n]:
    s1 += r[1]
    s2 += r[1]/kt6.vocab().N()
s1, s2
(961.0, 0.0955933552173462)
kt6.vocab().N(), kt6.vocab().B()
(10053, 1247)
800/1247
0.6415396952686447
threshold = .3
s = 0.0
mfw = list()
mrw = list()

for r in kt6.vocab().most_common():
    s += kt6.vocab().freq(r[0])
    
    if s > threshold:
        break
        
    mfw.append(r[0])
    
s = 0.0
for r in kt6.vocab().most_common()[::-1]:
    s += kt6.vocab().freq(r[0])
    
    if s > threshold:
        break
        
    mrw.append(r[0])
    
len(mfw), len(mrw), 1247
(9, 1156)
1247-(9+1156)
82
list(set(kt6.tokens) - set(mfw) - set(mrw))
['제',
 '있',
 '며',
 '어야',
 '받',
 '에서',
 '수',
 '조직',
 '으로',
 '기',
 '여야',
 '국가',
 '회의',
 '법률',
 '권리',
 '의결',
 '경우',
 '공무원',
 '대하',
 '②',
 '헌법',
 '국회',
 '그',
 '가',
 '되',
 '임명',
 '를',
 '없',
 '경제',
 '직무',
 '①',
 '다',
 '·',
 '의하',
 '및',
 '거나',
 '얻',
 '지',
 '아니하',
 '이상',
 '여',
 '③',
 '2',
 '항',
 '위원',
 '관하',
 '법원',
 '기타',
 '임기',
 '정부',
 '④',
 '바',
 '가지',
 '국무',
 '저',
 '국민',
 '정하',
 '의원',
 '과',
 '보장',
 '대통령',
 '와',
 '또는',
 '자유',
 'ㄹ',
 '로',
 '필요',
 '3',
 '1',
 '고',
 '때',
 '조',
 ',',
 '선거',
 '국회의원',
 '일',
 '모든',
 '의무',
 '이',
 '국',
 '위하',
 '사항']
k = 10, 100
b = .4, .6
heaps = []
heaps.append(Text(word_tokenize(gutenberg.open(gutenberg.fileids()[0]).read())).vocab())
for file in gutenberg.fileids()[1:]:
    corpus = gutenberg.open(file).read()
    heaps.append(heaps[-1]+Text(word_tokenize(corpus)).vocab())
len(heaps), heaps[0].N(), heaps[-1].N(), heaps[0].B(), heaps[-1].B()
(18, 191785, 2538838, 8406, 61835)
k = 15
b = .55
plt.plot([h.B() for h in heaps])
plt.plot([k*h.N()**b for h in heaps])
[<matplotlib.lines.Line2D at 0x2a02601d0>]

(k*10000000**b) * .07
7433.430736033454
(9+1156)/1247
0.9342421812349639
heaps = []
heaps.append(Text(kkm.morphs(kobill.open(kobill.fileids()[0]).read())).vocab())
for file in kobill.fileids()[1:]:
    corpus = kobill.open(file).read()
    heaps.append(heaps[-1]+Text(kkm.morphs(corpus)).vocab())
k = 12
b = .48
plt.plot([h.B() for h in heaps])
plt.plot([k*h.N()**b for h in heaps])
[<matplotlib.lines.Line2D at 0x29cd91850>]

heaps[-1].N(), (k*heaps[-1].N()**b) * .07
(21686, 101.30839236540183)
heaps[-1].B(), (k*10000000**b) * .07
(1489, 1924.3288283249292)
N-gram: N개의 token 구성된 시퀀스 freq(A,B,C,...) => P(A,B,C,...) => 결합확률
                              P(?|A,B) = P(A,B,?)/P(A,B)
                                       = freq(A,B,?)/N / freq(A,B)/N
                                       = freq(A,B,?)/freq(A,B)
def ngram(s, n=2, t=True): # t=True;어절, False;음절
    result = []
    
    if not t:
        s = list(s)
        
    for i in range(len(s)-(n-1)):
        result.append(''.join(s[i:i+n]))
        
    return result
tokens = kkm.morphs(law)
bigram = Text(ngram(tokens))
unigram = Text(ngram(tokens, n=1))
# NLU -> NLG
import re

seed = '대통령' # 대통령 은(0.33) 법률(0.22)
c = 0
s = 0.0

for i in range(10):
    result = {}
    for t in list(set(filter(lambda t:t.startswith(seed), bigram.tokens))):
        result[t] = bigram.count(t)/unigram.count(seed)
    n = sorted(result.items(), key=lambda r:r[1], reverse=True)[0]
    print(n)
    seed = re.sub(seed, '', n[0])
('대통령은', 0.3333333333333333)
('은법률', 0.22564102564102564)
('법률이', 0.48760330578512395)
('이정하', 0.2916666666666667)
('정하는', 0.5681818181818182)
('는바', 0.13167259786476868)
('바에', 1.0)
('에의하', 0.2530487804878049)
('의하여', 0.7951807228915663)
('여야하', 0.2080536912751678)
plaw = re.sub(r'^\s+|\s+$', '', re.sub(r'\s+', ' ', law))
trigram = Text(ngram(plaw, n=3, t=False))
bigram = Text(ngram(plaw, t=False))
unigram = Text(ngram(plaw, n=1, t=False))
# seed = '대통령은 국회의 국회의 국회의 ...' # 대통령 은(0.33) 법률(0.22)
# => P(B|A)
seed = '대통령은 법률이 정하는 바에 의하여 국회의 자유를 가진다. '
# => P(C|A,B)
c = 0
s = 0.0

result = {}
for t in list(set(filter(lambda t:t.startswith(seed[-2:]), trigram.tokens))):
    result[t] = trigram.count(t)/bigram.count(seed[-2:])
sorted(result.items(), key=lambda r:r[1], reverse=True)[0]
('. 제', 0.37464788732394366)
```