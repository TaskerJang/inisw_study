## 1. Bytes -> decode -> Str -> json.loads -> Python obj(Dict)

## 2. html parser -> dom BS4 -> element.Tag => find, find_all, find_parent, find_prev_sibling, find_next_sibling

## 3. dom.select

## 4. polymorphism : 다형성 => 같은 이름인데 다르게 처리하는 것. => like override => 뒤집어 쓰는 것. (상속과 관련)

## 5. overloading => 상속과 관련없는 것. like operator overloading, function overloading (파이썬은 기본적으로 지원하지 않음)

## 6. function overloading : 인자 값의 갯수에 따라 다르게 행동. 파이썬은 뒤엎으므로 기본적으로 지원 X

## 7. def x (a,b = None):
        print('')

=> default technic

## 8. class A (object, metaclass = type): 

## 9. type 은 클래스 타입을 반환해준다.

## 10. type은 새로운 타입을 만들어준다.

## 11. 관심의 분리 => type 은 클래스를 조절해준다.

## 12. abstract 클래스로부터 상속=> abstract method를 붙이지 않아도 된다.

## 13. 메타클래스를 상속 받는다. => 변수에 메타클래스를 대입!!

## 14. 싱글톤 : 단 한개의 인스턴스만 만들 수 있는 클래스

## 15. 메타 클래스 `__call__` -> 클래스에 괄호 붙일 수 있다.

## 16. `__new__`: 메타클래스를 클래스에서 사용할 수 있게 해줌.

## 17. @abstract method => 합성 함수

```python

HTTP.response.body 해석?
(Bytes) -> Content-type(MIME) ___/___; text/html, application/json
HTTP.request.header <= User-agent
Bytes->decode->Str; 이때부터 문자열 처리 가능 => RE(불편)
Bytes->decode->Str->json.loads->Python obj(Dict)
HTML(Markup;Document;Semi-structured) -> HTML Parser -> DOM(Tree) <= BS4
element.Tag => find, find_all (자식, 자손, 밑에 방향)
               find_parent, find_parents (부모, 조상, 위에 방향)
               find_prev_sibling(s), find_next_sibling(s) (부모를 공유하는 형제노드, 이전, 이후)
from requests import get

url = 'http://pythonscraping.com/pages/page3.html'
resp = get(url)
resp.status_code, resp.reason, resp.request.headers, resp.headers
(200,
 'OK',
 {'User-Agent': 'python-requests/2.31.0', 'Accept-Encoding': 'gzip, deflate, br', 'Accept': '*/*', 'Connection': 'keep-alive'},
 {'Server': 'nginx', 'Date': 'Thu, 14 Mar 2024 00:14:05 GMT', 'Content-Type': 'text/html', 'Last-Modified': 'Sat, 09 Jun 2018 19:15:59 GMT', 'Transfer-Encoding': 'chunked', 'Connection': 'keep-alive', 'ETag': 'W/"5b1c276f-965"', 'X-Powered-By': 'PleskLin', 'Content-Encoding': 'br'})
import re

re.search(r'text', resp.headers['content-type'])
<re.Match object; span=(0, 4), match='text'>
from bs4 import BeautifulSoup

dom = BeautifulSoup(resp.text, 'html.parser')
[tag.name for tag in dom.div.find_all(recursive=False)]

#                   DOM #document    [html.parser = lxml]
#       head                    body
#                                 div
#                      img   h1   div   table    div
#                        img   h1   div   table   p    div
['img', 'h1', 'div', 'table', 'div']
node = dom.find(string=re.compile(r'\$0\.50')).find_parent()
node.find_next_sibling().find('img') is not None
node.find_next_sibling().find('img').attrs['src']
'../img/gifts/img4.jpg'
from requests.compat import urljoin

url = urljoin(resp.request.url, node.find_next_sibling().find('img').attrs['src'])
resp = get(url)
resp.status_code, resp.reason, resp.request.headers, resp.headers
(200,
 'OK',
 {'User-Agent': 'python-requests/2.31.0', 'Accept-Encoding': 'gzip, deflate, br', 'Accept': '*/*', 'Connection': 'keep-alive'},
 {'Server': 'nginx', 'Date': 'Thu, 14 Mar 2024 00:35:41 GMT', 'Content-Type': 'image/jpeg', 'Content-Length': '85007', 'Last-Modified': 'Mon, 04 Aug 2014 00:49:04 GMT', 'Connection': 'keep-alive', 'ETag': '"53ded880-14c0f"', 'X-Powered-By': 'PleskLin', 'Accept-Ranges': 'bytes'})
resp.headers['content-type']
'image/jpeg'
fname = re.search(r'.+/(.+\.jpg)$', resp.url).group(1)
fp = open(fname, 'bw')
fp.write(resp.content)
fp.close()
[tag.name for tag in node.find_parents(limit=2)]
['tr', 'table']
node.find_parents(limit=2)[-1].name
'table'
node.find_parents(limit=2)[-1].find_next_sibling()
<div id="footer">
© Totally Normal Gifts, Inc. <br/>
+234 (617) 863-0736
</div>
dom.find(attrs={'id':'footer'})
<div id="footer">
© Totally Normal Gifts, Inc. <br/>
+234 (617) 863-0736
</div>
node.find_parents(limit=2)[-1].find_next_sibling() is dom.find(attrs={'id':'footer'})
True
dom.div.find_all(recursive=False)[-1] is dom.find(attrs={'id':'footer'})
True
<tag attributes=id, class> id => 고유한 값, class => 다중상속
node.find_parents(limit=2)[-1].find_all(recursive=False)[0]
node.find_parents(limit=2)[-1].find().find_all(recursive=False)[1].get_text()
'\nDescription\n'
# table에 있는 이미지 전부를 찾는 법
dom.table.find_all('img')
dom.table.find_all(attrs={'src':re.compile(r'jpg$')})
for tag in dom.table.find_all(recursive=False)[1:]:
    print(tag.find_all(recursive=False)[-1].find())
for tag in node.find_parent().find_previous_siblings()[:-1]:
    print(tag.find_all()[-1])
for tag in node.find_parent().find_next_siblings():
    print(tag.find_all()[-1])
<img src="../img/gifts/img1.jpg"/>
<img src="../img/gifts/img2.jpg"/>
<img src="../img/gifts/img3.jpg"/>
<img src="../img/gifts/img4.jpg"/>
<img src="../img/gifts/img6.jpg"/>
<img src="../img/gifts/img3.jpg"/>
<img src="../img/gifts/img2.jpg"/>
<img src="../img/gifts/img1.jpg"/>
<img src="../img/gifts/img6.jpg"/>
# CSS Selector
태그이름 {스타일}
      -------- 관심 X
#아이디
.클래스
.클래스A.클래스B
.클래스A .클래스B 자손(공백)
.클래스A > .클래스B 자식(>)
[속성=값]
태그이름, .클래스
가상선택자:first-child, last-child, nth-of-type, nth-child
dom.select('table img')
# dom.table.find_all('img')
dom.select('table img[src$="jpg"]')
# dom.table.find_all(attrs={'src':re.compile(r'jpg$')})
dom.select('table > tr:first-child ~ tr > *:nth-child(4) > :first-child')
dom.select('td:has(> img) > *')
# A ~ Z
# A 이후 부터 Z
# for tag in dom.table.find_all(recursive=False)[1:]:
#     print(tag.find_all(recursive=False)[-1].find())
# for tag in node.find_parent().find_previous_siblings()[:-1]:
#     print(tag.find_all()[-1])
# for tag in node.find_parent().find_next_siblings():
#     print(tag.find_all()[-1])
dom.select('tr > td:nth-of-type(4) > img')[0].find_parent()
<td>
<img src="../img/gifts/img1.jpg"/>
</td>
url = 'https://www.google.com/search?q=수지'
ua = 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Whale/3.24.223.24 Safari/537.36'
resp = get(url, headers={'user-agent':ua})
# 잘 받았는지 확인
dom = BeautifulSoup(resp.content, 'html.parser')
for tag in dom.select('a:has(> h3)'):
    print(tag.attrs['href'] if tag.has_attr('href') else None)
    print(tag.select_one('h3').get_text())
https://namu.wiki/w/%EC%88%98%EC%A7%80(1994)
수지(1994)
https://ko.wikipedia.org/wiki/%EC%88%98%EC%A7%80_(1994%EB%85%84)
수지 (1994년) - 위키백과, 우리 모두의 백과사전
https://namu.wiki/w/%EC%88%98%EC%A7%80
수지
https://m2.melon.com/artist/song.htm?artistId=514741
수지 (Suzy)- 아티스트채널 - 멜론
https://ko.wikipedia.org/wiki/%EC%88%98%EC%A7%80
수지 - 위키백과, 우리 모두의 백과사전
https://www.autotribune.co.kr/news/articleView.html?idxno=12365
"제가 죽어야겠네요"... 가수 수지, 연예인 동료들에 눈물 ...
https://www.hani.co.kr/arti/culture/culture_general/1112601.html
다시 아이돌 된 수지 “상처 많은 '이두나' 안아주고 싶어”
https://www.sujigu.go.kr/
수지구청
/search?q=%EC%88%98%EC%A7%80&sca_esv=6470011d51d3dd33&ei=A1TyZYanKKe9vr0PmZa_uAM&start=10&sa=N
결과 더보기
None
다시 시도
for tag in dom.select('a:has(.LC20lb)'):
    print(tag.attrs['href'] if tag.has_attr('href') else None)
    print(tag.select_one('h3').get_text())
https://namu.wiki/w/%EC%88%98%EC%A7%80(1994)
수지(1994)
https://ko.wikipedia.org/wiki/%EC%88%98%EC%A7%80_(1994%EB%85%84)
수지 (1994년) - 위키백과, 우리 모두의 백과사전
https://namu.wiki/w/%EC%88%98%EC%A7%80
수지
https://m2.melon.com/artist/song.htm?artistId=514741
수지 (Suzy)- 아티스트채널 - 멜론
https://ko.wikipedia.org/wiki/%EC%88%98%EC%A7%80
수지 - 위키백과, 우리 모두의 백과사전
https://www.autotribune.co.kr/news/articleView.html?idxno=12365
"제가 죽어야겠네요"... 가수 수지, 연예인 동료들에 눈물 ...
https://www.hani.co.kr/arti/culture/culture_general/1112601.html
다시 아이돌 된 수지 “상처 많은 '이두나' 안아주고 싶어”
https://www.sujigu.go.kr/
수지구청
for tag in dom.find_all(attrs={'class':re.compile(r'^LC20lb')}):
    print(tag.find_parent().attrs['href'])
    print(tag.get_text())
https://namu.wiki/w/%EC%88%98%EC%A7%80(1994)
수지(1994)
https://ko.wikipedia.org/wiki/%EC%88%98%EC%A7%80_(1994%EB%85%84)
수지 (1994년) - 위키백과, 우리 모두의 백과사전
https://namu.wiki/w/%EC%88%98%EC%A7%80
수지
https://m2.melon.com/artist/song.htm?artistId=514741
수지 (Suzy)- 아티스트채널 - 멜론
https://ko.wikipedia.org/wiki/%EC%88%98%EC%A7%80
수지 - 위키백과, 우리 모두의 백과사전
https://www.autotribune.co.kr/news/articleView.html?idxno=12365
"제가 죽어야겠네요"... 가수 수지, 연예인 동료들에 눈물 ...
https://www.hani.co.kr/arti/culture/culture_general/1112601.html
다시 아이돌 된 수지 “상처 많은 '이두나' 안아주고 싶어”
https://www.sujigu.go.kr/
수지구청
dom.select('div > div > span > a > h3')
[<h3 class="LC20lb MBeuO DKV0Md">수지(1994)</h3>,
 <h3 class="LC20lb MBeuO DKV0Md">수지 (1994년) - 위키백과, 우리 모두의 백과사전</h3>,
 <h3 class="LC20lb MBeuO DKV0Md">수지</h3>,
 <h3 class="LC20lb MBeuO DKV0Md">수지 (Suzy)- 아티스트채널 - 멜론</h3>,
 <h3 class="LC20lb MBeuO DKV0Md">수지 - 위키백과, 우리 모두의 백과사전</h3>,
 <h3 class="LC20lb MBeuO DKV0Md">"제가 죽어야겠네요"... 가수 수지, 연예인 동료들에 눈물 ...</h3>,
 <h3 class="LC20lb MBeuO DKV0Md">다시 아이돌 된 수지 “상처 많은 '이두나' 안아주고 싶어”</h3>,
 <h3 class="LC20lb MBeuO DKV0Md">수지구청</h3>]
url = 'https://search.naver.com/search.naver?where=nexearch&query=수지'
resp = get(url, headers={'user-agent':ua})
dom = BeautifulSoup(resp.text, 'html.parser')
for link in dom.select('.news_tit, .link_tit, .title_link'):
    print(link.attrs['href'])
    print(link.get_text())
http://www.newsis.com/view/?id=NISX20240313_0002658567&cID=10803&pID=14000
용인수지중앙공원에 56억 들여 4km 건강 숲길 조성
https://www.nocutnews.co.kr/news/6111089
1월 재정수지 8.3조 흑자로 출발
https://www.hankyung.com/article/202403149394i
새해 첫 달 총수입 67.1조…관리재정수지는 작년 보다 개선
https://www.yna.co.kr/view/AKR20240308020000002?input=1195m
1월 경상수지 30.5억달러 흑자…반도체 등 수출 회복
http://www.msoopent.com/page/page.html?acode=S61703024&mcd=02
SUZY - 매니지먼트숲
https://www.instagram.com/SKUUKZKY/
숮이 💄💅👡👠🎀👙🌂👗🌂🎀💋💌(@skuukzky) • Instagram 사진 및 동영상
https://namu.wiki/w/%EC%88%98%EC%A7%80(1994)
수지(1994) - 나무위키
https://ko.wikipedia.org/wiki/%EC%88%98%EC%A7%80_(1994%EB%85%84)
수지 (1994년) - 위키백과 한국
https://twitter.com/Suzy
숮이 | 트위터
https://blog.naver.com/ysgmt/223381090169
옷 마저 잘 입는 수지! 패션 / 사복 / 사진 모음
https://post.naver.com/viewer/postView.naver?volumeNo=37187263&memberNo=34005890&vType=VERTICAL
영화 속 첫사랑 비주얼 완성한 수지의 이 헤어스타일!
https://blog.naver.com/bap1234/223373547962
2024.02 수지 : 1일 인스타그램 업데이트
https://blog.naver.com/tngus8595/223374277803
내가 사랑해 마지않는, 아티스트 수지
https://blog.naver.com/nadiatear0/223318617931
박진영 만난 페이 근황 (지아 반응) 미쓰에이 절대 재결합 못하는 이유=수지
https://blog.naver.com/pooh0512/223241771244
완벽히 물오른 수지 로맨스 드라마 : 넷플릭스 이두나 심쿵할 포인트 3 / EP.02
https://blog.naver.com/travis88/223325700661
수지의 멘탈, 그녀가 롱런하는 이유
len(dom.select('.news_tit, .link_tit, .title_link'))
16
url = 'https://search.daum.net/search?w=tot&q=수지'
resp = get(url, headers={'user-agent':ua})
dom = BeautifulSoup(resp.text, 'html.parser')
for link in dom.select('c-doc-web > c-title'):
    print(link.attrs['data-href'])
    print(link.contents)
https://namu.wiki/w/%EC%88%98%EC%A7%80(1994)
[<b>수지</b>, '(1994) - 나무위키']
http://100.daum.net/encyclopedia/view/156XX58614229
[<b>수지</b>]
https://v.daum.net/v/G5cJFpwMiO
['[패션엔 ] ', <b>수지</b>, ', 군더더기 없는 데님 핏! 우아하고 힙하게 돌아온 첫사랑의 봄 데님룩 - 콘텐츠뷰']
https://cafe.daum.net/SoulDresser/FLTB/789653?q=%EC%88%98%EC%A7%80&re=1
[<b>수지</b>, ' 친언니가 찍어준 ', <b>수지</b>, ' 사진들']
https://table.cafe.daum.net/p/1000055110/191990178113321472
['김우빈X', <b>수지</b>, ', 김은숙X이병헌 신작 ‘다 이루어질지니’ 캐스팅 [공식]']
https://news.zum.com/articles/89218905
['1월 경상', <b>수지</b>, ' 30.5억달러 흑자...상품', <b>수지</b>, ' 10개월 연속 흑자(상보)']
https://table.cafe.daum.net/p/1019101842/241378098830594304
['한국 무역', <b>수지</b>, ' 세계 200등? ㅠㅠ']
https://blog.naver.com/efro800417/223374449319
['<임장. 용인의 강남! ', <b>수지</b>, '>']
https://brunch.co.kr/@10f57453fee84e4/244
['국제', <b>수지</b>, '와 가계', <b>수지</b>]
https://jumpintotheworld.tistory.com/161
['방아쇠 ', <b>수지</b>, ' 증후군 치료법 원인 증상']
http://story.kakao.com/_AYkA23/0KvhtwYAwd0
[<b>수지</b>, ' - 카카오스토리']
# robots.txt 확인
# 400번대, 500번대 분기
from requests.compat import urlparse

def canFetch(url, path):
    resp = get(urljoin(url, '/robots.txt'))
    
    if resp.status_code != 200:
        return True
    
    disallowList = [link for link in re.findall(r'Disallow\s*:\s*(.+)$',
                                                resp.text, re.IGNORECASE|re.MULTILINE)]
    
    if urlparse(path).path in disallowList:
        return False

    return True

canFetch('https://www.google.com', '/search/about')
True
from requests import request
from requests.exceptions import HTTPError
from time import sleep

def download(url, params={}, data={}, headers={}, method='GET', retries=3):
    if not canFetch(url, url):
        print('수집하면 안됨')
    
    resp = request(method, url, params=params, data=data, headers=headers)
    
    try:
        resp.raise_for_status()
    except HTTPError as e:
        if 499 < resp.status_code and retries > 0:
            print('재시도 중')
            sleep(5)
            return download(url, params, data, headers, method, retries-1)
        else:
            print(e.response.status_code)
            print(e.request.headers)
            print(e.response.headers)
            return None
        
    return resp

download('https://httpbin.org/status/500')
재시도 중
재시도 중
재시도 중
500
{'User-Agent': 'python-requests/2.31.0', 'Accept-Encoding': 'gzip, deflate, br', 'Accept': '*/*', 'Connection': 'keep-alive'}
{'Date': 'Thu, 14 Mar 2024 02:20:10 GMT', 'Content-Type': 'text/html; charset=utf-8', 'Content-Length': '0', 'Connection': 'keep-alive', 'Server': 'gunicorn/19.9.0', 'Access-Control-Allow-Origin': '*', 'Access-Control-Allow-Credentials': 'true'}
download('https://www.google.com/search?q=수지')
수집하면 안됨
<Response [200]>
# Crawler ; link를 따라다니는 애
1. link를 찾아야 됨 => link
   a[href], form[action], iframe[src] ///// img[src], video/audio[src], css/js[src]
2. 따라다니게끔 만들어야 함 => 전략
   WWW => X(Search space 알 수 없음) => Tree 탐색 DFS, BFS
   DFS; Stack; FILO
   BFS; Queue; FIFO
   Focused
3. 새롭게 찾은 link 검사(absolute path) 정규화 => 규칙
   javascript:, #, ., .., / => urljoin
4. 새롭게 찾은 link 방문한적이 있는지 검사 => 관리
   seen? => list => DB(ORM)
           root;검색결과
    a:블로그A          b:블로그B
c      d         e       f
Queue; root-a-b-c-d-e-f
Stack; root-a-b-e-f-c-d
List=>간단하게
URLs = ['http://inisw.kr']
Visited = []

while URLs:
    url = URLs.pop(-1) # 0:FIFO;Queue, -1:LIFO:FILO;Stack
    
    Visited.append(url)
    
    resp = download(url)
    
    if resp is None:
        continue
        
    if re.search(r'text\/html', resp.headers['content-type']):
        dom = BeautifulSoup(resp.text, 'html.parser')
        
        for link in dom.select('''a[href], frame[src], iframe[src], img[src],
        audio[src], video[src], style[src], link[src]'''):
            newURL = link.attrs['href'] if link.has_attr('href') else link.attrs['src']
            
            if not re.match(r'#|javascript|mailto', newURL):
                normalizedURL = urljoin(url, newURL)
                if normalizedURL not in Visited and \
                   normalizedURL not in URLs:
                    URLs.append(normalizedURL)
                    
    print(len(URLs), len(Visited))
1 1
24 2
675 3
680 4
679 5
678 6
677 7
676 8
692 9
691 10
690 11
689 12
688 13
687 14
686 15
687 16
692 17
697 18
---------------------------------------------------------------------------
KeyboardInterrupt                         Traceback (most recent call last)
Cell In[208], line 9
      5 url = URLs.pop(-1) # 0:FIFO;Queue, -1:LIFO:FILO;Stack
      7 Visited.append(url)
----> 9 resp = download(url)
     11 if resp is None:
     12     continue

Cell In[190], line 9, in download(url, params, data, headers, method, retries)
      6 if not canFetch(url, url):
      7     print('수집하면 안됨')
----> 9 resp = request(method, url, params=params, data=data, headers=headers)
     11 try:
     12     resp.raise_for_status()

File /opt/homebrew/anaconda3/lib/python3.11/site-packages/requests/api.py:59, in request(method, url, **kwargs)
     55 # By using the 'with' statement we are sure the session is closed, thus we
     56 # avoid leaving sockets open which can trigger a ResourceWarning in some
     57 # cases, and look like a memory leak in others.
     58 with sessions.Session() as session:
---> 59     return session.request(method=method, url=url, **kwargs)

File /opt/homebrew/anaconda3/lib/python3.11/site-packages/requests/sessions.py:589, in Session.request(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)
    584 send_kwargs = {
    585     "timeout": timeout,
    586     "allow_redirects": allow_redirects,
    587 }
    588 send_kwargs.update(settings)
--> 589 resp = self.send(prep, **send_kwargs)
    591 return resp

File /opt/homebrew/anaconda3/lib/python3.11/site-packages/requests/sessions.py:747, in Session.send(self, request, **kwargs)
    744         pass
    746 if not stream:
--> 747     r.content
    749 return r

File /opt/homebrew/anaconda3/lib/python3.11/site-packages/requests/models.py:899, in Response.content(self)
    897         self._content = None
    898     else:
--> 899         self._content = b"".join(self.iter_content(CONTENT_CHUNK_SIZE)) or b""
    901 self._content_consumed = True
    902 # don't need to release the connection; that's been handled by urllib3
    903 # since we exhausted the data.

File /opt/homebrew/anaconda3/lib/python3.11/site-packages/requests/models.py:816, in Response.iter_content.<locals>.generate()
    814 if hasattr(self.raw, "stream"):
    815     try:
--> 816         yield from self.raw.stream(chunk_size, decode_content=True)
    817     except ProtocolError as e:
    818         raise ChunkedEncodingError(e)

File /opt/homebrew/anaconda3/lib/python3.11/site-packages/urllib3/response.py:624, in HTTPResponse.stream(self, amt, decode_content)
    608 """
    609 A generator wrapper for the read() method. A call will block until
    610 ``amt`` bytes have been read from the connection or until the
   (...)
    621     'content-encoding' header.
    622 """
    623 if self.chunked and self.supports_chunked_reads():
--> 624     for line in self.read_chunked(amt, decode_content=decode_content):
    625         yield line
    626 else:

File /opt/homebrew/anaconda3/lib/python3.11/site-packages/urllib3/response.py:828, in HTTPResponse.read_chunked(self, amt, decode_content)
    825     return
    827 while True:
--> 828     self._update_chunk_length()
    829     if self.chunk_left == 0:
    830         break

File /opt/homebrew/anaconda3/lib/python3.11/site-packages/urllib3/response.py:758, in HTTPResponse._update_chunk_length(self)
    756 if self.chunk_left is not None:
    757     return
--> 758 line = self._fp.fp.readline()
    759 line = line.split(b";", 1)[0]
    760 try:

File /opt/homebrew/anaconda3/lib/python3.11/socket.py:706, in SocketIO.readinto(self, b)
    704 while True:
    705     try:
--> 706         return self._sock.recv_into(b)
    707     except timeout:
    708         self._timeout_occurred = True

File /opt/homebrew/anaconda3/lib/python3.11/ssl.py:1311, in SSLSocket.recv_into(self, buffer, nbytes, flags)
   1307     if flags != 0:
   1308         raise ValueError(
   1309           "non-zero flags not allowed in calls to recv_into() on %s" %
   1310           self.__class__)
-> 1311     return self.read(nbytes, buffer)
   1312 else:
   1313     return super().recv_into(buffer, nbytes, flags)

File /opt/homebrew/anaconda3/lib/python3.11/ssl.py:1167, in SSLSocket.read(self, len, buffer)
   1165 try:
   1166     if buffer is not None:
-> 1167         return self._sslobj.read(len, buffer)
   1168     else:
   1169         return self._sslobj.read(len)

KeyboardInterrupt: 
```