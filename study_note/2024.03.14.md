## 1. Bytes -> decode -> Str -> json.loads -> Python obj(Dict)

## 2. html parser -> dom BS4 -> element.Tag => find, find_all, find_parent, find_prev_sibling, find_next_sibling

## 3. dom.select

## 4. polymorphism : ë‹¤í˜•ì„± => ê°™ì€ ì´ë¦„ì¸ë° ë‹¤ë¥´ê²Œ ì²˜ë¦¬í•˜ëŠ” ê²ƒ. => like override => ë’¤ì§‘ì–´ ì“°ëŠ” ê²ƒ. (ìƒì†ê³¼ ê´€ë ¨)

## 5. overloading => ìƒì†ê³¼ ê´€ë ¨ì—†ëŠ” ê²ƒ. like operator overloading, function overloading (íŒŒì´ì¬ì€ ê¸°ë³¸ì ìœ¼ë¡œ ì§€ì›í•˜ì§€ ì•ŠìŒ)

## 6. function overloading : ì¸ì ê°’ì˜ ê°¯ìˆ˜ì— ë”°ë¼ ë‹¤ë¥´ê²Œ í–‰ë™. íŒŒì´ì¬ì€ ë’¤ì—ìœ¼ë¯€ë¡œ ê¸°ë³¸ì ìœ¼ë¡œ ì§€ì› X

## 7. def x (a,b = None):
        print('')

=> default technic

## 8. class A (object, metaclass = type): 

## 9. type ì€ í´ë˜ìŠ¤ íƒ€ì…ì„ ë°˜í™˜í•´ì¤€ë‹¤.

## 10. typeì€ ìƒˆë¡œìš´ íƒ€ì…ì„ ë§Œë“¤ì–´ì¤€ë‹¤.

## 11. ê´€ì‹¬ì˜ ë¶„ë¦¬ => type ì€ í´ë˜ìŠ¤ë¥¼ ì¡°ì ˆí•´ì¤€ë‹¤.

## 12. abstract í´ë˜ìŠ¤ë¡œë¶€í„° ìƒì†=> abstract methodë¥¼ ë¶™ì´ì§€ ì•Šì•„ë„ ëœë‹¤.

## 13. ë©”íƒ€í´ë˜ìŠ¤ë¥¼ ìƒì† ë°›ëŠ”ë‹¤. => ë³€ìˆ˜ì— ë©”íƒ€í´ë˜ìŠ¤ë¥¼ ëŒ€ì…!!

## 14. ì‹±ê¸€í†¤ : ë‹¨ í•œê°œì˜ ì¸ìŠ¤í„´ìŠ¤ë§Œ ë§Œë“¤ ìˆ˜ ìˆëŠ” í´ë˜ìŠ¤

## 15. ë©”íƒ€ í´ë˜ìŠ¤ `__call__` -> í´ë˜ìŠ¤ì— ê´„í˜¸ ë¶™ì¼ ìˆ˜ ìˆë‹¤.

## 16. `__new__`: ë©”íƒ€í´ë˜ìŠ¤ë¥¼ í´ë˜ìŠ¤ì—ì„œ ì‚¬ìš©í•  ìˆ˜ ìˆê²Œ í•´ì¤Œ.

## 17. @abstract method => í•©ì„± í•¨ìˆ˜

```python

HTTP.response.body í•´ì„?
(Bytes) -> Content-type(MIME) ___/___; text/html, application/json
HTTP.request.header <= User-agent
Bytes->decode->Str; ì´ë•Œë¶€í„° ë¬¸ìì—´ ì²˜ë¦¬ ê°€ëŠ¥ => RE(ë¶ˆí¸)
Bytes->decode->Str->json.loads->Python obj(Dict)
HTML(Markup;Document;Semi-structured) -> HTML Parser -> DOM(Tree) <= BS4
element.Tag => find, find_all (ìì‹, ìì†, ë°‘ì— ë°©í–¥)
               find_parent, find_parents (ë¶€ëª¨, ì¡°ìƒ, ìœ„ì— ë°©í–¥)
               find_prev_sibling(s), find_next_sibling(s) (ë¶€ëª¨ë¥¼ ê³µìœ í•˜ëŠ” í˜•ì œë…¸ë“œ, ì´ì „, ì´í›„)
from requests import get

url = 'http://pythonscraping.com/pages/page3.html'
resp = get(url)
resp.status_code, resp.reason, resp.request.headers, resp.headers
(200,
 'OK',
 {'User-Agent': 'python-requests/2.31.0', 'Accept-Encoding': 'gzip, deflate, br', 'Accept': '*/*', 'Connection': 'keep-alive'},
 {'Server': 'nginx', 'Date': 'Thu, 14 Mar 2024 00:14:05 GMT', 'Content-Type': 'text/html', 'Last-Modified': 'Sat, 09 Jun 2018 19:15:59 GMT', 'Transfer-Encoding': 'chunked', 'Connection': 'keep-alive', 'ETag': 'W/"5b1c276f-965"', 'X-Powered-By': 'PleskLin', 'Content-Encoding': 'br'})
import re

re.search(r'text', resp.headers['content-type'])
<re.Match object; span=(0, 4), match='text'>
from bs4 import BeautifulSoup

dom = BeautifulSoup(resp.text, 'html.parser')
[tag.name for tag in dom.div.find_all(recursive=False)]

#                   DOM #document    [html.parser = lxml]
#       head                    body
#                                 div
#                      img   h1   div   table    div
#                        img   h1   div   table   p    div
['img', 'h1', 'div', 'table', 'div']
node = dom.find(string=re.compile(r'\$0\.50')).find_parent()
node.find_next_sibling().find('img') is not None
node.find_next_sibling().find('img').attrs['src']
'../img/gifts/img4.jpg'
from requests.compat import urljoin

url = urljoin(resp.request.url, node.find_next_sibling().find('img').attrs['src'])
resp = get(url)
resp.status_code, resp.reason, resp.request.headers, resp.headers
(200,
 'OK',
 {'User-Agent': 'python-requests/2.31.0', 'Accept-Encoding': 'gzip, deflate, br', 'Accept': '*/*', 'Connection': 'keep-alive'},
 {'Server': 'nginx', 'Date': 'Thu, 14 Mar 2024 00:35:41 GMT', 'Content-Type': 'image/jpeg', 'Content-Length': '85007', 'Last-Modified': 'Mon, 04 Aug 2014 00:49:04 GMT', 'Connection': 'keep-alive', 'ETag': '"53ded880-14c0f"', 'X-Powered-By': 'PleskLin', 'Accept-Ranges': 'bytes'})
resp.headers['content-type']
'image/jpeg'
fname = re.search(r'.+/(.+\.jpg)$', resp.url).group(1)
fp = open(fname, 'bw')
fp.write(resp.content)
fp.close()
[tag.name for tag in node.find_parents(limit=2)]
['tr', 'table']
node.find_parents(limit=2)[-1].name
'table'
node.find_parents(limit=2)[-1].find_next_sibling()
<div id="footer">
Â© Totally Normal Gifts, Inc. <br/>
+234 (617) 863-0736
</div>
dom.find(attrs={'id':'footer'})
<div id="footer">
Â© Totally Normal Gifts, Inc. <br/>
+234 (617) 863-0736
</div>
node.find_parents(limit=2)[-1].find_next_sibling() is dom.find(attrs={'id':'footer'})
True
dom.div.find_all(recursive=False)[-1] is dom.find(attrs={'id':'footer'})
True
<tag attributes=id, class> id => ê³ ìœ í•œ ê°’, class => ë‹¤ì¤‘ìƒì†
node.find_parents(limit=2)[-1].find_all(recursive=False)[0]
node.find_parents(limit=2)[-1].find().find_all(recursive=False)[1].get_text()
'\nDescription\n'
# tableì— ìˆëŠ” ì´ë¯¸ì§€ ì „ë¶€ë¥¼ ì°¾ëŠ” ë²•
dom.table.find_all('img')
dom.table.find_all(attrs={'src':re.compile(r'jpg$')})
for tag in dom.table.find_all(recursive=False)[1:]:
    print(tag.find_all(recursive=False)[-1].find())
for tag in node.find_parent().find_previous_siblings()[:-1]:
    print(tag.find_all()[-1])
for tag in node.find_parent().find_next_siblings():
    print(tag.find_all()[-1])
<img src="../img/gifts/img1.jpg"/>
<img src="../img/gifts/img2.jpg"/>
<img src="../img/gifts/img3.jpg"/>
<img src="../img/gifts/img4.jpg"/>
<img src="../img/gifts/img6.jpg"/>
<img src="../img/gifts/img3.jpg"/>
<img src="../img/gifts/img2.jpg"/>
<img src="../img/gifts/img1.jpg"/>
<img src="../img/gifts/img6.jpg"/>
# CSS Selector
íƒœê·¸ì´ë¦„ {ìŠ¤íƒ€ì¼}
      -------- ê´€ì‹¬ X
#ì•„ì´ë””
.í´ë˜ìŠ¤
.í´ë˜ìŠ¤A.í´ë˜ìŠ¤B
.í´ë˜ìŠ¤A .í´ë˜ìŠ¤B ìì†(ê³µë°±)
.í´ë˜ìŠ¤A > .í´ë˜ìŠ¤B ìì‹(>)
[ì†ì„±=ê°’]
íƒœê·¸ì´ë¦„, .í´ë˜ìŠ¤
ê°€ìƒì„ íƒì:first-child, last-child, nth-of-type, nth-child
dom.select('table img')
# dom.table.find_all('img')
dom.select('table img[src$="jpg"]')
# dom.table.find_all(attrs={'src':re.compile(r'jpg$')})
dom.select('table > tr:first-child ~ tr > *:nth-child(4) > :first-child')
dom.select('td:has(> img) > *')
# A ~ Z
# A ì´í›„ ë¶€í„° Z
# for tag in dom.table.find_all(recursive=False)[1:]:
#     print(tag.find_all(recursive=False)[-1].find())
# for tag in node.find_parent().find_previous_siblings()[:-1]:
#     print(tag.find_all()[-1])
# for tag in node.find_parent().find_next_siblings():
#     print(tag.find_all()[-1])
dom.select('tr > td:nth-of-type(4) > img')[0].find_parent()
<td>
<img src="../img/gifts/img1.jpg"/>
</td>
url = 'https://www.google.com/search?q=ìˆ˜ì§€'
ua = 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Whale/3.24.223.24 Safari/537.36'
resp = get(url, headers={'user-agent':ua})
# ì˜ ë°›ì•˜ëŠ”ì§€ í™•ì¸
dom = BeautifulSoup(resp.content, 'html.parser')
for tag in dom.select('a:has(> h3)'):
    print(tag.attrs['href'] if tag.has_attr('href') else None)
    print(tag.select_one('h3').get_text())
https://namu.wiki/w/%EC%88%98%EC%A7%80(1994)
ìˆ˜ì§€(1994)
https://ko.wikipedia.org/wiki/%EC%88%98%EC%A7%80_(1994%EB%85%84)
ìˆ˜ì§€ (1994ë…„) - ìœ„í‚¤ë°±ê³¼, ìš°ë¦¬ ëª¨ë‘ì˜ ë°±ê³¼ì‚¬ì „
https://namu.wiki/w/%EC%88%98%EC%A7%80
ìˆ˜ì§€
https://m2.melon.com/artist/song.htm?artistId=514741
ìˆ˜ì§€ (Suzy)- ì•„í‹°ìŠ¤íŠ¸ì±„ë„ - ë©œë¡ 
https://ko.wikipedia.org/wiki/%EC%88%98%EC%A7%80
ìˆ˜ì§€ - ìœ„í‚¤ë°±ê³¼, ìš°ë¦¬ ëª¨ë‘ì˜ ë°±ê³¼ì‚¬ì „
https://www.autotribune.co.kr/news/articleView.html?idxno=12365
"ì œê°€ ì£½ì–´ì•¼ê² ë„¤ìš”"... ê°€ìˆ˜ ìˆ˜ì§€, ì—°ì˜ˆì¸ ë™ë£Œë“¤ì— ëˆˆë¬¼ ...
https://www.hani.co.kr/arti/culture/culture_general/1112601.html
ë‹¤ì‹œ ì•„ì´ëŒ ëœ ìˆ˜ì§€ â€œìƒì²˜ ë§ì€ 'ì´ë‘ë‚˜' ì•ˆì•„ì£¼ê³  ì‹¶ì–´â€
https://www.sujigu.go.kr/
ìˆ˜ì§€êµ¬ì²­
/search?q=%EC%88%98%EC%A7%80&sca_esv=6470011d51d3dd33&ei=A1TyZYanKKe9vr0PmZa_uAM&start=10&sa=N
ê²°ê³¼ ë”ë³´ê¸°
None
ë‹¤ì‹œ ì‹œë„
for tag in dom.select('a:has(.LC20lb)'):
    print(tag.attrs['href'] if tag.has_attr('href') else None)
    print(tag.select_one('h3').get_text())
https://namu.wiki/w/%EC%88%98%EC%A7%80(1994)
ìˆ˜ì§€(1994)
https://ko.wikipedia.org/wiki/%EC%88%98%EC%A7%80_(1994%EB%85%84)
ìˆ˜ì§€ (1994ë…„) - ìœ„í‚¤ë°±ê³¼, ìš°ë¦¬ ëª¨ë‘ì˜ ë°±ê³¼ì‚¬ì „
https://namu.wiki/w/%EC%88%98%EC%A7%80
ìˆ˜ì§€
https://m2.melon.com/artist/song.htm?artistId=514741
ìˆ˜ì§€ (Suzy)- ì•„í‹°ìŠ¤íŠ¸ì±„ë„ - ë©œë¡ 
https://ko.wikipedia.org/wiki/%EC%88%98%EC%A7%80
ìˆ˜ì§€ - ìœ„í‚¤ë°±ê³¼, ìš°ë¦¬ ëª¨ë‘ì˜ ë°±ê³¼ì‚¬ì „
https://www.autotribune.co.kr/news/articleView.html?idxno=12365
"ì œê°€ ì£½ì–´ì•¼ê² ë„¤ìš”"... ê°€ìˆ˜ ìˆ˜ì§€, ì—°ì˜ˆì¸ ë™ë£Œë“¤ì— ëˆˆë¬¼ ...
https://www.hani.co.kr/arti/culture/culture_general/1112601.html
ë‹¤ì‹œ ì•„ì´ëŒ ëœ ìˆ˜ì§€ â€œìƒì²˜ ë§ì€ 'ì´ë‘ë‚˜' ì•ˆì•„ì£¼ê³  ì‹¶ì–´â€
https://www.sujigu.go.kr/
ìˆ˜ì§€êµ¬ì²­
for tag in dom.find_all(attrs={'class':re.compile(r'^LC20lb')}):
    print(tag.find_parent().attrs['href'])
    print(tag.get_text())
https://namu.wiki/w/%EC%88%98%EC%A7%80(1994)
ìˆ˜ì§€(1994)
https://ko.wikipedia.org/wiki/%EC%88%98%EC%A7%80_(1994%EB%85%84)
ìˆ˜ì§€ (1994ë…„) - ìœ„í‚¤ë°±ê³¼, ìš°ë¦¬ ëª¨ë‘ì˜ ë°±ê³¼ì‚¬ì „
https://namu.wiki/w/%EC%88%98%EC%A7%80
ìˆ˜ì§€
https://m2.melon.com/artist/song.htm?artistId=514741
ìˆ˜ì§€ (Suzy)- ì•„í‹°ìŠ¤íŠ¸ì±„ë„ - ë©œë¡ 
https://ko.wikipedia.org/wiki/%EC%88%98%EC%A7%80
ìˆ˜ì§€ - ìœ„í‚¤ë°±ê³¼, ìš°ë¦¬ ëª¨ë‘ì˜ ë°±ê³¼ì‚¬ì „
https://www.autotribune.co.kr/news/articleView.html?idxno=12365
"ì œê°€ ì£½ì–´ì•¼ê² ë„¤ìš”"... ê°€ìˆ˜ ìˆ˜ì§€, ì—°ì˜ˆì¸ ë™ë£Œë“¤ì— ëˆˆë¬¼ ...
https://www.hani.co.kr/arti/culture/culture_general/1112601.html
ë‹¤ì‹œ ì•„ì´ëŒ ëœ ìˆ˜ì§€ â€œìƒì²˜ ë§ì€ 'ì´ë‘ë‚˜' ì•ˆì•„ì£¼ê³  ì‹¶ì–´â€
https://www.sujigu.go.kr/
ìˆ˜ì§€êµ¬ì²­
dom.select('div > div > span > a > h3')
[<h3 class="LC20lb MBeuO DKV0Md">ìˆ˜ì§€(1994)</h3>,
 <h3 class="LC20lb MBeuO DKV0Md">ìˆ˜ì§€ (1994ë…„) - ìœ„í‚¤ë°±ê³¼, ìš°ë¦¬ ëª¨ë‘ì˜ ë°±ê³¼ì‚¬ì „</h3>,
 <h3 class="LC20lb MBeuO DKV0Md">ìˆ˜ì§€</h3>,
 <h3 class="LC20lb MBeuO DKV0Md">ìˆ˜ì§€ (Suzy)- ì•„í‹°ìŠ¤íŠ¸ì±„ë„ - ë©œë¡ </h3>,
 <h3 class="LC20lb MBeuO DKV0Md">ìˆ˜ì§€ - ìœ„í‚¤ë°±ê³¼, ìš°ë¦¬ ëª¨ë‘ì˜ ë°±ê³¼ì‚¬ì „</h3>,
 <h3 class="LC20lb MBeuO DKV0Md">"ì œê°€ ì£½ì–´ì•¼ê² ë„¤ìš”"... ê°€ìˆ˜ ìˆ˜ì§€, ì—°ì˜ˆì¸ ë™ë£Œë“¤ì— ëˆˆë¬¼ ...</h3>,
 <h3 class="LC20lb MBeuO DKV0Md">ë‹¤ì‹œ ì•„ì´ëŒ ëœ ìˆ˜ì§€ â€œìƒì²˜ ë§ì€ 'ì´ë‘ë‚˜' ì•ˆì•„ì£¼ê³  ì‹¶ì–´â€</h3>,
 <h3 class="LC20lb MBeuO DKV0Md">ìˆ˜ì§€êµ¬ì²­</h3>]
url = 'https://search.naver.com/search.naver?where=nexearch&query=ìˆ˜ì§€'
resp = get(url, headers={'user-agent':ua})
dom = BeautifulSoup(resp.text, 'html.parser')
for link in dom.select('.news_tit, .link_tit, .title_link'):
    print(link.attrs['href'])
    print(link.get_text())
http://www.newsis.com/view/?id=NISX20240313_0002658567&cID=10803&pID=14000
ìš©ì¸ìˆ˜ì§€ì¤‘ì•™ê³µì›ì— 56ì–µ ë“¤ì—¬ 4km ê±´ê°• ìˆ²ê¸¸ ì¡°ì„±
https://www.nocutnews.co.kr/news/6111089
1ì›” ì¬ì •ìˆ˜ì§€ 8.3ì¡° í‘ìë¡œ ì¶œë°œ
https://www.hankyung.com/article/202403149394i
ìƒˆí•´ ì²« ë‹¬ ì´ìˆ˜ì… 67.1ì¡°â€¦ê´€ë¦¬ì¬ì •ìˆ˜ì§€ëŠ” ì‘ë…„ ë³´ë‹¤ ê°œì„ 
https://www.yna.co.kr/view/AKR20240308020000002?input=1195m
1ì›” ê²½ìƒìˆ˜ì§€ 30.5ì–µë‹¬ëŸ¬ í‘ìâ€¦ë°˜ë„ì²´ ë“± ìˆ˜ì¶œ íšŒë³µ
http://www.msoopent.com/page/page.html?acode=S61703024&mcd=02
SUZY - ë§¤ë‹ˆì§€ë¨¼íŠ¸ìˆ²
https://www.instagram.com/SKUUKZKY/
ìˆ®ì´ ğŸ’„ğŸ’…ğŸ‘¡ğŸ‘ ğŸ€ğŸ‘™ğŸŒ‚ğŸ‘—ğŸŒ‚ğŸ€ğŸ’‹ğŸ’Œ(@skuukzky) â€¢ Instagram ì‚¬ì§„ ë° ë™ì˜ìƒ
https://namu.wiki/w/%EC%88%98%EC%A7%80(1994)
ìˆ˜ì§€(1994) - ë‚˜ë¬´ìœ„í‚¤
https://ko.wikipedia.org/wiki/%EC%88%98%EC%A7%80_(1994%EB%85%84)
ìˆ˜ì§€ (1994ë…„) - ìœ„í‚¤ë°±ê³¼ í•œêµ­
https://twitter.com/Suzy
ìˆ®ì´ | íŠ¸ìœ„í„°
https://blog.naver.com/ysgmt/223381090169
ì˜· ë§ˆì € ì˜ ì…ëŠ” ìˆ˜ì§€! íŒ¨ì…˜ / ì‚¬ë³µ / ì‚¬ì§„ ëª¨ìŒ
https://post.naver.com/viewer/postView.naver?volumeNo=37187263&memberNo=34005890&vType=VERTICAL
ì˜í™” ì† ì²«ì‚¬ë‘ ë¹„ì£¼ì–¼ ì™„ì„±í•œ ìˆ˜ì§€ì˜ ì´ í—¤ì–´ìŠ¤íƒ€ì¼!
https://blog.naver.com/bap1234/223373547962
2024.02 ìˆ˜ì§€ : 1ì¼ ì¸ìŠ¤íƒ€ê·¸ë¨ ì—…ë°ì´íŠ¸
https://blog.naver.com/tngus8595/223374277803
ë‚´ê°€ ì‚¬ë‘í•´ ë§ˆì§€ì•ŠëŠ”, ì•„í‹°ìŠ¤íŠ¸ ìˆ˜ì§€
https://blog.naver.com/nadiatear0/223318617931
ë°•ì§„ì˜ ë§Œë‚œ í˜ì´ ê·¼í™© (ì§€ì•„ ë°˜ì‘) ë¯¸ì“°ì—ì´ ì ˆëŒ€ ì¬ê²°í•© ëª»í•˜ëŠ” ì´ìœ =ìˆ˜ì§€
https://blog.naver.com/pooh0512/223241771244
ì™„ë²½íˆ ë¬¼ì˜¤ë¥¸ ìˆ˜ì§€ ë¡œë§¨ìŠ¤ ë“œë¼ë§ˆ : ë„·í”Œë¦­ìŠ¤ ì´ë‘ë‚˜ ì‹¬ì¿µí•  í¬ì¸íŠ¸ 3 / EP.02
https://blog.naver.com/travis88/223325700661
ìˆ˜ì§€ì˜ ë©˜íƒˆ, ê·¸ë…€ê°€ ë¡±ëŸ°í•˜ëŠ” ì´ìœ 
len(dom.select('.news_tit, .link_tit, .title_link'))
16
url = 'https://search.daum.net/search?w=tot&q=ìˆ˜ì§€'
resp = get(url, headers={'user-agent':ua})
dom = BeautifulSoup(resp.text, 'html.parser')
for link in dom.select('c-doc-web > c-title'):
    print(link.attrs['data-href'])
    print(link.contents)
https://namu.wiki/w/%EC%88%98%EC%A7%80(1994)
[<b>ìˆ˜ì§€</b>, '(1994) - ë‚˜ë¬´ìœ„í‚¤']
http://100.daum.net/encyclopedia/view/156XX58614229
[<b>ìˆ˜ì§€</b>]
https://v.daum.net/v/G5cJFpwMiO
['[íŒ¨ì…˜ì—” ] ', <b>ìˆ˜ì§€</b>, ', êµ°ë”ë”ê¸° ì—†ëŠ” ë°ë‹˜ í•! ìš°ì•„í•˜ê³  í™í•˜ê²Œ ëŒì•„ì˜¨ ì²«ì‚¬ë‘ì˜ ë´„ ë°ë‹˜ë£© - ì½˜í…ì¸ ë·°']
https://cafe.daum.net/SoulDresser/FLTB/789653?q=%EC%88%98%EC%A7%80&re=1
[<b>ìˆ˜ì§€</b>, ' ì¹œì–¸ë‹ˆê°€ ì°ì–´ì¤€ ', <b>ìˆ˜ì§€</b>, ' ì‚¬ì§„ë“¤']
https://table.cafe.daum.net/p/1000055110/191990178113321472
['ê¹€ìš°ë¹ˆX', <b>ìˆ˜ì§€</b>, ', ê¹€ì€ìˆ™Xì´ë³‘í—Œ ì‹ ì‘ â€˜ë‹¤ ì´ë£¨ì–´ì§ˆì§€ë‹ˆâ€™ ìºìŠ¤íŒ… [ê³µì‹]']
https://news.zum.com/articles/89218905
['1ì›” ê²½ìƒ', <b>ìˆ˜ì§€</b>, ' 30.5ì–µë‹¬ëŸ¬ í‘ì...ìƒí’ˆ', <b>ìˆ˜ì§€</b>, ' 10ê°œì›” ì—°ì† í‘ì(ìƒë³´)']
https://table.cafe.daum.net/p/1019101842/241378098830594304
['í•œêµ­ ë¬´ì—­', <b>ìˆ˜ì§€</b>, ' ì„¸ê³„ 200ë“±? ã… ã… ']
https://blog.naver.com/efro800417/223374449319
['<ì„ì¥. ìš©ì¸ì˜ ê°•ë‚¨! ', <b>ìˆ˜ì§€</b>, '>']
https://brunch.co.kr/@10f57453fee84e4/244
['êµ­ì œ', <b>ìˆ˜ì§€</b>, 'ì™€ ê°€ê³„', <b>ìˆ˜ì§€</b>]
https://jumpintotheworld.tistory.com/161
['ë°©ì•„ì‡  ', <b>ìˆ˜ì§€</b>, ' ì¦í›„êµ° ì¹˜ë£Œë²• ì›ì¸ ì¦ìƒ']
http://story.kakao.com/_AYkA23/0KvhtwYAwd0
[<b>ìˆ˜ì§€</b>, ' - ì¹´ì¹´ì˜¤ìŠ¤í† ë¦¬']
# robots.txt í™•ì¸
# 400ë²ˆëŒ€, 500ë²ˆëŒ€ ë¶„ê¸°
from requests.compat import urlparse

def canFetch(url, path):
    resp = get(urljoin(url, '/robots.txt'))
    
    if resp.status_code != 200:
        return True
    
    disallowList = [link for link in re.findall(r'Disallow\s*:\s*(.+)$',
                                                resp.text, re.IGNORECASE|re.MULTILINE)]
    
    if urlparse(path).path in disallowList:
        return False

    return True

canFetch('https://www.google.com', '/search/about')
True
from requests import request
from requests.exceptions import HTTPError
from time import sleep

def download(url, params={}, data={}, headers={}, method='GET', retries=3):
    if not canFetch(url, url):
        print('ìˆ˜ì§‘í•˜ë©´ ì•ˆë¨')
    
    resp = request(method, url, params=params, data=data, headers=headers)
    
    try:
        resp.raise_for_status()
    except HTTPError as e:
        if 499 < resp.status_code and retries > 0:
            print('ì¬ì‹œë„ ì¤‘')
            sleep(5)
            return download(url, params, data, headers, method, retries-1)
        else:
            print(e.response.status_code)
            print(e.request.headers)
            print(e.response.headers)
            return None
        
    return resp

download('https://httpbin.org/status/500')
ì¬ì‹œë„ ì¤‘
ì¬ì‹œë„ ì¤‘
ì¬ì‹œë„ ì¤‘
500
{'User-Agent': 'python-requests/2.31.0', 'Accept-Encoding': 'gzip, deflate, br', 'Accept': '*/*', 'Connection': 'keep-alive'}
{'Date': 'Thu, 14 Mar 2024 02:20:10 GMT', 'Content-Type': 'text/html; charset=utf-8', 'Content-Length': '0', 'Connection': 'keep-alive', 'Server': 'gunicorn/19.9.0', 'Access-Control-Allow-Origin': '*', 'Access-Control-Allow-Credentials': 'true'}
download('https://www.google.com/search?q=ìˆ˜ì§€')
ìˆ˜ì§‘í•˜ë©´ ì•ˆë¨
<Response [200]>
# Crawler ; linkë¥¼ ë”°ë¼ë‹¤ë‹ˆëŠ” ì• 
1. linkë¥¼ ì°¾ì•„ì•¼ ë¨ => link
   a[href], form[action], iframe[src] ///// img[src], video/audio[src], css/js[src]
2. ë”°ë¼ë‹¤ë‹ˆê²Œë” ë§Œë“¤ì–´ì•¼ í•¨ => ì „ëµ
   WWW => X(Search space ì•Œ ìˆ˜ ì—†ìŒ) => Tree íƒìƒ‰ DFS, BFS
   DFS; Stack; FILO
   BFS; Queue; FIFO
   Focused
3. ìƒˆë¡­ê²Œ ì°¾ì€ link ê²€ì‚¬(absolute path) ì •ê·œí™” => ê·œì¹™
   javascript:, #, ., .., / => urljoin
4. ìƒˆë¡­ê²Œ ì°¾ì€ link ë°©ë¬¸í•œì ì´ ìˆëŠ”ì§€ ê²€ì‚¬ => ê´€ë¦¬
   seen? => list => DB(ORM)
           root;ê²€ìƒ‰ê²°ê³¼
    a:ë¸”ë¡œê·¸A          b:ë¸”ë¡œê·¸B
c      d         e       f
Queue; root-a-b-c-d-e-f
Stack; root-a-b-e-f-c-d
List=>ê°„ë‹¨í•˜ê²Œ
URLs = ['http://inisw.kr']
Visited = []

while URLs:
    url = URLs.pop(-1) # 0:FIFO;Queue, -1:LIFO:FILO;Stack
    
    Visited.append(url)
    
    resp = download(url)
    
    if resp is None:
        continue
        
    if re.search(r'text\/html', resp.headers['content-type']):
        dom = BeautifulSoup(resp.text, 'html.parser')
        
        for link in dom.select('''a[href], frame[src], iframe[src], img[src],
        audio[src], video[src], style[src], link[src]'''):
            newURL = link.attrs['href'] if link.has_attr('href') else link.attrs['src']
            
            if not re.match(r'#|javascript|mailto', newURL):
                normalizedURL = urljoin(url, newURL)
                if normalizedURL not in Visited and \
                   normalizedURL not in URLs:
                    URLs.append(normalizedURL)
                    
    print(len(URLs), len(Visited))
1 1
24 2
675 3
680 4
679 5
678 6
677 7
676 8
692 9
691 10
690 11
689 12
688 13
687 14
686 15
687 16
692 17
697 18
---------------------------------------------------------------------------
KeyboardInterrupt                         Traceback (most recent call last)
Cell In[208], line 9
      5 url = URLs.pop(-1) # 0:FIFO;Queue, -1:LIFO:FILO;Stack
      7 Visited.append(url)
----> 9 resp = download(url)
     11 if resp is None:
     12     continue

Cell In[190], line 9, in download(url, params, data, headers, method, retries)
      6 if not canFetch(url, url):
      7     print('ìˆ˜ì§‘í•˜ë©´ ì•ˆë¨')
----> 9 resp = request(method, url, params=params, data=data, headers=headers)
     11 try:
     12     resp.raise_for_status()

File /opt/homebrew/anaconda3/lib/python3.11/site-packages/requests/api.py:59, in request(method, url, **kwargs)
     55 # By using the 'with' statement we are sure the session is closed, thus we
     56 # avoid leaving sockets open which can trigger a ResourceWarning in some
     57 # cases, and look like a memory leak in others.
     58 with sessions.Session() as session:
---> 59     return session.request(method=method, url=url, **kwargs)

File /opt/homebrew/anaconda3/lib/python3.11/site-packages/requests/sessions.py:589, in Session.request(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)
    584 send_kwargs = {
    585     "timeout": timeout,
    586     "allow_redirects": allow_redirects,
    587 }
    588 send_kwargs.update(settings)
--> 589 resp = self.send(prep, **send_kwargs)
    591 return resp

File /opt/homebrew/anaconda3/lib/python3.11/site-packages/requests/sessions.py:747, in Session.send(self, request, **kwargs)
    744         pass
    746 if not stream:
--> 747     r.content
    749 return r

File /opt/homebrew/anaconda3/lib/python3.11/site-packages/requests/models.py:899, in Response.content(self)
    897         self._content = None
    898     else:
--> 899         self._content = b"".join(self.iter_content(CONTENT_CHUNK_SIZE)) or b""
    901 self._content_consumed = True
    902 # don't need to release the connection; that's been handled by urllib3
    903 # since we exhausted the data.

File /opt/homebrew/anaconda3/lib/python3.11/site-packages/requests/models.py:816, in Response.iter_content.<locals>.generate()
    814 if hasattr(self.raw, "stream"):
    815     try:
--> 816         yield from self.raw.stream(chunk_size, decode_content=True)
    817     except ProtocolError as e:
    818         raise ChunkedEncodingError(e)

File /opt/homebrew/anaconda3/lib/python3.11/site-packages/urllib3/response.py:624, in HTTPResponse.stream(self, amt, decode_content)
    608 """
    609 A generator wrapper for the read() method. A call will block until
    610 ``amt`` bytes have been read from the connection or until the
   (...)
    621     'content-encoding' header.
    622 """
    623 if self.chunked and self.supports_chunked_reads():
--> 624     for line in self.read_chunked(amt, decode_content=decode_content):
    625         yield line
    626 else:

File /opt/homebrew/anaconda3/lib/python3.11/site-packages/urllib3/response.py:828, in HTTPResponse.read_chunked(self, amt, decode_content)
    825     return
    827 while True:
--> 828     self._update_chunk_length()
    829     if self.chunk_left == 0:
    830         break

File /opt/homebrew/anaconda3/lib/python3.11/site-packages/urllib3/response.py:758, in HTTPResponse._update_chunk_length(self)
    756 if self.chunk_left is not None:
    757     return
--> 758 line = self._fp.fp.readline()
    759 line = line.split(b";", 1)[0]
    760 try:

File /opt/homebrew/anaconda3/lib/python3.11/socket.py:706, in SocketIO.readinto(self, b)
    704 while True:
    705     try:
--> 706         return self._sock.recv_into(b)
    707     except timeout:
    708         self._timeout_occurred = True

File /opt/homebrew/anaconda3/lib/python3.11/ssl.py:1311, in SSLSocket.recv_into(self, buffer, nbytes, flags)
   1307     if flags != 0:
   1308         raise ValueError(
   1309           "non-zero flags not allowed in calls to recv_into() on %s" %
   1310           self.__class__)
-> 1311     return self.read(nbytes, buffer)
   1312 else:
   1313     return super().recv_into(buffer, nbytes, flags)

File /opt/homebrew/anaconda3/lib/python3.11/ssl.py:1167, in SSLSocket.read(self, len, buffer)
   1165 try:
   1166     if buffer is not None:
-> 1167         return self._sslobj.read(len, buffer)
   1168     else:
   1169         return self._sslobj.read(len)

KeyboardInterrupt: 
```