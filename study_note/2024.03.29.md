```python
[Tokenizing; Feature Extraction]
split, splitlines
sent_tokenize
word_tokenize, regex_tokenize, tweettokenize

morpheme analyze, POS tagger, Nouns Extractor => tagset

Ngram -> Tokenizer(collocation), Language Model(P(t+1|t1...t))
                                 => 띄어쓰기, 다음단어예측
Branch Entropy, Perplexity(Cohesion score) => subword

BPE => WPM => SPM

[Normalization; Feature Selection]
Punk? => ', ., => format
case, stopwords(most frequently..) => Zipf
      => 한글?  [to be or not to be] => X, ngram(명사군+은,는,이,가...)
      => 메시지, 욕설 필터링
        
[Edit Distance]
아녕하세요 => 안녕+하세요
Hamming, Levin => dkssud, 골려대

다음주
Naive Bayes, Regression(Linear, Logisitic), Gradient Method(Asc, Desc)
P(D|theta), Neural Network(ANN, Perceptron, SLP)
            Layer(Weight), Output(Loss-J; Binary, Multinomial,
                                  Softmax, Activate Func.), MLP, DNN
# stopwords(most frequently..) => Zipf
#       => 한글?  [to be or not to be] => X, ngram(명사군+은,는,이,가...)
#       => 메시지, 욕설 필터링
from konlpy.tag import Hannanum, Kkma, Komoran, Okt

ma = list()
ma.append(Hannanum())
ma.append(Kkma())
ma.append(Komoran())
ma.append(Okt())
d = '''
아마존은 지난 27일(현지시간) 앤스로픽에 27억5000만달러(약 3조7000억원)를 추가로 투자한다고 발표했다. 아마존이 앤스로픽에 투자한 총 금액은 지난해 9월 단행한 12억5000만달러(약 1조6700억원) 투자를 포함해 40억달러(약 5조4000억원)에 이른다.

이는 아마존 역사상 최대 규모의 외부 투자이면서, 앤스로픽이 유치한 총 투자 금액(73억달러·약 9조8600억원)의 55%에 달하는 수준이다. 이를 통해 아마존은 앤스로픽 지분 일부도 보유하게 됐다. 다만 이사회에는 참여하지 않기로 했다.
'''
from collections import Counter

dc = Counter([pos[1] for pos in ma[0].pos(d)])
dc
Counter({'N': 44, 'J': 23, 'E': 17, 'S': 11, 'P': 7, 'X': 7, 'M': 2})
Counter([pos[0] for pos in ma[0].pos(d) if pos[1] == 'J'])
Counter([pos[0] for pos in ma[0].pos(d) if pos[1] == 'E'])
Counter([pos[0] for pos in ma[0].pos(d) if pos[1] == 'X'])
Counter([pos[0] for pos in ma[0].pos(d) if pos[1] == 'M'])
Counter({'총': 1, '다만': 1})
stopwords = ['J', 'E', 'X', 'M']
len([pos[0] for pos in ma[0].pos(d) if pos[1] not in stopwords])
62/111
0.5585585585585585
sum(Counter([pos[1] for pos in ma[1].pos(d)]).values())
159
import re

len([pos[0] for pos in ma[1].pos(d) if re.match(r'N|V', pos[1])])
89
89/159
0.559748427672956
# 한글일때, Zipf, 품사, 음절
len([pos[0] for pos in ma[1].pos(d) if len(pos[0]) > 1])
58
len([pos[0] for pos in ma[1].pos(d)
     if len(pos[0]) > 1 and re.match(r'N|V', pos[1])]),\
' '.join([pos[0] for pos in ma[1].pos(d)
     if len(pos[0]) > 1 and re.match(r'N|V', pos[1])])
(50,
 '아마존 지나 27 현지 시간 27 5000 달러 7000 추가 투자 발표 아마존 투자 금액 지난해 단행 12 5000 달러 6700 투자 포함 40 달러 4000 이르 아마존 사상 최대 규모 외부 투자 이면서 유치 투자 금액 73 달러 8600 55 달하 수준 통하 아마존 지분 일부 보유 이사회 참여')
'선물포장', '선 물 포 장' => X
'시발', '시 발', '동시발생' => X
**                **
# 1. 사전 베이스
stopwords = ['시발', '씨발']

d = '시발 씨발'
' '.join([t for t in d.split() if t not in stopwords])

r = list()
for t in d.split():
    if t in stopwords:
        r.append('*'*len(t))
    else:
        r.append(t)
' '.join(r)
'** **'
# 2. 정규식
stopwords = re.compile('(시발|씨발|시방)')

d = '시발 씨발 시방 시발놈'
# ' '.join([t for t in d.split() if t not in stopwords])

r = list()
for t in d.split():
    r.append(stopwords.sub('*'*len(stopwords.search(t).group(1)), t))
' '.join(r)
'** ** ** **놈'
p = re.compile('선.*물.*포.*장')

d = ['선물포장', '선물 포장', '선 물 포 장']

for t in d:
    if p.search(t):
        print(t)
선물포장
선물 포장
선 물 포 장
stopwords = re.compile('(시발|씨발|시방)')

d = '시발 씨발 시방 시발놈 ㅅㅣ발'
# ' '.join([t for t in d.split() if t not in stopwords])

r = list()
for t in d.split():
    r.append(stopwords.sub('*'*len(stopwords.search(t).group(1)), t))
' '.join(r)
---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
Cell In[57], line 8
      6 r = list()
      7 for t in d.split():
----> 8     r.append(stopwords.sub('*'*len(stopwords.search(t).group(1)), t))
      9 ' '.join(r)

AttributeError: 'NoneType' object has no attribute 'group'
image.png

from nltk.tokenize import word_tokenize

data = '''low low low low low
          lOwer Lower
          newest Newest newest newest newest newest
          widest widest widest'''

def getData(d):
    vocab = {}
    for token in word_tokenize(d.lower()):
        token = ' '.join(token) + ' </w>'
        if token in vocab.keys(): # .kyes() X
            vocab[token] += 1
        else:
            vocab[token] = 1
    return vocab
def getPairs(v, n=2):
    pairs = dict()
    
    for k, v in v.items():
        tokens = k.split()
        for i in range(len(tokens)-(n-1)): # ngram
            token = ' '.join(tokens[i:i+n])
            
            if token not in pairs:
                pairs[token] = v
            else:
                pairs[token] += v
                
    return pairs
def mergePairs(b, v):
    vocab = dict()
    
    for k, v in v.items():
        newk = re.sub(b, re.sub(' ', '', b), k)
        vocab[newk] = v
        
    return vocab
vocab = getData(data)
for i in range(6):
    pairs = getPairs(vocab)
    best = max(pairs, key=pairs.get) # sorted(pairs, key=pairs.get, reverse=True)
    vocab = mergePairs(best, vocab)
[re.sub(r'</w>', '', t).split() for t in vocab.keys()]
[['low'], ['low', 'e', 'r'], ['ne', 'w', 'est'], ['w', 'i', 'd', 'est']]
data = '시발 시발 시발 시발 시이발 씨발 씨발 씨발 시~발'
vocab = getData(data)
bestList = list()
for i in range(4):
    pairs = getPairs(vocab)
    best = max(pairs, key=pairs.get) # sorted(pairs, key=pairs.get, reverse=True)
    bestList.append(best)
    vocab = mergePairs(best, vocab)
# [re.sub(r'</w>', '', t).split() for t in vocab.keys()]
[t.split() for t in vocab.keys()], bestList
([['시발</w>'], ['시이', '발</w>'], ['씨발</w>'], ['시', '~', '발</w>']],
 ['발 </w>', '시 발</w>', '씨 발</w>', '시 이'])
def getData2(d):
    vocab = {}
    for token in word_tokenize(d.lower()):
        token = '<w> ' + ' '.join(token) + ' </w>'
        if token in vocab.keys(): # .kyes() X
            vocab[token] += 1
        else:
            vocab[token] = 1
    return vocab

data = '시발 시발 시발 시발 시이발 씨발 씨발 씨발 시~발'
vocab = getData2(data)
bestList = list()
for i in range(4):
    pairs = getPairs(vocab)
    best = max(pairs, key=pairs.get) # sorted(pairs, key=pairs.get, reverse=True)
    bestList.append(best)
    vocab = mergePairs(best, vocab)
# [re.sub(r'</w>', '', t).split() for t in vocab.keys()]
[t.split() for t in vocab.keys()], bestList
([['<w>시발</w>'],
  ['<w>시', '이', '발</w>'],
  ['<w>씨', '발</w>'],
  ['<w>시', '~', '발</w>']],
 ['발 </w>', '<w> 시', '<w>시 발</w>', '<w> 씨'])
start = list()
end = list()
stopwords = list()

for p in bestList:
    p = re.sub(r'\s', '', p)
    
    if re.match('<w>', p):
        start.append(re.sub('<w>', r'\\b', p))
    
    if re.search('<\/w>$', p):
        end.append(re.sub('<\/w>', r'\\b', p))
        
    # 단어로 끝나는것 처리 안함

pattens = list()
for s in start:
    for e in end:
        pattens.append(re.compile(s+'.*?'+e))
pattens
[re.compile(r'\b시.*?발\b', re.UNICODE),
 re.compile(r'\b시.*?<w>시발\b', re.UNICODE),
 re.compile(r'\b시발</w>.*?발\b', re.UNICODE),
 re.compile(r'\b시발</w>.*?<w>시발\b', re.UNICODE),
 re.compile(r'\b씨.*?발\b', re.UNICODE),
 re.compile(r'\b씨.*?<w>시발\b', re.UNICODE)]
q = '시발 씨이발 시이발 시423423발'.split()

for t in q:
    for p in pattens:
        if p.search(t):
            print(t, '*'*len(t))
시발 **
씨이발 ***
시이발 ***
시423423발 ********
# 음절 => 자음, 모음 => 초/중/종
가-힣
ord('가'), ord('힣'), ord('힣')-ord('가')
(44032, 55203, 11171)
[chr(ord('가')+i) for i in range(28)]
['가',
 '각',
 '갂',
 '갃',
 '간',
 '갅',
 '갆',
 '갇',
 '갈',
 '갉',
 '갊',
 '갋',
 '갌',
 '갍',
 '갎',
 '갏',
 '감',
 '갑',
 '값',
 '갓',
 '갔',
 '강',
 '갖',
 '갗',
 '갘',
 '같',
 '갚',
 '갛']
[chr(ord('가')+(28*i)) for i in range(21)]
['가',
 '개',
 '갸',
 '걔',
 '거',
 '게',
 '겨',
 '계',
 '고',
 '과',
 '괘',
 '괴',
 '교',
 '구',
 '궈',
 '궤',
 '귀',
 '규',
 '그',
 '긔',
 '기']
[chr(ord('가')+(28*21*i)) for i in range(19)]
['가',
 '까',
 '나',
 '다',
 '따',
 '라',
 '마',
 '바',
 '빠',
 '사',
 '싸',
 '아',
 '자',
 '짜',
 '차',
 '카',
 '타',
 '파',
 '하']
print([chr(ord('ㄱ')+i) for i in range(30)])
print([chr(ord('ㅏ')+i) for i in range(21)])
['ㄱ', 'ㄲ', 'ㄳ', 'ㄴ', 'ㄵ', 'ㄶ', 'ㄷ', 'ㄸ', 'ㄹ', 'ㄺ', 'ㄻ', 'ㄼ', 'ㄽ', 'ㄾ', 'ㄿ', 'ㅀ', 'ㅁ', 'ㅂ', 'ㅃ', 'ㅄ', 'ㅅ', 'ㅆ', 'ㅇ', 'ㅈ', 'ㅉ', 'ㅊ', 'ㅋ', 'ㅌ', 'ㅍ', 'ㅎ']
['ㅏ', 'ㅐ', 'ㅑ', 'ㅒ', 'ㅓ', 'ㅔ', 'ㅕ', 'ㅖ', 'ㅗ', 'ㅘ', 'ㅙ', 'ㅚ', 'ㅛ', 'ㅜ', 'ㅝ', 'ㅞ', 'ㅟ', 'ㅠ', 'ㅡ', 'ㅢ', 'ㅣ']
choList = ['ㄱ', 'ㄲ', 'ㄴ', 'ㄷ', 'ㄸ', 'ㄹ', 'ㅁ', 'ㅂ', 'ㅃ', 'ㅅ', 'ㅆ', 'ㅇ', 'ㅈ', 'ㅉ', 'ㅊ', 'ㅋ', 'ㅌ', 'ㅍ', 'ㅎ']
jungList = ['ㅏ', 'ㅐ', 'ㅑ', 'ㅒ', 'ㅓ', 'ㅔ', 'ㅕ', 'ㅖ', 'ㅗ', 'ㅘ', 'ㅙ', 'ㅚ', 'ㅛ', 'ㅜ', 'ㅝ', 'ㅞ', 'ㅟ', 'ㅠ', 'ㅡ', 'ㅢ', 'ㅣ']
jongList = [' ', 'ㄱ', 'ㄲ', 'ㄳ', 'ㄴ', 'ㄵ', 'ㄶ', 'ㄷ', 'ㄹ', 'ㄺ', 'ㄻ', 'ㄼ', 'ㄽ', 'ㄾ', 'ㄿ', 'ㅀ', 'ㅁ', 'ㅂ', 'ㅄ', 'ㅅ', 'ㅆ', 'ㅇ', 'ㅈ', 'ㅊ', 'ㅋ', 'ㅌ', 'ㅍ', 'ㅎ']

def chr2tri(c):
    r = c
    
    if len(c) ==1 and re.search(r'[가-힣]', c):
        code = ord(c) - ord('가')
        cho, temp = divmod(code, (28*21))
        jung, jong = divmod(temp, 28)
        r = ''.join((choList[cho], jungList[jung], jongList[jong]))
        
    return r

''.join([chr2tri(c) for c in '고려Koreaㄷㅐ학굙'])
'ㄱㅗ ㄹㅕ Koreaㄷㅐㅎㅏㄱㄱㅛㄺ'
def tri2chr(*c):
    r = ''.join(c)
    
    if len(c) == 3:
        umjeol = choList.index(c[0])*28*21
        umjeol += jungList.index(c[1])*28
        umjeol += jongList.index(c[2])
        r = chr(ord('가')+umjeol)
        
    return r

''.join([chr2tri(c) for c in '고려Koreaㄷㅐ학굙']),\
''.join([tri2chr(*tuple(chr2tri(c))) for c in '고려Koreaㄷㅐ학굙'])
('ㄱㅗ ㄹㅕ Koreaㄷㅐㅎㅏㄱㄱㅛㄺ', '고려Koreaㄷㅐ학굙')
def hamming(s1, s2):
    r = 0
    
    if len(s1) != len(s2):
        r = None
    else:
        r = sum([1 if c1 != c2 else 0 for c1, c2 in zip(s1, s2)])
    
    return r
hamming('안녕하세요', '아녕하세요'),\
hamming('안녕하세요', '아녕하세요') / len('안녕하세요')
(1, 0.2)
c1 = ''.join([chr2tri(c) for c in '안녕하세요'])
c2 = ''.join([chr2tri(c) for c in '아녕하세요'])
hamming(c1, c2),\
hamming(c1, c2) / len(c1)
(1, 0.06666666666666667)
%E1%84%89%E1%85%B3%E1%84%8F%E1%85%B3%E1%84%85%E1%85%B5%E1%86%AB%E1%84%89%E1%85%A3%E1%86%BA%202024-03-29%20%E1%84%8B%E1%85%A9%E1%84%92%E1%85%AE%2012.11.01.png

# Hamming
# 1. 다른 문자열 검사 X
# 2. 모든 문자를 동일하게 검사(문자마다의 상대적 차이를 X)

# Leven
def lev(s1, s2):
    if len(s1) == 0:
        return len(s2)
    
    if len(s2) == 0:
        return len(s1)
    
    if s1[0] == s2[0]:
        return lev(s1[1:], s2[1:])
    else:
        l1 = lev(s1[1:], s2) + 1
        l2 = lev(s1, s2[1:]) + 2
        l3 = lev(s1[1:], s2[1:]) + 0.1
        return min((l1, l2, l3))
lev(c1, c2),\
lev(c1, c2)/len(c1)
(0.1, 0.006666666666666667)     
```