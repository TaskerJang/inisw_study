## 1. feature selection

## 2. classification => 분류

## 3. 숫자가 아닌 데이터를 숫자화 하기

## 4. plateau => 고원 현상 => 더이상 데이터로 학습 되지 않는 상태

## 5. 충분하지 못한 데이터 => 대표성 문제 발생

## 6. isna = isnull => missing value 확인

## 7. .columns[any()] => 미싱데이터 추적

## 8. sum을 활용하여 미씽데이터의 갯수까지 추적 가능

## 9. missingno => missing 데이터 시각화가 매우 잘 되어있다.

## 10. 미씽 데이터 처리 => impute

```python

Regression(Linear/Logistic)
(X,Y), Theta*X, Logistic(Theta*X), Softmax
Logit => X = log p/(1-p)
         Theta*X = log p/(1-p)
         p = Bernouii
         mu(x) => Logistic
X     = {x1, x2, ..., xn}
Theta = {t1, t2, ..., tn}
      => x1t1 + x2t2 + x3t3 ... d t1
import numpy as np
import matplotlib.pyplot as plt
X = np.linspace(np.pi, np.pi*2, 100)
Y = np.zeros((len(X)))
Y[np.cos(X) > 0] = 1
X = np.c_[np.ones(len(X)), X]
h = 1e-5
theta = np.random.rand(2)
epoch = 20000

SE = lambda truey, predy: (truey - predy)**2
print(theta)
print(np.sum(SE(Y, X.dot(theta))), np.mean(SE(Y, X.dot(theta))))
for _ in range(epoch):
#     for x, y in zip(X, Y):
    theta = theta + h*2*X.T.dot(Y - X.dot(theta))
[0.48790371 0.80607717]
1449.29414965036 14.4929414965036
plt.scatter(X[:,1],Y)
plt.plot(X[:,1],X.dot(theta))
theta
array([-0.67631237,  0.25741828])

np.sum(SE(Y, X.dot(theta))), np.mean(SE(Y, X.dot(theta)))
(7.192559256222778, 0.07192559256222779)
trainingSet = [(1, 'Chinese Beijing Chinese', 'yes'),
               (2, 'Chinese Chinese Shanghai', 'yes'),
               (3, 'Chinese Macao', 'yes'),
               (4, 'Tokyo Japan Chinese', 'no')]
testSet = [(5, 'Chinese Chinese Chinese Tokyo Japan')]
# Vectorize(Boolean)
# [d[1].lower().split() for d in trainingSet]
V = list(set('\n'.join([d[1] for d in trainingSet]).lower().split()))
X = np.zeros((4,len(V)))
for i, d in enumerate(trainingSet):
    for t in d[1].lower().split():
        if t in V:
            j = V.index(t)
            X[i,j] = 1
X
array([[0., 0., 1., 0., 1., 0.],
       [1., 0., 1., 0., 0., 0.],
       [0., 1., 1., 0., 0., 0.],
       [0., 0., 1., 1., 0., 1.]])
Y = np.zeros((4,))
for i, d in enumerate(trainingSet):
    if d[2] == 'yes':
        Y[i] = 1
Y
array([1., 1., 1., 0.])
newX = np.c_[np.ones(len(X)), X]
h = 1e-5
theta = np.random.rand(newX.shape[-1])
epoch = 20000

SE = lambda truey, predy: (truey - predy)**2
history = list()

print(theta)
print(np.sum(SE(Y, newX.dot(theta))), np.mean(SE(Y, newX.dot(theta))))

for i, _ in enumerate(range(epoch)):
#     for x, y in zip(X, Y):
    theta = theta + h*2*newX.T.dot(Y - newX.dot(theta))
    
    if i % 1000 == 0:
        history.append(np.mean(SE(Y, newX.dot(theta))))
[0.85263111 0.40044767 0.78945889 0.68929508 0.36090132 0.33919508
 0.42036411]
8.834248647580235 2.208562161895059
plt.plot(history)
[<matplotlib.lines.Line2D at 0x126f9fed0>]

T = np.zeros((len(testSet), len(V)))

for i, d in enumerate(testSet):
    for t in d[1].lower().split():
        if t in V:
            j = V.index(t)
            T[i,j] = 1
T
array([[0., 0., 1., 1., 0., 1.]])
np.c_[np.ones((1,)), T].dot(theta) > .5
array([False])
# Vectorize => Orthogonal
# ; Boolean(*), Count(*), TF-IDF, Weight...
# Linear => Logistic vs. Naive Bayes
# 10시 35분
X = np.array([[0,0],
              [0,1],
              [1,0],
              [1,1]])
Y = np.array([[0],
              [0],
              [0],
              [1]]) # OR
def dataLoader(x, y, n=1):
    # Shuffling
#     [i for i in range(len(x))]
    D = np.random.permutation(np.c_[x,y])
    
    # Batch
    for i in range(len(x)//n):
        yield (D[i*n:i*n+n,:-1], D[i*n:i*n+n,-1:])
        
[_ for _ in dataLoader(X, Y, n=1)]
[(array([[0, 0]]), array([[0]])),
 (array([[1, 1]]), array([[1]])),
 (array([[1, 0]]), array([[0]])),
 (array([[0, 1]]), array([[0]]))]
W = np.random.rand(X.shape[-1], Y.shape[-1])
B = np.random.rand(Y.shape[-1])

lr = 1e-5
epoch = 100000

N = 4
J = lambda y, _y: (y-_y)**2

loss = list()

for i in range(epoch):
    for x, y in dataLoader(X, Y, N):
#         x.shape => (n,2)
#         w.shape => (2,1)
#         x @ W => (n,1) + B
        Yhat = x @ W + B # => (n,1)
        dLoss = -2*(y - Yhat) # => 2(Yhat - Y) => (n,1)
#         dB = np.mean(dLoss)*1 # n개의 평균
        dB = np.sum(dLoss)
#         dW = x.T @ dLoss / N # => (2,n) (n,1) => (2,1)
        dW = x.T @ dLoss
        B = B - lr*dB
        W = W - lr*dW
        
    if epoch % 100 == 0:
        loss.append(np.sum(J(Y, X@W+B)))
plt.plot(loss)
X@W > .5
array([[False],
       [False],
       [False],
       [ True]])

N = 4

W = np.random.rand(X.shape[-1]+1, Y.shape[-1])

lr = 1e-5
epoch = 100000

J = lambda y, _y: (y-_y)**2

loss = list()

for i in range(epoch):
    for x, y in dataLoader(X, Y, N):
        x = np.c_[np.ones(N), x]
#         x.shape => (n,2)
#         w.shape => (2,1)
        Yhat = x @ W # => (n,1)
        dLoss = -2*(y - Yhat) # => 2(Yhat - Y) => (n,1)
#         dW = x.T @ dLoss / N # => (2,n) (n,1) => (2,1)
        dW = x.T @ dLoss
        W = W - lr*dW
        
    if epoch % 100 == 0:
        loss.append(np.sum(J(Y, np.c_[np.ones(len(X)), X]@W)))
plt.plot(loss)
np.c_[np.ones(len(X)), X]@W > .5
array([[False],
       [False],
       [False],
       [ True]])

```