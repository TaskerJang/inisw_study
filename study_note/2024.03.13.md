## 1. JSON이란 무엇인가?

## 2. json.load, loads, json.dump, dumps 등등..

## 3. querystring, form-data

## 4. get, post, put 방식의 차이

## 5. XML이란 무엇인가?

## 6. DTD란 무엇인가?

## 7. 데이터 수집 과정:
1. 검색을 한다.
2. 해당 컨텐츠 ex) 뉴스, 이미지 등등에 대한 (가져오려는 정보의 특수한 elements를 찾는다.) 선정하기.
3. status_code와 reason 확인 => (200, 'ok')
4. 정규표현식을 사용해서 가져오고 싶은 컨텐츠 정보만 추출하기

## 8. dom.find_all, find_parent, find_next_siblings 이용해서 찾기

## 9. 대칭 차집합 A ^ B

## 10. 클래스에서는 인스턴스 변수 접근할 수 없다.

## 11. 인스턴스는 .mro 사용 불가. 클래스만 .mro 사용 가능하다.

## 12. dir() =/ `__dir__`

## 13. 인스턴스는 부모 클래스가 없다.

## 14. list.`__bases__` => object => 부모를 알려준다.

## 15. `__base__` => 첫번 째 부모에게 찾아간다.

## 16. mro =/ `__bases__`

## 17. 합성 함수: 다른 클래스의 인스턴스를 안에 집어넣는 것.

## 18. composition pattern : 객체지향 vs 함수형

## 19. 상속의 문제점을 위해 composition을 쓴다. 

## 20. composition 사용해서 상속과 똑같은 효과 내기!

=> attribute = 클래스에 정의된 모든 것.

## 21. `__getattr__`=> attribute 에러가 발생하면 다음을 실행하시오.

## 22. `__abs__` => 절댓값 계산

## 23. a = input(함수명) => getattr(인자, a) => 함수 실행된다.

## 24. `__getattribute__`=> 숫자+ 공백 또는 괄호 + . + `__getattribute__` => 공백의 의미. => 1.은 float이므로 (=`__getattr__`)

## 25. sklearn.naive_bayes -> 공통점을 모아서 규약으로 만든다.

## 26. 파이썬은 추상화를 상속의 문법으로 만든다.

## 27. 추상화 => 공통적인 규칙을 뽑아서 이 규칙을 강제로 하게 한다.

## 28. abstract method: 무조건 구현해야된다. 

## 29. duck typing과 composition을 사용해서 상속 구현 가능하다.

## 30. 추상화는 상속 및 duck typing과 결합 가능하다.

## 31. abstract class => abc

## 32. iterable => `__iter__` => 가능
## 33. iterator => `__iter__, __next__`=> 가능


```python

아무 데이터 수집 X
1. robots.txt (O)
2. 트래픽(delay)
3. 이용약관(반드시 확인)
4. DB(저작권)
5. 개인정보 X
=> 개인, 공적, 연구용, 배포X, 공유X, Opt-out관리

HTTP(HyperText)
- TCP/IP; 요청-응답(Header)
Header
- Status Code, Reason 반드시 확인
- Content-type; text/html*, application/*, image/*, video/*, ...
bytes->text
- User-agent, X-CSRFTOKEN, Referer...

HTTP-Urllib-Requests(High-level*)
- URL parsing: Bytes 주고 받아야 함
Requests
- Response객체
- Request객체
- HTTP Error; raise_for_status()
for Test; httpbin.org(method, status)
from requests import request

url = 'https://httpbin.org/post'
resp = request('POST', url, params={'이름1':'값1'}, data={'이름2':'값2'})
resp.status_code, resp.reason
(200, 'OK')
resp.headers['content-type']
'application/json'
import json
json.load(fp), loads(str)
json.dump(fp), dumps(str)
json.loads(resp.text)['args'].keys(),
    json.loads(resp.text)['args'].values()
(dict_keys(['k']), dict_values(['v']))
json.loads(resp.text)['form']
{'이름': '값'}
resp.json()
{'args': {'이름1': '값1'},
 'data': '',
 'files': {},
 'form': {'이름2': '값2'},
 'headers': {'Accept': '*/*',
             'Accept-Encoding': 'gzip, deflate, br',
             'Content-Length': '30',
             'Content-Type': 'application/x-www-form-urlencoded',
             'Host': 'httpbin.org',
             'User-Agent': 'python-requests/2.31.0',
             'X-Amzn-Trace-Id': 'Root=1-65f0f338-36a3ce983cd064b91214a57b'},
 'json': None,
 'origin': '163.152.3.163',
 'url': 'https://httpbin.org/post?이름1=값1'}
from requests.compat import urlparse
urlparse(resp.request.url).query, resp.request.body
# ---> querystring                ---> form-data
('%EC%9D%B4%EB%A6%841=%EA%B0%921', '%EC%9D%B4%EB%A6%842=%EA%B0%922')
<form action='주소' method="POST">
<input name="이름" value="값">...
</form>
%%writefile text.xml
<xml>
<url>
<loc>https://www.korea.ac.kr/</LOC>
<lastmod>2023-05-17</lastmod>
<changefreq>always</changefreq>
<priority>0.9</priority>
</url>
</xml>
Overwriting text.xml
ua = 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Whale/3.24.223.24 Safari/537.36'
url = 'https://www.google.com/search'
params = {
    'q':'수지',
    'sourceid':'chrome',
    'ie':'UTF-8'
}

resp = request('GET', url, params=params, headers={'user-agent':ua})
resp.status_code, resp.reason
(200, 'OK')
resp.headers, resp.request.headers
({'Content-Type': 'text/html; charset=UTF-8', 'Date': 'Wed, 13 Mar 2024 01:21:48 GMT', 'Expires': '-1', 'Cache-Control': 'private, max-age=0', 'Strict-Transport-Security': 'max-age=31536000', 'Content-Security-Policy': "object-src 'none';base-uri 'self';script-src 'nonce-XbRoUjL5_N_K45BH2prGNA' 'strict-dynamic' 'report-sample' 'unsafe-eval' 'unsafe-inline' https: http:;report-uri https://csp.withgoogle.com/csp/gws/cdt1", 'Cross-Origin-Opener-Policy': 'same-origin-allow-popups; report-to="gws"', 'Report-To': '{"group":"gws","max_age":2592000,"endpoints":[{"url":"https://csp.withgoogle.com/csp/report-to/gws/cdt1"}]}', 'Accept-CH': 'Sec-CH-UA-Platform, Sec-CH-UA-Platform-Version, Sec-CH-UA-Full-Version, Sec-CH-UA-Arch, Sec-CH-UA-Model, Sec-CH-UA-Bitness, Sec-CH-UA-Full-Version-List, Sec-CH-UA-WoW64', 'Permissions-Policy': 'unload=()', 'Origin-Trial': 'Ap+qNlnLzJDKSmEHjzM5ilaa908GuehlLqGb6ezME5lkhelj20qVzfv06zPmQ3LodoeujZuphAolrnhnPA8w4AIAAABfeyJvcmlnaW4iOiJodHRwczovL3d3dy5nb29nbGUuY29tOjQ0MyIsImZlYXR1cmUiOiJQZXJtaXNzaW9uc1BvbGljeVVubG9hZCIsImV4cGlyeSI6MTY4NTY2Mzk5OX0=, AvudrjMZqL7335p1KLV2lHo1kxdMeIN0dUI15d0CPz9dovVLCcXk8OAqjho1DX4s6NbHbA/AGobuGvcZv0drGgQAAAB9eyJvcmlnaW4iOiJodHRwczovL3d3dy5nb29nbGUuY29tOjQ0MyIsImZlYXR1cmUiOiJCYWNrRm9yd2FyZENhY2hlTm90UmVzdG9yZWRSZWFzb25zIiwiZXhwaXJ5IjoxNjkxNTM5MTk5LCJpc1N1YmRvbWFpbiI6dHJ1ZX0=', 'P3P': 'CP="This is not a P3P policy! See g.co/p3phelp for more info."', 'Content-Encoding': 'br', 'Server': 'gws', 'X-XSS-Protection': '0', 'X-Frame-Options': 'SAMEORIGIN', 'Transfer-Encoding': 'chunked', 'Set-Cookie': '1P_JAR=2024-03-13-01; expires=Fri, 12-Apr-2024 01:21:48 GMT; path=/; domain=.google.com; Secure; SameSite=none, AEC=Ae3NU9P668bva7YBHToms1_iNKx277p3se-NeFChV2YRLV727SdzCVPOEW0; expires=Mon, 09-Sep-2024 01:21:48 GMT; path=/; domain=.google.com; Secure; HttpOnly; SameSite=lax, NID=512=L2fyoK0vrjBSPXJyBDBsrpS59xXZ6SLUc2hh3Rv5Gqx5KiUG9O6RAJaAAcDw5DiYuus9dijO7E66-rjkTR9RYXLCYLpOF7mhinWUdcXySYOZ-cjr2K02nnGa8ws0mD4FlX81RabBdS--B-NvI5kIYED91x6V33ODKRp2AOwAwH4; expires=Thu, 12-Sep-2024 01:21:48 GMT; path=/; domain=.google.com; Secure; HttpOnly; SameSite=none', 'Alt-Svc': 'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'},
 {'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Whale/3.24.223.24 Safari/537.36', 'Accept-Encoding': 'gzip, deflate, br', 'Accept': '*/*', 'Connection': 'keep-alive'})
from requests.compat import unquote
from html import unescape
import re

re.findall(r'<a jsname="UWckNb" href="(.+?)"[^>]+?><br><h3 class="LC20lb MBeuO DKV0Md">(.+?)</h3>', resp.text)
[('https://namu.wiki/w/%EC%88%98%EC%A7%80(1994)', '수지(1994)'),
 ('https://ko.wikipedia.org/wiki/%EC%88%98%EC%A7%80_(1994%EB%85%84)',
  '수지 (1994년) - 위키백과, 우리 모두의 백과사전'),
 ('https://namu.wiki/w/%EC%88%98%EC%A7%80', '수지'),
 ('https://m2.melon.com/artist/song.htm?artistId=514741',
  '수지 (Suzy)- 아티스트채널 - 멜론'),
 ('https://ko.wikipedia.org/wiki/%EC%88%98%EC%A7%80',
  '수지 - 위키백과, 우리 모두의 백과사전'),
 ('https://www.hani.co.kr/arti/culture/culture_general/1112601.html',
  "다시 아이돌 된 수지 “상처 많은 '이두나' 안아주고 싶어”"),
 ('https://www.autotribune.co.kr/news/articleView.html?idxno=12365',
  '"제가 죽어야겠네요"... 가수 수지, 연예인 동료들에 눈물 ...'),
 ('https://www.sujigu.go.kr/', '수지구청')]
url = 'https://search.naver.com/search.naver'
params = {
    'where':'nexearch',
    'sm':'top_hty',
    'fbm':'0',
    'ie':'utf8',
    'query':'수지'
}
resp = request('GET', url, params=params)
resp.status_code, resp.reason
(200, 'OK')
resp.headers, resp.request.headers, resp.request.url, resp.request.body
({'Date': 'Wed, 13 Mar 2024 01:38:10 GMT', 'Content-Type': 'text/html; charset=UTF-8', 'Transfer-Encoding': 'chunked', 'Connection': 'keep-alive', 'Set-Cookie': 'page_uid=iQZvllpzL8VsslYTIqlsssssteK-171054; path=/; domain=.naver.com, _naver_usersession_=LC/EuaLFOsiwTY87jEjelA==; path=/; expires=Wed, 13-Mar-24 01:43:10 GMT; domain=.naver.com', 'X-Frame-Options': 'SAMEORIGIN', 'X-XSS-Protection': '1; report=/p/er/post/xss', 'Cache-Control': 'no-cache, no-store, must-revalidate, max-age=0', 'Pragma': 'no-cache', 'Referrer-Policy': 'unsafe-url', 'Vary': 'Accept-Encoding', 'Content-Encoding': 'gzip', 'Server': 'nxg', 'Accept-CH': 'Sec-CH-UA, Sec-CH-UA-Arch, Sec-CH-UA-Bitness, Sec-CH-UA-Full-Version-List, Sec-CH-UA-Mobile, Sec-CH-UA-Model, Sec-CH-UA-Platform, Sec-CH-UA-Platform-Version, Sec-CH-UA-WoW64'},
 {'User-Agent': 'python-requests/2.31.0', 'Accept-Encoding': 'gzip, deflate, br', 'Accept': '*/*', 'Connection': 'keep-alive'},
 'https://search.naver.com/search.naver?where=nexearch&sm=top_hty&fbm=0&ie=utf8&query=%EC%88%98%EC%A7%80',
 None)
re.findall(r'<a href="([^"]+?)" class="news_tit"[^>]+?>(.+?)</a>', resp.text)
[('http://www.newsis.com/view/?id=NISX20240313_0002658567&cID=10803&pID=14000',
  '용인<mark>수지</mark>중앙공원에 56억 들여 4km 건강 숲길 조성'),
 ('https://www.hankyung.com/article/202403110472i',
  '3월 1~10일 수출 13% 감소…무역<mark>수지</mark> 13억달러 적자'),
 ('https://www.yna.co.kr/view/AKR20240308020000002?input=1195m',
  '1월 경상<mark>수지</mark> 30.5억달러 흑자…반도체 등 수출 회복'),
 ('https://www.mk.co.kr/article/10959326',
  '“반도체 수출 호조”…1월 경상<mark>수지</mark> 9개월 연속 흑자')]
re.findall(r'<a target="_blank" href="([^"]+?)" class="link_tit"[^>]+?>(.+?)</a>', resp.text)
[('http://www.msoopent.com/page/page.html?acode=S61703024&amp;mcd=02',
  'SUZY - 매니지먼트숲'),
 ('https://www.instagram.com/SKUUKZKY/',
  '숮이 💄💅👡👠🎀👙🌂👗🌂🎀💋💌(@skuukzky) • Instagram 사진 및 동영상'),
 ('https://namu.wiki/w/%EC%88%98%EC%A7%80(1994)',
  '<mark>수지</mark>(1994) - 나무위키'),
 ('https://ko.wikipedia.org/wiki/%EC%88%98%EC%A7%80_(1994%EB%85%84)',
  '<mark>수지</mark> (1994년) - 위키백과 한국'),
 ('https://twitter.com/Suzy', '숮이 | 트위터')]
url = 'https://search.daum.net/search'
params = {
    'w':'tot',
    'DA':'YZR',
    't__nil_searchbox':'btn',
    'q':'황정민'
}

resp = request('GET', url, params=params)
resp.status_code, resp.reason
(200, 'OK')
re.findall(r'<c-menu-share .+? data-title="(.+?)" .+? data-href="(.+?)"[^>]+?>(.+?)</c-title>',
           resp.text)
# resp.text
[('황정민 - 나무위키',
  'https://namu.wiki/w/%ED%99%A9%EC%A0%95%EB%AF%BC',
  '<b>황정민</b> - 나무위키'),
 ('황정민',
  'https://ko.wikipedia.org/wiki/%ED%99%A9%EC%A0%95%EB%AF%BC',
  '<b>황정민</b>'),
 ('황정민 배우 찐 혈육 (남동생)',
  'https://cafe.daum.net/SoulDresser/FLTB/831198?q=%ED%99%A9%EC%A0%95%EB%AF%BC&re=1',
  '<b>황정민</b> 배우 찐 혈육 (남동생)'),
 ('[FULL] 영화 &amp;#39;서울의 봄&amp;#39; 배우 ✨황정민&amp;amp;정우성&amp;amp;박해준&amp;amp;김성균✨을 정희에 모십니다👀 ｜정오의 희망곡 김신영입니다',
  'https://table.cafe.daum.net/p/1183489353/248271909297102848',
  '[FULL] 영화 &#39;서울의 봄&#39; 배우 ✨<b>황정민</b>&amp;정우성&amp;박해준&amp;김성균✨을 정희에 모십니다👀 ｜정오의 희망곡 김신영입니다'),
 ('[스포엔 뉴세이] 황정민에 빠져들어 실제로 &ldquo;우리 사랑하게 해달라&amp;#34; 말했던 탑여배우 - 콘텐츠뷰',
  'https://v.daum.net/v/zhj2ZR1maN',
  '[스포엔 뉴세이] <b>황정민</b>에 빠져들어 실제로 “우리 사랑하게 해달라&#34; 말했던 탑여배우 - 콘텐츠뷰'),
 ('&amp;#39;서울의 봄&amp;#39; 관객수 손익분기점 제작비 황정민 정우성 김성수',
  'https://movie2863.tistory.com/1084',
  '&#39;서울의 봄&#39; 관객수 손익분기점 제작비 <b>황정민</b> 정우성 김성수'),
 ('배우 황정민 프로필 | 나이 | 부인 | 아내 | 키 | 고향 | 학력 | 영화 | 총정리✅',
  'https://star.studioice.co.kr/319',
  '배우 <b>황정민</b> 프로필 | 나이 | 부인 | 아내 | 키 | 고향 | 학력 | 영화 | 총정리✅'),
 ('황정민 밤양갱 뭐니ㅋㅋ',
  'https://gall.dcinside.com/mgallery/board/view/?id=gaon&no=7106037',
  '<b>황정민</b> 밤양갱 뭐니ㅋㅋ'),
 ('영화 리뷰-《서울의 봄》 - 한국 영화 / 감독 김성수-황정민, 정우성 / 141분',
  'https://brunch.co.kr/@sopia1357/637',
  '영화 리뷰-《서울의 봄》 - 한국 영화 / 감독 김성수-<b>황정민</b>, 정우성 / 141분'),
 ('황정민 나이 프로필 아내 영화',
  'https://rocco67.tistory.com/43',
  '<b>황정민</b> 나이 프로필 아내 영화'),
 ('오마이스타 - 카카오스토리',
  'http://story.kakao.com/_aXEWi6/jOn0mIzJYda',
  '오마이스타 - 카카오스토리')]
url = 'https://search.naver.com/search.naver'
params = {
    'where':'nexearch',
    'sm':'top_hty',
    'fbm':'0',
    'ie':'utf8',
    'query':'수지'
}
resp = request('GET', url, params=params)
resp.status_code, resp.reason
(200, 'OK')
re.findall(r'<img data-image-viewer-trigger .+? src="(.+?)"', resp.text)
['https://search.pstatic.net/common/?src=http%3A%2F%2Fimgnews.naver.net%2Fimage%2F5129%2F2023%2F07%2F28%2F1690461044_19211135-6_20230728070403219.jpg&type=ff332_332',
 'https://search.pstatic.net/common/?src=http%3A%2F%2Fimgnews.naver.net%2Fimage%2F5902%2F2023%2F03%2F16%2F0000000033_001_20230316093803292.jpg&type=ff332_332',
 'https://search.pstatic.net/common/?src=http%3A%2F%2Fblogfiles.naver.net%2FMjAyNDAyMDFfODAg%2FMDAxNzA2Nzk3NzU1NTc0.HNkgYUJ2UMgTqX6RDetG6_vepjkXj4Xw3zbhANSM9P0g.A3hUHFhnX7Tv4b2E-OfD1iuCdksH_7r1S1b9W4Qi-CAg.JPEG.dlaalwlsfjq%2F1706797748458.jpg&type=ff332_332',
 'https://search.pstatic.net/common/?src=http%3A%2F%2Fimgnews.naver.net%2Fimage%2F011%2F2023%2F10%2F18%2F0004250393_001_20231018112607650.jpg&type=ff332_332',
 'https://search.pstatic.net/common/?src=http%3A%2F%2Fimgnews.naver.net%2Fimage%2F082%2F2023%2F07%2F27%2F0001224312_001_20230727152901140.jpg&type=ff332_332',
 'https://search.pstatic.net/common/?src=http%3A%2F%2Fblogfiles.naver.net%2FMjAyMzA5MTRfMTQg%2FMDAxNjk0NjU1MzUwNjg5.JslvLFjehmxakktAFS2fe21fdUn3jvLvAZs-aFnUm3Yg.lt12sBjqHkF9PZhj_hND04nEGaMhKWPvpqQmfsdZKKkg.JPEG.tntn0218%2F%25B4%25D9%25BF%25EE%25B7%25CE%25B5%25E5.jpeg&type=ff332_332',
 'https://search.pstatic.net/common/?src=http%3A%2F%2Fimgnews.naver.net%2Fimage%2F5712%2F2023%2F06%2F09%2F0000174023_001_20230609150802716.jpg&type=ff332_332',
 'https://search.pstatic.net/sunny/?src=https%3A%2F%2Fimg.hankyung.com%2Fphoto%2F202212%2FBF.32232049.1.jpg&type=ff332_332',
 'https://search.pstatic.net/common/?src=http%3A%2F%2Fblogfiles.naver.net%2FMjAyMzA0MThfMTEg%2FMDAxNjgxNzgxMDkxOTY5._7g0PTDhCaYlbWCDU8O1N44E90nZxUWE2XiU7Dff6zog.3H9TbFRgZUXyiKVtd19I1EXB2y5Dv0TmPXOoGWWSfFEg.PNG.mcca11ister%2F2023-04-18_10%253B22%253B34.PNG&type=ff332_332',
 'https://search.pstatic.net/common/?src=http%3A%2F%2Fimgnews.naver.net%2Fimage%2F5291%2F2023%2F12%2F15%2F0001920872_001_20231215113202850.jpg&type=ff332_332']
data = '''
<a href="/" a=1 b=2 c=3>
'''
re.findall(r'<\w+((?:\s*.+?=[\'\"]?.+?[\'\"]?)*)>',data)
[' href="/" a=1 b=2 c=3']
검색결과(구글, 네이버, 다음)에서 링크/제목 가져왔고,
네이버 이미지 링크 가져옴. => RE
from bs4 import BeautifulSoup
%%writefile test.html
<p>내용</p>
Overwriting test.html
html = '''
<html>
 <head></head>
 <body>
  <div>
   <ul>
    <li>첫번째  -> </li>
    <li>두번째  -> </li>
  </div> -> </ul>
   </ul> -> </div>
 </body>
</HTML>
'''
from bs4 import BeautifulSoup

dom1 = BeautifulSoup(html, 'html.parser')
dom2 = BeautifulSoup(html, 'lxml')
dom3 = BeautifulSoup(html, 'html5lib')
html = '''
<html>
 <head></head>
 <body>
  <div>
   <p>
    <a href="/a.html">go to page</a>
   </p>
  </div>
 </body>
</HTML>
'''
# dom1 = BeautifulSoup(html, 'html.parser')
# dom2 = BeautifulSoup(html, 'lxml')
# dom3 = BeautifulSoup(html, 'html5lib')
dom = BeautifulSoup(html, 'html.parser')
type(dom), type(dom.html), type(dom.html.body)
(bs4.BeautifulSoup, bs4.element.Tag, bs4.element.Tag)
dom.html.body.div.p.a is dom.body.a,
dom.div.a is dom.a
True
dom.a.attrs['href'], dom.a.get_text()
('/a.html', 'go to page')
html = '''
<html>
 <head></head>
 <body>
  <div>
   <p>
    <a href="/a.html">go to page1</a>
    <a href="/b.html">go to page2</a>
   </p>
  </div>
 </body>
</HTML>
'''
dom = BeautifulSoup(html, 'html.parser')
a2 = [tag for tag in dom.p.children][-2]
a2 is dom.a
False
dom.a.attrs, a2.attrs
({'href': '/a.html'}, {'href': '/b.html'})
dom.p.find() is dom.a
True
dom.p.find_all()[-1] is a2
True
import re

# 1 - 태그이름(텍스트, 정규식)
dom.p.find_all('a')
dom.p.find_all(re.compile('h[1-6]'))

# 2 - 태그 속성(attributes), 정규식
# class="LC20lb Dsed"
dom.p.find_all(attrs={'href':re.compile('html$')})

# 3 - 자식/자손?
dom.p.find_all()
[<a href="/a.html">go to page1</a>, <a href="/b.html">go to page2</a>]
html = '''
<html>
 <head></head>
 <body>
  <div>
   <p>
    <a href="/a.html">go to page1</a>
    <a href="/b.html">go to page2</a>
    <div>
       <p>
        <a href="/c.html">go to page3</a>
        <a href="/d.html">go to page4</a>
       </p>
       <div>
       <p>
        <a href="/e.html">go to page5</a>
        <a href="/f.html">go to page6</a>
       </p>
      </div>
      </div>
   </p>
  </div>
 </body>
</HTML>
'''
dom = BeautifulSoup(html, 'html.parser')
# 3 - 자식/자손?
[tag.name for tag in dom.p.find_all(recursive=True)]

# 4 - 문자열(태그내용)
dom.p.find_all(string='go to page1')[0].find_parent() is dom.a
dom.p.find_all(string=re.compile('3|4$'))

# 5 - 갯수
dom.p.find_all(limit=1)[0] is dom.p.find()
dom.p.find() is dom.a
1
[tag.name for tag in dom.p.find_parents()]
['div', 'body', 'html', '[document]']
# div > p > a , a, div
#           *
len(dom.a.find_next_siblings())
2
```