## 1. 정형 vs 비정형 => 칼럼의 순서를 바꿨을 때 의미가 달라지는가에 따라 달라진다.

## 2. 현실 세계의 데이터는 난잡하다.

## 3. 중복데이터 및 불일치 데이터를 어떻게 처리할 것인가? => nomalize => 데이터 전처리

## 4. data cleaning (랭글링) => 많은 시간 소요

## 5. 피처 엔지니어링

## 6. digital signature

## 7. tidy data <=> dirty data

## 8. missing data

## 9. 정규표현식 => 칼럼 추출

## 10. 칼럼 이름 변경 => 메서드명과 겹치는 경우를 대비해서.

## 11. iloc => 추출한 것에서 다시한번 인덱스를 계산하여 추출. 마지막 인덱스 제외

## 12. loc => 마지막 인덱스까지 포함

## 13. 영역을 뽑기 위해서 [1:3, '칼럼명':'칼럼명']

## 14. at vs iat => 특정 지점 추출

```python

from konlpy.tag import Kkma

Kkma().pos('아버지가방에들어가신다.')
OpenJDK 64-Bit Server VM warning: Attempt to protect stack guard pages failed.
OpenJDK 64-Bit Server VM warning: Attempt to deallocate stack guard pages failed.
[('아버지', 'NNG'),
 ('가방', 'NNG'),
 ('에', 'JKM'),
 ('들어가', 'VV'),
 ('시', 'EPH'),
 ('ㄴ다', 'EFN'),
 ('.', 'SF')]
http://konlpy.org/ko/latest/
!java -version
openjdk version "1.8.0_402"
OpenJDK Runtime Environment (Zulu 8.76.0.17-CA-macos-aarch64) (build 1.8.0_402-b06)
OpenJDK 64-Bit Server VM (Zulu 8.76.0.17-CA-macos-aarch64) (build 25.402-b06, mixed mode)
자바 - jpype - konlpy
P(A|B) = P(A,B) => A,B가 동시에 발생한 사건
         ------
          P(B)  => B라고 하는 사건하에
P(A,B) = P(A|B)P(B)
B가 일어났을때, A가 일어났을 확률

P(B|A) = P(B,A) = P(A,B) = P(A|B)P(B)
         ------
          P(A)
P(B,A) = P(B|A)P(A)

A가 given B의 확률
P(B|A) = Likeli P(A|B)P(B) Prior
          --------
             P(A) => evidence, constant
첫번째 어절 => A ㅌ {1음절:11172, ...} ... 모든 case 데이터가 있어야만 P(A) 계산가능 => 불가능
!pip install nltk
import nltk
nltk.download('gutenberg')
nltk.download('punkt')
from nltk.corpus import gutenberg
corpus = gutenberg.open(gutenberg.fileids()[0]).read()
from nltk.tokenize import sent_tokenize, word_tokenize, regexp_tokenize, TweetTokenizer
# {문장, ...} ㅌ {문단, ...} ㅌ 문서, 문장이 문서를 구성하는 가장 작은 단위
# 문장을 분리
len(corpus.splitlines()), len(sent_tokenize(corpus))
sum([len(s) for s in sent_tokenize(corpus)])/7493
116.64780461764313
sent_tokenize('한글에서는 어떻게 분리하는지 봅시다... 볼까요 ? 볼래요!')
['한글에서는 어떻게 분리하는지 봅시다... 볼까요 ?', '볼래요!']
.?! + 띄어쓰기(공백문자)
data = '''
정부의 의과대학 정원 증원에 반발하는 전국 의대 교수들의 '무더기 사직'이 현실화됐다. 교수들은 정부가 '2000명 증원'을 철회하는 것이 대화의 선결 조건이라며 강경한 입장이다. 

전국의과대학교수협의회(전의교협)는 25일 "의대 입학정원 증원은 의대 교육의 파탄을 넘어 의료체계를 붕괴시킬 게 자명하다"며 정부에 의대 증원 배정 선(先)철회를 촉구하고 나섰다. 

전의교협은 "정부에 의한 입학 정원과 정원 배정 철회가 없는 한 이번 위기는 해결될 수 없다"며 "정부의 철회 의사가 있다면 국민들 앞에서 모든 현안을 논의할 준비가 돼 있다"고 했다.

윤석열 대통령이 전날 집단사직 전공의에 대한 행정처분 등에 대해 '유연한 처리'를 주문하였고, 동시에 한동훈 국민의힘 비상대책위원장 겸 총괄선거대책위원장이 '중재'를 자처하고 나섰지만 증원 정책 철회 없이는 의미 있는 대화 진전은 힘들다는 게 의대 교수들 입장이다. 
'''

len([s for s in data.splitlines() if len(s) > 1]), len(sent_tokenize(data))
(4, 5)
len(corpus.split()), len(word_tokenize(corpus)),\
len(set(corpus.split())), len(set(word_tokenize(corpus)))
(158167, 191785, 17409, 8406)
len(data.split()), len(word_tokenize(data)),\
len(set(data.split())), len(set(word_tokenize(data)))
(104, 135, 95, 107)
len(regexp_tokenize(corpus, r'\b\w+\b')), len(set(regexp_tokenize(corpus, r'\b\w+\b')))
(161983, 7723)
word_tokenize, regexp_tokenize, TweetTokenizer
음절, 어절, 형태소, ..., 
Ngram => P, Markov Assumption
entropy, perplexity => P
BPE, BT, SPT, + MA
Tokenizing => Normalizing(Stopwords...) => Feature Selection => Model
tt = TweetTokenizer()
tt.tokenize(':) =( ㅠㅠ'), word_tokenize(':)')
([':)', '=(', 'ㅠㅠ', 'ㅇ아아아'], [':', ')'])
len(tt.tokenize(corpus)), len(set(tt.tokenize(corpus)))
(193228, 8938)
token => 어절, 음절, 단어, 형태소, 어간, 어미, 어근, 접사, 구, 품사, ...
from nltk.text import Text
t = Text(word_tokenize(corpus))
t.count('Emma')
855
t.vocab().B(), t.vocab().N(), len(t.tokens)
(8406, 191785, 191785)
t.vocab().freq('Emma')
=> FreqDist
0.004458117162447532
t.vocab().most_common(50)
[(',', 12016),
 ('.', 6355),
 ('to', 5125),
 ('the', 4844),
 ('and', 4653),
 ('of', 4272),
 ('I', 3177),
 ('--', 3100),
 ('a', 3001),
 ("''", 2452),
 ('was', 2383),
 ('her', 2360),
 (';', 2353),
 ('not', 2242),
 ('in', 2103),
 ('it', 2103),
 ('be', 1965),
 ('she', 1774),
 ('``', 1735),
 ('that', 1729),
 ('you', 1664),
 ('had', 1605),
 ('as', 1387),
 ('he', 1365),
 ('for', 1320),
 ('have', 1301),
 ('is', 1221),
 ('with', 1185),
 ('very', 1151),
 ('but', 1148),
 ('Mr.', 1091),
 ('his', 1084),
 ('!', 1063),
 ('at', 996),
 ('so', 918),
 ("'s", 866),
 ('Emma', 855),
 ('all', 831),
 ('could', 824),
 ('would', 813),
 ('been', 755),
 ('him', 748),
 ('on', 674),
 ('Mrs.', 668),
 ('any', 651),
 ('?', 621),
 ('my', 619),
 ('no', 616),
 ('Miss', 592),
 ('were', 590)]
t.collocations()
Mr. Knightley; Mrs. Weston; Frank Churchill; Mr. Elton; Miss
Woodhouse; Miss Bates; Mrs. Elton; Miss Fairfax; Mr. Weston; Jane
Fairfax; every thing; Mr. Woodhouse; every body; young man; great
deal; dare say; Maple Grove; Mrs. Goddard; John Knightley; Miss Smith
t.count('Weston')
429
t.concordance('Emma')
Displaying 25 of 855 matches:
[ Emma by Jane Austen 1816 ] VOLUME I CHAPT
ane Austen 1816 ] VOLUME I CHAPTER I Emma Woodhouse , handsome , clever , and 
both daughters , but particularly of Emma . Between _them_ it was more the int
 friend very mutually attached , and Emma doing just what she liked ; highly e
r own . The real evils , indeed , of Emma 's situation were the power of havin
ding-day of this beloved friend that Emma first sat in mournful thought of any
ing only half a mile from them ; but Emma was aware that great must be the dif
y . It was a melancholy change ; and Emma could not but sigh over it , and wis
 the rest of her life at Hartfield . Emma smiled and chatted as cheerfully as 
able to tell her how we all are . '' Emma spared no exertions to maintain this
 ' I have a great regard for you and Emma ; but when it comes to the question 
ful , troublesome creature ! '' said Emma playfully . `` That is what you have
e few people who could see faults in Emma Woodhouse , and the only one who eve
is was not particularly agreeable to Emma herself , she knew it would be so mu
g thought perfect by every body . `` Emma knows I never flatter her , '' said 
t be a gainer . '' `` Well , '' said Emma , willing to let it pass -- '' you w
re of meeting every day . '' `` Dear Emma bears every thing so well , '' said 
ss her more than she thinks for . '' Emma turned away her head , divided betwe
nd smiles . `` It is impossible that Emma should not miss such a companion , '
en one matter of joy to me , '' said Emma , '' and a very considerable one -- 
od to them , by interference . '' `` Emma never thinks of herself , if she can
etter thing . Invite him to dinner , Emma , and help him to the best of the fi
 could not think , without pain , of Emma 's losing a single pleasure , or suf
 of her companionableness : but dear Emma was of no feeble character ; she was
, was so just and so apparent , that Emma , well as she knew her father , was 
t.dispersion_plot(['Emma', 'Weston', 'Elton'])

t.similar('Emma')
she it he i weston you her harriet elton him me knightley jane that
and the all there they them
collocation(연어), co-occurence(공기어)
-> 이웃한 패턴       -> 같이 자주 나오는 표현
from nltk.collocations import BigramAssocMeasures, BigramCollocationFinder
bigram = BigramCollocationFinder.from_words(word_tokenize(corpus))
bigram.nbest(BigramAssocMeasures.pmi, 10)
[('26th', 'ult.'),
 ('Abominable', 'scoundrel'),
 ('Agricultural', 'Reports'),
 ('Austen', '1816'),
 ('Baronne', "d'Almane"),
 ('Candles', 'everywhere.'),
 ('Clayton', 'Park'),
 ('Comtesse', "d'Ostalis"),
 ('DEAR', 'MADAM'),
 ('Farmer', 'Mitchell')]
Ngram => Language Model => NLU, NLG
P(의과대학|정부의)
P(정원|정부의, 의과대학)
P(증원에|정부의, 의과대학, 정원) ~ P(증원에|정원, 예산)
                            P(정원,예산,증원에)/P(정원,예산)
                            P(증원에|예산) => P(예산,증원에)/P(예산)
                                           count(예산,증원에)/N / count(예산)/N
                                           count(예산,증원에)/count(예산)

P(F|A,B,C,D,E)

Bigram => Markov 1st Assumption
Trigram => Markov 2nd Assumptino

S => A, B, C, D, ..., Z
P(S) => P(Z|A,...,Y)P(A,...,Y)
     => P(Z|Y)P(Y|X)P(X|W)...
    
Ngram, Ngram Lanuage Model
def ngram(text, n=2):
    result = list()
    for i in range(len(text)-(n-1)):
        result.append(''.join(text[i:i+n]))
    return result

ngram('의대 교수 줄사직 시작됐다.'.split())
['의대교수', '교수줄사직', '줄사직시작됐다.']
bigram = ngram(word_tokenize(corpus))
unigram = ngram(word_tokenize(corpus), 1)
bigramText = Text(bigram)
unigramText = Text(unigram)
unigramText.count('Emma'), unigramText.vocab().N(), \
unigramText.count('Emma')/unigramText.vocab().N(), unigramText.vocab().freq('Emma')
(855, 191785, 0.004458117162447532, 0.004458117162447532)
import re

def findToken(k, t):
    result = list()
    for token in t:
        if re.match(k, token):
            result.append(token)
            
    return result

# for k in findToken('Emma', bigramText.tokens):
#     print(k, bigramText.count(k)/unigramText.count('Emma'))
    
nextword = {k:bigramText.count(k)/unigramText.count('Emma')
 for k in findToken('Emma', bigramText.tokens)}
sorted(nextword.items(), key=lambda r:r[1], reverse=True)[:5]
[('Emma,', 0.21403508771929824),
 ("Emma's", 0.08654970760233918),
 ('Emmawas', 0.07485380116959064),
 ('Emmacould', 0.07134502923976609),
 ('Emma.', 0.05847953216374269)]
```