## 1. https://nlp.stanford.edu/IR-book/newslides.html

## 2. 판다스 -> 정형 데이터 최적 처리

## 3. map => series. function. dict

## 4. applymap => dataframe. function

## 5. transform => groupby

## 6. apply => series. function. dataframe. function. axis

## 7. ufunc => numpy 기반 모든 함수 사용 가능

## 8. FacetGrid

## 9. 콜드 스타트. 롱테일. gray sheep & black sheep 문제

## 10. corelation table

## 11. pandas.read_csv (names : 칼럼명) => 변경할 칼럼명을 적어서 칼럼명을 바꿀 수 있다.

## 12. rename => 칼럼명 변경 가능

## 13. pivot -> 사용자 지정 테이블 생성 가능

## 14. povot_table => 두개 이상의 값이 들어감. 

## 15. crosstab => 모양 자유롭게 생성 가능

## 16. unstack().fillna(0)

## 17. corr() => 컬럼 와이즈, 페어 와이즈 => 칼럼끼리의 연산

## 18. sort_values => 정렬

## 19. 함수 사용해서 편리하게 추출 가능.

```python
https://nlp.stanford.edu/IR-book/newslides.html

trainingSet = [(1, 'Chinese Beijing Chinese', 'yes'),
               (2, 'Chinese Chinese Shanghai', 'yes'),
               (3, 'Chinese Macao', 'yes'),
               (4, 'Tokyo Japan Chinese', 'no')]
testSet = [(5, 'Chinese Chinese Chinese Tokyo Japan')]
image.png

from nltk.tokenize import word_tokenize
from nltk.text import Text

C = ('yes', 'no')

def training(C, D):
    # 1
    V = list()
    for d in D:
        for v in word_tokenize(d[1].lower()):
            if v not in V:
                V.append(v)
    
    # 2
    N = len(D)
    
    # 3
    prior = dict() # 5
    condprob = dict() # 10
    for c in C:
        # 4
        Nc = len([d for d in D if d[-1] == c])
        # 5
        prior[c] = Nc/N
        # 6
        textc = Text(word_tokenize('\n'.join([d[1] for d in D if d[-1] == c]).lower()))
        # 7
#         for v in V:
#             # 8 - #6
#             Tct = textc.count(v)
        # 7
        Tct = dict()
        for v in V:
            Tct[v] = textc.count(v)
            
        # 9
        for v in V:
            # 10 - Smoothing
            if v not in condprob:
                condprob[v] = dict()

            condprob[v][c] = (textc.count(v)+1)/(textc.vocab().N()+len(V))
#                 print(f'{textc.count(v)}+1/{textc.vocab().N()}+{len(V)}')
            
    return V, prior, condprob
    
V, prior, condprob = training(C, trainingSet)
image.png

from math import log

def testing(C, V, prior, condprob, d):
    # 1
    W = list()
    for t in d:
        W.append(word_tokenize(t[1].lower())) # 학습과 동일한 방식으로 토큰화
    # 2 
    score = list()
    for w in W: # 문서마다
        # 3
        score.append(dict())
#         print(f'P(c={c})={prior[c]}')
        # 4
        for c in C:
            score[-1][c] = prior[c]
            for t in w: # 문서내 토큰마다
                # 5
                if t in V:
                    score[-1][c] += log(condprob[t][c])
#                     print(f'P({t}|{c})={condprob[t][c]}')
                    
    return score, {(i+1):max(s, key=s.get) for i, s in enumerate(score)}

testing(C, V, prior, condprob, testSet)
([{'yes': -7.070008240392129, 'no': -7.270386983881371}], {1: 'yes'})
testing(C, V, prior, condprob, [(None, 'Tokyo'), (None, 'Chinese')])
([{'yes': -1.8890573296152589, 'no': -1.2540773967762742},
  {'yes': -0.09729786038720367, 'no': -1.2540773967762742}],
 {1: 'no', 2: 'yes'})
# 스팸 분류기
C, Dataset(스팸), Tokenizer(한글), NB Classifier
from requests.sessions import Session

cookies = '''
'''

sess = Session()
for line in cookies.splitlines():
    if len(line) > 1:
        c = line.split()
        sess.cookies.set(c[0], c[1])
import re

url = 'https://mail.naver.com/json/list'
params = {
    'folderSN': 0,
    'page': 1,
    'viewMode': 'time',
    'previewMode': 1,
    'sortField': 1,
    'sortType': 0,
    'u': '본인계정'
}

dataset = list()

for i in range(1, 10):
    params['page'] = i
    resp = sess.post(url, params=params)
    if re.search(r'text/plain', resp.headers['content-type']):
        for mail in resp.json()['mailData']:
            dataset.append((mail['subject'], 'ham'))
params['folderSN'] = 5
for i in range(1, 10):
    params['page'] = i
    resp = sess.post(url, params=params)
    if re.search(r'text/plain', resp.headers['content-type']):
        for mail in resp.json()['mailData']:
            dataset.append((mail['subject'], 'spam'))
len(dataset)
270
```