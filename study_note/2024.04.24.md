```python
Relevance => Similarity(Likelihood, Bayes, )
          => Vector Space
    
Dimension => Concepts(Orthogonal, Independent) = Bag of Words
          => Term(Token), Class(Category), Cluster...
    
V = {w1, w2, ..., wn}
Q = {t1, t2, ..., tm ㅌ V}
Di = {ti1, ti2, ..., tik ㅌ V}
C = {d1, d2, ..., di}
Rel(Q, D) = Sim(Q, D) => Sim(Rep(Q), Rep(D))

Document-Term Matrix(DTM) => 색인(느림)
Term-Document Matrix(TDM) => 역색인(Linked List)
in-memory     on-disk(Inverted Index)
  Term1   -   Doc1, Doc2, ...
    
                            Zipf
Rep? => Dim's importance => Weight(문서내 많이 나온 중요TF, 다른 문서에서 안나온 중요IDF)
image.png

from math import log

tf1 = lambda f: 1 if f > 0 else 0
tf2 = lambda f: f
tf3 = lambda f, s: f/s
tf4 = lambda f: log(1+f)
tf5 = lambda f, m: .5 + .5*(f/m)
tf6 = lambda k, f, m: k + (1-k)*(f/m)
import matplotlib.pyplot as plt
f = [1,10,100,1000,10000]
plt.plot([tf1(v) for v in f], c='k')         # boolean
# plt.plot([tf2(v) for v in f])              # raw freq
plt.plot([tf3(v, sum(f)) for v in f], c='r') # normalization
# plt.plot([tf4(v) for v in f], c='g')       # log
plt.plot([tf5(v, max(f)) for v in f], c='b') # K=.5 double normalization
plt.plot([tf6(0, v, max(f)) for v in f], c='g') # K, double normalization
[<matplotlib.lines.Line2D at 0x116b4ce10>]

image.png

idf1 = lambda df:1
idf2 = lambda df, n:log(n/df)
idf3 = lambda df, n:log(n/(1+df))+1
idf4 = lambda df, m:log(m/(1+df))
idf5 = lambda df, n:log((n-df)/df)
df = [1, 10, 100, 1000, 10000]
# plt.plot([idf1(v) for v in df], c='k')
plt.plot([idf2(v, max(df)+1) for v in df], c='r')
plt.plot([idf3(v, max(df)+1) for v in df], c='g')
plt.plot([idf4(v, max(df)) for v in df], c='b')
# plt.plot([idf5(v, max(df)+1) for v in df])
[<matplotlib.lines.Line2D at 0x116ba1f50>]

tf6(k=0), idf2
plt.plot([tf6(0, f[0], max(f))*idf2(v, max(df)+1) for v in df], c='r')
plt.plot([tf6(0, f[4], max(f))*idf2(v, max(df)+1) for v in df], c='g')
[<matplotlib.lines.Line2D at 0x127a62b10>]

^                                                     ^
녹색, TF가 높고, DF가 낮은                 TF가 높고, DF가 높은
Dictionay              Posting
단어:위치               문서번호:빈도:다음위치
- TF: 문서별로 max freq 찾아야 함
- IDF: df
from konlpy.corpus import kobill
from konlpy.tag import Komoran
from nltk.tokenize import word_tokenize, sent_tokenize
from struct import pack, unpack
import re
ma = Komoran()
def ngram(s, n=2):
    rst = list()
    for i in range(len(s)-(n-1)):
        rst.append(s[i:i+n])
    return rst
from os import listdir

def fileids(path):
    return [path+('/' if path[-1] != '/' else '')+f
            for f in listdir(path) if re.search(r'[.]txt$', f)]
def preprocessing(d):
    d = d.lower()
    d = re.sub(r'[a-z0-9\-_.]{3,}@[a-z0-9\-_.]{3,}(?:[.]?[a-z]{2})+', ' ', d)
    d = re.sub(r'[‘’ⓒ\'\"“”…=□*◆:]', ' ', d)
    d = re.sub(r'\s+', ' ', d)
    d = re.sub(r'^\s|\s$', '', d)
    return d
def indexer(f): # Map
    with open(f, 'r', encoding='utf8') as fp:
        text = preprocessing(fp.read())
    
    localMap = dict()
    for t in word_tokenize(text):
        if t not in localMap:
            localMap[t] = 0
        localMap[t] += 1
        
    for s in sent_tokenize(text): # 메모리 오류
        for t in ma.morphs(s):
            if t not in localMap:
                localMap[t] = 0
            localMap[t] += 1
        
    for t in ngram(text):
        if t not in localMap:
            localMap[t] = 0
        localMap[t] += 1
        
    return localMap
def indexing(path):
    C = [{'no':i+1,'name':f,'maxfreq':0} for i,f in enumerate(fileids(path))]
    Dictionary = dict()
    Posting = 'posting.dat'
    
    fp = open(Posting, 'wb')

    for i, f in enumerate(C):
        mapped = indexer(f['name'])
        # reducing
        for t, f in mapped.items():
            if f > C[i]['maxfreq']:
                C[i]['maxfreq'] = f
                
            if t not in Dictionary.keys():
                Dictionary[t] = dict()
                Dictionary[t]['ppos'] = fp.tell()
                Dictionary[t]['wpos'] = -1
                Dictionary[t]['df'] = 1
                fp.write(pack('iii', i, f, -1))
            else:
                Dictionary[t]['df'] += 1
                pos = Dictionary[t]['ppos']
                Dictionary[t]['ppos'] = fp.tell()
                fp.write(pack('iii', i, f, pos))
        
    fp.close()
        
    return C, Dictionary, Posting

C, V, posting = indexing('./news/')
N = len(C)

fp2 = open('weight.dat', 'wb')

with open(posting, 'rb') as fp:
    for t, info in V.items():
#         df = 0
#         opos = pos
#         while pos != -1:
#             fp.seek(pos)
#             i, f, pos = unpack('iii', fp.read(12))
#             df += 1
#             
#         pos = opos
        df = info['df']
        V[t]['wpos'] = fp2.tell()
        pos = info['ppos']
        while pos != -1:
            fp.seek(pos)
            i, f, pos = unpack('iii', fp.read(12))
            w = tf6(0, f, C[i]['maxfreq']) * idf2(df, N)
            fp2.write(pack('if', i, w))
fp2.close()
with open('weight.dat', 'rb') as fp:
    for t, info in V.items():
        i = 0
        while i < info['df']:
            fp.seek(info['wpos']+8*i)
            no, w = unpack('if', fp.read(8))
            i += 1
2/34, log(N/1), (2/34)*log(N/1)
(0.058823529411764705, 3.9318256327243257, 0.23128386074848975)
fileids('./news/')[5]
'./news/0004319099.txt'
qkv = indexer(fileids('./news/')[5])
qw = dict()
maxfreq = max(qkv.values())
N = len(C)
for t, f in qkv.items():
    w = tf6(0, f, maxfreq)*idf2(V[t]['df'], N)
    qw[t] = w
# Distance
result = dict()

with open('weight.dat', 'rb') as fp:
    for t, info in V.items():
        i = 0
        while i < info['df']:
            fp.seek(info['wpos']+8*i)
            no, w = unpack('if', fp.read(8))
            
            if no not in result:
                result[no] = 0.0
                
            result[no] += ((qw[t] if t in qw else 0) - w)**2
            
            i += 1
# list(map(lambda r:(C[r[0]], r), sorted(result.items(), key=lambda r:r[1])))
for i, dist in sorted(result.items(), key=lambda r:r[1]):
    print(C[i]['name'], dist)
    with open(C[i]['name'], 'r', encoding='utf8') as f:
        d = f.read()
        kv = indexer(C[i]['name'])
        print(len(d), len(kv), sum(kv.values()))
./news/0004319099.txt 5.766152109182643e-15
2395 1379 3700
./news/0005017436.txt 5.973258216091687
4091 2379 6666
./news/0005700276.txt 6.344657234361346
3323 1814 5362
./news/0003350027.txt 6.7341773894570975
2483 1546 3830
./news/0012452262.txt 7.033054267984212
1945 1119 2847
./news/0003350025.txt 7.513221387874604
2961 1805 4567
./news/0000452995.txt 7.842064714916135
2000 1272 2995
./news/0000651954.txt 7.916282436574732
1708 1132 2657
./news/0005017507.txt 8.061064499928621
2586 1432 3880
./news/0004965009.txt 8.204046349508705
2194 1336 3362
./news/0005397125.txt 8.490861888244565
3362 1906 5433
./news/0003918339.txt 8.70785210564157
2675 1575 4215
./news/0005278375.txt 8.740826862923354
2005 1227 2988
./news/0003287006.txt 9.250006818833032
2586 1631 4256
./news/0002285927.txt 9.604718212065393
2179 1343 3322
./news/0007438076.txt 9.791650922512574
2381 1330 3681
./news/0012450211.txt 9.949923553448928
4851 2455 7635
./news/0005161727.txt 10.068080602522395
1786 1103 2663
./news/0000792571.txt 11.099779059656269
3039 1784 4854
./news/0005397777.txt 11.24817792756414
1968 1185 2936
./news/0004319090.txt 11.423441713034569
1851 1093 2700
./news/0007437289.txt 11.567178176453101
2300 1377 3525
./news/0005700520.txt 11.869201782037221
2931 1739 4694
./news/0011689407.txt 12.170287455676286
1585 1039 2282
./news/0011689239.txt 12.449507391675224
1864 1161 2790
./news/0005700595.txt 12.569696262386183
3736 2136 6098
./news/0005397120.txt 12.937530959766537
2646 1477 4140
./news/0003918255.txt 13.04520272569555
2058 1267 3097
./news/0005278789.txt 13.312643944123755
3066 1775 4736
./news/0000823331.txt 13.837443134591377
4635 2374 7620
./news/0002015097.txt 14.000957483497809
2236 1250 3437
./news/0005162239.txt 14.481298178088409
2248 1352 3403
./news/0000869473.txt 14.773695157316736
1883 1127 2820
./news/0005162312.txt 14.861004859360229
2332 1464 3528
./news/0005397071.txt 15.822603228540741
2167 1252 3248
./news/0003555640.txt 16.531185296790742
2819 1568 4409
./news/0000271067.txt 16.626668124500245
2063 1321 3104
./news/0000020040.txt 16.661979674409597
2001 1216 3038
./news/0002015230.txt 17.04779191251308
9283 3426 15753
./news/0001154752.txt 17.46018053006316
1787 1108 2761
./news/0002682510.txt 17.661457631830523
1908 1185 2784
./news/0007436688.txt 17.949966182156192
2812 1656 4389
./news/0003918245.txt 18.299662739588783
1431 982 2216
./news/0000981048.txt 18.326600390990006
2840 1538 4399
./news/0014588125.txt 18.394075420931657
2211 1335 3345
./news/0007436055.txt 21.010842528674058
2178 1290 3303
./news/0002628697.txt 21.75947054074176
1841 1115 2756
./news/0005162265.txt 23.61788391747478
3315 1836 5284
./news/0005017505.txt 23.957389324965884
2418 1373 3633
./news/0012452115.txt 27.629044643891923
2322 1100 3440
./news/0003192578.txt 31.88931880229611
2074 1272 3158
with open('./news/0005700276.txt', 'r', encoding='utf8') as f:
    print(preprocessing(f.read()))
당근과 채찍 강공 일단 올스톱장기전 준비 대화 협력 모색[이데일리 이지현 기자] 조건 없는 또 기탄없는, 진정성 있는 대화가 이루어지기를 다시 한번 희망하고 또 촉구합니다. 박민수 보건복지부 2차관은 26일 정부서울청사에서 열린 의사 집단행동 중앙재난안전대책본부 정례브리핑에서 의료계를 향해 이같이 말했다. [이데일리 방인권 기자] 정부의 의대 정원 배분에 반발한 전국 의대 교수들이 집단 사직서 제출을 예고한 25일 서울 시내 대학 병원에서 의료진이 발걸음을 옮기고 있다.◇ 미복귀 전공의 처분 시기 미정 전공의들과 의대 교수들은 2000명 증원을 전면 철폐 후 대화하겠다고 으름장을 놓고 있다. 그러면서 전공의에 이어 교수들도 사직 투쟁을 벌이고 있다. 이에 대해 정부는 조건 없이 대화에 나서달라고 호소한 것이다. 정부는 보다 적극적인 의료계와의 대화 노력을 통해 지금의 이 갈등 상황을 조속히 수습해 나간다는 방침이다. 현재 실무 작업에 착수했고, 빠른 시일내 대화의 자리를 마련할 계획이다.박민수 2차관은 의대 교수는 사직서를 내지 말고, 학교와 병원을 지켜주기 바란다 며 많은 국민과 언론, 시민단체 뿐 아니라 정부와 여당 그리고 의료계 내 많은 이들이 대화를 주문하고 있다. 이러한 목소리를 외면해서는 안 된다. 진정성 있는 자세로 조건 없이 대화에 임할 것을 제안한다 고 말했다. 이어 그것이 국민을 위하고, 제자를 위한 길 이라며 소모적인 논쟁을 그치고, 대한민국 보건의료의 미래를 위한 건설적인 논의를 함께해 나가자 고 말했다. 전공의를 향해서도 조건없이 돌아오라고 호소했다. 박 차관은 전공의 집단행동으로 현장에 남아 있는 의사와 간호사 등 의료진은 소진되고 있음에도, 환자들은 제때 진료를 받지 못하고 있으며, 병원은 하루에 수억, 십수억의 손실이 생기고 있다 며 무엇보다 가장 큰 비용은 지금도 생사의 기로에 있는 환자와 그 가족들의 고통이다. 어떤 이유로도 전공의의 집단행동은 정당화되지 않는다. 지금이라도 속히 현장으로 복귀할 것을 다시 한번 촉구한다 고 말했다. 정부는 당초 이날부터 미복귀 전공의에 대해 행정처리 하려던 것을 미룬 상태다. 윤석열 대통령의 유연한 처리 주문에 후속 작업을 진행 중인 것으로 알려졌다. 박 차관은 처분 시기, 처분의 기간 이런 것들이 검토 대상이 될 수 있을 것 이라며 의료계와의 대화가 이루어져야 좀 더 분명하게 정할 수 있을 것 같다. 현재는 아직 결정된 바가 없다 고 말했다. 박민수 보건복지부 2차관◇ 상급병원 pa 간호사·협력병원 추가 정부는 의대 증원 후속 조치를 5월 안에 마무리한다는 계획이다. 국무조정실 주관으로 만든 의대교육 지원 태스크포스(tf) 는 이날 회의를 열고 대학별 교육여건 개선 수요조사 계획을 논의한다. 교육부 현장점검팀은 이날부터 29일까지 각 의대를 방문해 교육여건 개선에 필요한 현장 의견을 듣는다.박 차관은 의사 증원을 포함한 의료개혁에 대한 정부의 의지는 흔들림이 없다 며 정부는 국민 여러분께 약속드린 대로 의료개혁을 차질 없이 마무리하겠다 고 밝혔다. 정부는 전공의에 이어 교수들까지 사직하는 상황에서도 진료 공백이 발생하지 않도록 추가 대책도 추진 중이다. 우선 상급종합병원과 100개 진료협력병원 간 환자 의뢰와 회송이 원활히 이루어지도록 암 진료 등 전문 분야를 고려해 지정을 확대한다. 진료지원(pa) 간호사 도 확대한다. 현재 상급종합병원에서 약 5000명의 진료지원 간호사가 활동하고 있다. 향후 상급종합병원은 1599명, 공공의료기관은 320명 등 총 1900여명의 진료지원 간호사가 추가로 증원될 예정이다. 이달 말에는 332개 종합병원을 대상으로 한 조사까지 완료하면 그 규모는 더 늘어날 것으로 보인다.정부는 앞으로 정부는 시범사업의 내실 있는 운영을 위해 진료지원 간호사 표준 교육·훈련 프로그램 을 제공할 계획이다. 우선, 수술, 외과, 내과, 응급 중증 4개 분야 프로그램을 4월 중 제공하고, 시범사업 기간 심혈관, 신장투석, 상처장루, 집중영양 4개 분야프로그램을 추가로 확대할 방침이다. 시범사업 이후에도 교육·훈련 프로그램 분야를 지속 확대할 계획이다. 박 차관은 내실 있는 시범사업 운영을 바탕으로 진료지원 간호사 제도화에 필요한 조치도 추진해 나가겠다 고 강조했다. 이지현( ) 기자 프로필 이지현 기자 구독 구독중 구독자 0 응원수 0 안녕하세요. 이지현 기자입니다. 조건 없이 대화하자 의대 교수 왕따 조장 정부 대응(상보) [속보]정부 의료계에 조건 없이 대화하자 이데일리의 구독 많은 기자를 구독해보세요! 닫기 copyright 이데일리. all rights reserved. 무단 전재 및 재배포 금지. 이 기사는 언론사에서 사회 섹션으로 분류했습니다. 기사 섹션 분류 안내 기사의 섹션 정보는 해당 언론사의 분류를 따르고 있습니다. 언론사는 개별 기사를 2개 이상 섹션으로 중복 분류할 수 있습니다. 닫기 구독 메인에서 바로 보는 언론사 편집 뉴스 지금 바로 구독해보세요! 구독중 메인에서 바로 보는 언론사 편집 뉴스 지금 바로 확인해보세요! [22대 총선] 주요 격전지 판세분석 확인하세요! qr 코드를 클릭하면 크게 볼 수 있어요. qr을 촬영해보세요. [22대 총선] 주요 격전지 판세분석 확인하세요! 닫기 2024년 신년운세·토정비결·사주 확인! 주요뉴스해당 언론사에서 선정하며 언론사 페이지(아웃링크)로 이동해 볼 수 있습니다. [속보] 전공의 조건 없이 현장으로 돌아오라 총선 화두에 오른 셰셰 논란, 중국 현지 반응은[중국나라] 시신 보기 무섭다 딸 장례식에도 오지 않은 부모[그해 오늘] 멱살 잡고싶네 악성민원·신상털기에 숨진 공무원 순직 신청 음주운전·범인 도피 가수 이루, 항소심도 집유 죄송하다 이 기사를 추천합니다 기사 추천은 24시간 내 50회까지 참여할 수 있습니다. 닫기 쏠쏠정보 0 흥미진진 0 공감백배 0 분석탁월 0 후속강추 0 모두에게 보여주고 싶은 기사라면?beta 이 기사를 추천합니다 버튼을 눌러주세요. 집계 기간 동안 추천을 많이 받은 기사는 네이버 자동 기사배열 영역에 추천 요소로 활용됩니다. 레이어 닫기 이데일리 언론사홈 바로가기 언론사 구독 후 기사보기 구독 없이 계속 보기 기자 구독 후 기사보기 구독 없이 계속 보기
```

```python

from sklearn.datasets import load_iris
data = load_iris(as_frame=True)
iris = data.frame
iris
sepal length (cm)	sepal width (cm)	petal length (cm)	petal width (cm)	target
0	5.1	3.5	1.4	0.2	0
1	4.9	3.0	1.4	0.2	0
2	4.7	3.2	1.3	0.2	0
3	4.6	3.1	1.5	0.2	0
4	5.0	3.6	1.4	0.2	0
...	...	...	...	...	...
145	6.7	3.0	5.2	2.3	2
146	6.3	2.5	5.0	1.9	2
147	6.5	3.0	5.2	2.0	2
148	6.2	3.4	5.4	2.3	2
149	5.9	3.0	5.1	1.8	2
150 rows × 5 columns

from sklearn.model_selection import cross_val_score
from sklearn.linear_model import LogisticRegression
cross_val_score(LogisticRegression(), iris.iloc[:,:-1], iris.target, cv=10)
C:\Users\user\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
array([1.        , 0.93333333, 1.        , 1.        , 0.93333333,
       0.93333333, 0.93333333, 1.        , 1.        , 1.        ])
from sklearn.neighbors import KNeighborsClassifier
cross_val_score(KNeighborsClassifier(), iris.iloc[:,:-1], iris.target, cv=10)
array([1.        , 0.93333333, 1.        , 1.        , 0.86666667,
       0.93333333, 0.93333333, 1.        , 1.        , 1.        ])
from sklearn.model_selection import cross_val_predict
cross_val_predict(KNeighborsClassifier(), iris.iloc[:,:-1], iris.target, cv=10)
array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
       0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
       1, 1, 1, 1, 2, 1, 2, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1,
       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2,
       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])
from sklearn.model_selection import KFold, StratifiedKFold
cv = StratifiedKFold()
t = cv.split(iris.iloc[:,:-1], iris.target)
next(t)
(array([ 10,  11,  12,  13,  14,  15,  16,  17,  18,  19,  20,  21,  22,
         23,  24,  25,  26,  27,  28,  29,  30,  31,  32,  33,  34,  35,
         36,  37,  38,  39,  40,  41,  42,  43,  44,  45,  46,  47,  48,
         49,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,  70,  71,
         72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,  84,
         85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,
         98,  99, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120,
        121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133,
        134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146,
        147, 148, 149]),
 array([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  50,  51,  52,
         53,  54,  55,  56,  57,  58,  59, 100, 101, 102, 103, 104, 105,
        106, 107, 108, 109]))
cross_val_predict(KNeighborsClassifier(), iris.iloc[:,:-1], iris.target, cv=cv)
array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
       0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
       1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1,
       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2,
       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])
Model = algorithm (hyperparameter) + data
from sklearn.ensemble import VotingClassifier
 
vc = VotingClassifier([('knn',KNeighborsClassifier()),('lr', LogisticRegression())])
from sklearn.model_selection import train_test_split
X_train,X_test,y_train,y_test = train_test_split(iris.iloc[:,:-1], iris.target)
vc.fit(X_train,y_train)
C:\Users\user\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
  VotingClassifier?i
knn

 KNeighborsClassifier?
lr

 LogisticRegression?
vars(vc)
{'estimators': [('knn', KNeighborsClassifier()), ('lr', LogisticRegression())],
 'voting': 'hard',
 'weights': None,
 'n_jobs': None,
 'flatten_transform': True,
 'verbose': False,
 'le_': LabelEncoder(),
 'classes_': array([0, 1, 2]),
 'estimators_': [KNeighborsClassifier(), LogisticRegression()],
 'named_estimators_': {'knn': KNeighborsClassifier(),
  'lr': LogisticRegression()},
 'feature_names_in_': array(['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)',
        'petal width (cm)'], dtype=object)}
dir(vc)
['__abstractmethods__',
 '__annotations__',
 '__class__',
 '__delattr__',
 '__dict__',
 '__dir__',
 '__doc__',
 '__eq__',
 '__format__',
 '__ge__',
 '__getattribute__',
 '__getstate__',
 '__gt__',
 '__hash__',
 '__init__',
 '__init_subclass__',
 '__le__',
 '__lt__',
 '__module__',
 '__ne__',
 '__new__',
 '__reduce__',
 '__reduce_ex__',
 '__repr__',
 '__setattr__',
 '__setstate__',
 '__sizeof__',
 '__sklearn_clone__',
 '__str__',
 '__subclasshook__',
 '__weakref__',
 '_abc_impl',
 '_build_request_for_signature',
 '_check_feature_names',
 '_check_n_features',
 '_check_voting',
 '_collect_probas',
 '_doc_link_module',
 '_doc_link_template',
 '_doc_link_url_param_generator',
 '_estimator_type',
 '_get_default_requests',
 '_get_doc_link',
 '_get_metadata_request',
 '_get_param_names',
 '_get_params',
 '_get_tags',
 '_log_message',
 '_more_tags',
 '_parameter_constraints',
 '_predict',
 '_replace_estimator',
 '_repr_html_',
 '_repr_html_inner',
 '_repr_mimebundle_',
 '_required_parameters',
 '_set_params',
 '_sk_visual_block_',
 '_sklearn_auto_wrap_output_keys',
 '_validate_data',
 '_validate_estimators',
 '_validate_names',
 '_validate_params',
 '_weights_not_none',
 'classes_',
 'estimators',
 'estimators_',
 'feature_names_in_',
 'fit',
 'fit_transform',
 'flatten_transform',
 'get_feature_names_out',
 'get_metadata_routing',
 'get_params',
 'le_',
 'n_features_in_',
 'n_jobs',
 'named_estimators',
 'named_estimators_',
 'predict',
 'predict_proba',
 'score',
 'set_fit_request',
 'set_output',
 'set_params',
 'set_score_request',
 'transform',
 'verbose',
 'voting',
 'weights']
from sklearn.ensemble import BaggingClassifier
bb = BaggingClassifier()
bb.fit(X_train,y_train)

  BaggingClassifier?i
BaggingClassifier()
from sklearn.ensemble import StackingClassifier
lr = LogisticRegression()
lr.fit(X_train,y_train)
C:\Users\user\anaconda3\lib\site-packages\sklearn\linear_model\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

  LogisticRegression?i
LogisticRegression()
from sklearn.metrics import confusion_matrix, classification_report
confusion_matrix(y_test, lr.predict(X_test))
array([[14,  0,  0],
       [ 0, 15,  1],
       [ 0,  0,  8]], dtype=int64)
print(classification_report(y_test, lr.predict(X_test)))
              precision    recall  f1-score   support

           0       1.00      1.00      1.00        14
           1       1.00      0.94      0.97        16
           2       0.89      1.00      0.94         8

    accuracy                           0.97        38
   macro avg       0.96      0.98      0.97        38
weighted avg       0.98      0.97      0.97        38

 
```