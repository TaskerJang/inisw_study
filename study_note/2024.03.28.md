## 1. graphical perception: the visual decoding of quantitative

## 2. 시각화의 목적 => 학습이 어느정도까지 이루어졌는가

## 3. import 파이썬 파일로 사용 가능.

## 4. as => 축약어

## 5. from 파일명 import 변수 => 해당 파일에서 변수를 가져와라!

## 6. from 파일명 import 변수 as 축약어 => 해당 파일에서 변수를 가져와서 축약어로 사용 가능

## 7. import * => 모든 것을 가져온다. but __ 붙어있으면 안 된다.

## 8. __ => 언더바를 붙인 다는 것은 내부용으로 쓰겠다는 것이다.

## 9. __all__ => 

## 10. state machine => 변수를 지정하지 않고 가장 가까이 있는 것을 찾는다.

## 11. figure(figsize(좌표값, 좌표값))// axes => 전체에서 해당 비율을 그려준다.

## 12. ggplot -> 문법을 활용한 시각화

## 13. polar => 극자표 생성

## 14. plt. 다양한 옵션들 사용 가능.

## 15. subplot vs plot => 

## 16. seborn => 다양한 시각화 기능들 지원

## 17. violinplot, boxplot, boxenplot 등등 다양한 시각화들이 지원된다.

## 18. hue -> 색깔로 구분

```python

# 띄어쓰기 => Ngram(LanguageModel)
q = 'ABCDE...'
i =   ^
r = ''

3gram
r += q[:(n-1)] 0...2 [0,1]
  => 'AB'
while i < len(q)-(n-1):
    1. i=0
       k = r[-(n-1):] => 'AB'
       p1 = freq('AB')
       keys = [freq('AB?'), ....]
       max => condProb. key[-1] => ' '
       r += ' ' => 'AB '
    2. i=0
       k = r[-(n-1):] => 'B '
       r += q[i:i+(n-1)] 1...3 [1,2]
       q[i] = freq('B')
       keys = [freq('B?'), ....]
       max => condProb. key[-1] => '?'
       r += q[i+(n-1)]
       i += 1
    3. i=1
from os import listdir

def fileids(path):
    return list(map(lambda f:path + ('' if path[-1] == '/' else '/')+f,
                    listdir(path)))
def ngram(s, n=2, t=True):
    result = []
    
    if not t:
        s = list(s)
        
    for i in range(len(s)-(n-1)):
        result.append(tuple(s[i:i+n]))
        
    return result
def findKey(gram, key):
    k = tuple(key)
    return list(filter(lambda g:g[:len(k)] == k, gram.keys()))
import re

corpus = list()
for f in fileids('news'):
    with open(f, 'r', encoding='utf8') as fp:
        corpus.append(re.sub(r'^\s+|\s+$', '',
                   re.sub(r'\s+', ' ',
                          re.sub(r'\sCopyright.+', ' ',
                                 re.sub(r'[\xa0-\xff]', ' ', fp.read())))))
from nltk.tokenize import sent_tokenize

gram = {2:list(), 3:list(), 4:[], 5:[]}
for c in corpus:
    for s in sent_tokenize(c):
        gram[2].extend(ngram(s, 2, False))
        gram[3].extend(ngram(s, 3, False))
        gram[4].extend(ngram(s, 4, False))
        gram[5].extend(ngram(s, 5, False))
from collections import Counter

gram = {2:Counter(gram[2]), 3:Counter(gram[3]), 4:Counter(gram[4]),
        5:Counter(gram[5])}
def autoSpacing(q, n=2):
    i = 0
    r = list()
    
    r.append(q[i:i+(n-1)])
    while i < len(q)-(n-1):
        k = ''.join(r[-(n-1):])
        keys = findKey(gram[n], k)
        candidates = {j:gram[n][j] for j in keys}
        try:
            best = sorted(candidates.items(), key=lambda r:r[1], reverse=True)[0]
            if best[0][-1] == ' ':
                r.append(' ')
        except:
            print(k, keys)
        
        r.append(q[i+(n-1)])
        i += 1
        
    return ''.join(r)

autoSpacing(sent_tokenize(corpus[0])[0].replace(' ', ''), 3),\
sent_tokenize(corpus[0])[0]
출실 []
('초고금리 급전대출 ‘사기 주의 보’ 금융 감독원은 최근 불법 대부업자가 최대 수 천만원 의대 출실행을 빌미로 초고금리 불법 거래를 강요 하고 살인적인 이 자를 빼돌린 뒤 잠적하는 사례가 발생하고 있다며 26일 소비자주 의경보를 발령했다.',
 '초고금리 급전 대출 ‘사기 주의보’ 금융감독원은 최근 불법 대부업자가 최대 수천만원의 대출 실행을 빌미로 초고금리 불법 거래를 강요하고 살인적인 이자를 빼돌린 뒤 잠적하는 사례가 발생하고 있다며 26일 소비자 주의경보를 발령했다.')
2gram => P(A,B)/P(A)     => 1음절 단어 + 공백
3gram => P(A,B,C)/P(A,B) => 2음절 단어
[]
q = '대출'
max({k:gram[len(q)+1][k] for k in findKey(gram[len(q)+1], q)}.values()),\
{k:gram[len(q)+1][k] for k in findKey(gram[len(q)+1], q)}
(5,
 {('대', '출', ' '): 5,
  ('대', '출', '이'): 1,
  ('대', '출', '을'): 1,
  ('대', '출', '은'): 1})
# Branch Entropy => stemming
# -sum(p*log p)
# p=> P(x_i|x_0 ... x_i-1)
# =>    P(?|A)
#     = P(A,?)/P(A)
      p = freq(A,?)/freq(A)
       [(AB, 빈도)
        (AC, 빈도)
        (AD, 빈도)]
tokens = Counter('\n'.join(corpus).split())
from math import log
from collections import defaultdict

def stemming(q):
    threshold = 10.0
    for i in range(len(q)):
        candidates = defaultdict(lambda:0)
        given = sum([tokens[k] for k in tokens.keys() if re.match(q[:i+1], k)])
#               sum([빈도                 k.startswith('금')])
        for k in [t for t in tokens.keys() if re.match(q[:i+1], t)]:
            candidates[k[:i+2]] += tokens[k]
        be = -sum([(v/given)*log(v/given) for k, v in candidates.items()])

        if be > threshold:
#             print(i+1, q[:i+1], q[i+1:])
            break
            
        threshold = be

    return (q,) if i == len(q)-1 else (q[:i+1], q[i+1:])
    
stemming('금융감독원')
# 금, 금? BE
# 금융, 금융? BE
# 금융감, 금융감? BE
# 금융감독, 금융감독? BE
L(어근,어간) / R(접사,어미)
('금융', '감독원')
punctuation, re.escape(punctuation)
('!"#$%&\'()*+,-./:;<=>?@[\\]^_`{|}~',
 '!"\\#\\$%\\&\'\\(\\)\\*\\+,\\-\\./:;<=>\\?@\\[\\\\\\]\\^_`\\{\\|\\}\\~')
from string import punctuation
# 문자열 오류나면, 기호들 제거
for t in re.sub(r'[{}]'.format(punctuation), '', corpus[1]).split():
    r = stemming(t)
    if len(r) > 1:
        print(r)
[(k,tokens[k]) for k in tokens.keys() if re.match('금융', k)]
# sum([tokens[k] for k in tokens.keys() if re.match('대출', k)])
[('금융감독원은', 2), ('금융의', 1), ('금융', 3), ('금융교육', 1), ('금융부에서', 1), ('금융권을', 1)]
from collections import defaultdict
{1:1}[2], {1:1}.get(2), {1:1}.get(2, 0)
---------------------------------------------------------------------------
KeyError                                  Traceback (most recent call last)
Cell In[109], line 1
----> 1 {1:1}[2], {1:1}.get(2), {1:1}.get(2, 0)

KeyError: 2
temp = defaultdict(int)
temp[2]
0
# Perplexity
image.png

전체음절 ㅠP(Xt+1|x1...xt)
(4)   => sqrt(1/P(x2|x1) * 1/(P(x3|x1,x2) * 1/P(x4|x1,x2,x3), 4)
                P(x1,x2)    P(x1,x2,x3)     P(x1,x2,x3,x4)    P(x1,x2,x3,x4)      freq(x1,x2,x3,x4)
                --------    -----------     -------------- = ---------------- =   ------------------
                   x1         x1,x2           x1,x2,x3             P(x1)             freq(x1)
Cohesion score
from math import sqrt

def cohesion(t):
    t1 = t[:1]
    freq_t1 = sum([tokens[x] for x in list(filter(lambda k:re.match(t1, k), tokens.keys()))])
#     print(t1, freq_t1)

    threshold = 0.0
    
    for i in range(1, len(t)):
        t2 = t[:i+1]
        freq_t2 = sum([tokens[x] for x in list(filter(lambda k:re.match(t2, k), tokens.keys()))])
        score = (freq_t2/freq_t1)**(1/len(t2))
#         print(t2, freq_t2, freq_t2/freq_t1)
        if score < threshold:
#             print(i)
            break
    
        threshold = score
        
    return (t,) if i == len(t)-1 else (t[:i], t[i:])
        
cohesion('감독원')
('감독원',)
for t in re.sub(r'[{}]'.format(punctuation), '', corpus[1]).split():
    if len(t) > 2:
        r = cohesion(t)
        if len(r) > 1:
            print(r)
[Tokenizing]
re
split(), splitlines() <= 구분자
sent_tokenize, word_tokenize, regex_tokenize, TweetTokinize <= 언어적 X, 구두점(문장기호)
한국어특징; 형태소분석기, 명사추출기, 품사부착기
언어적특징; Ngram => 띄어쓰기, 다음글자 예측
통계적방법; Branch-entropy, Perplexity(=> Cohesion score) => 분절을 생성
         BPE => 분절을 생성, 패턴=>불용어처리
# Byte Pair Encoding
from nltk.corpus import stopwords
stop = stopwords.open('english').read()
from string import punctuation

# Nomarlization
# 1. 대소문자 일치
# 2. 문장기호 어떻게 처리? = 삭제
# 3. 불용어처리 => 선택, 전략

d = "To be or not to be."
r = []
for t in re.sub(f'[{punctuation}]', '', d.lower()).split():
    if t not in stop:
        r.append(t)
r
[]
```