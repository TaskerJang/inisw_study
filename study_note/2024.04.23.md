```python

https://www.cs.virginia.edu/~hw5x/Course/IR2021-Spring/docs/
1. 크롤링 수집된 문서(kobill)
2. 토크나이징 / 노말라이징
3. BoW 문서가 벡터로 표현
from konlpy.corpus import kobill
from konlpy.tag import Komoran
from nltk.tokenize import word_tokenize, sent_tokenize
import re
kobill.fileids()
['1809896.txt',
 '1809897.txt',
 '1809895.txt',
 '1809894.txt',
 '1809890.txt',
 '1809891.txt',
 '1809893.txt',
 '1809892.txt',
 '1809899.txt',
 '1809898.txt']
def ngram(s, n=2):
    rst = list()
    for i in range(len(s)-(n-1)):
        rst.append(s[i:i+n])
    return rst
ma = Komoran()
OpenJDK 64-Bit Server VM warning: Attempt to protect stack guard pages failed.
OpenJDK 64-Bit Server VM warning: Attempt to deallocate stack guard pages failed.
V = list()
for f in kobill.fileids():
    d = kobill.open(f).read()
    d = re.sub(r'^\s|\s$', '', re.sub(r'\s+', ' ', d))
    V.extend(word_tokenize(d))
    V.extend(ngram(d, 2))
    V.extend(ngram(d, 3))
    V.extend(ma.morphs(d))
    V.extend(['/'.join(t) for t in ma.pos(d)])
len(V), len(set(V))
(134694, 15274)
from collections import Counter
cv = Counter(V)
CV = list(set(V))
D = kobill.fileids()
DTM = [[0 for j in range(len(CV))] for i in range(len(D))]
for f in D:
    i = D.index(f)
    
    d = kobill.open(f).read()
    d = re.sub(r'^\s|\s$', '', re.sub(r'\s+', ' ', d))
    
    for t in word_tokenize(d)+ngram(d, 2)+ngram(d, 3)+\
             ma.morphs(d)+['/'.join(t) for t in ma.pos(d)]:
        j = CV.index(t)
        DTM[i][j] = 1
Q = '대통령 국민 국회'

rst = list()

for q in Q.split(): # |Q|
    k = CV.index(q)
    rst.append(list())
    ######### 보틀넥
    for i, d in enumerate(DTM): # |D|
        for j, t in enumerate(d): # |V|
            if k == j and t == 1:
                rst[-1].append(i)
candi = set(rst[0])
for r in rst[1:]:
    candi = candi.intersection(set(r))
list(candi), rst
([0], [[0, 3, 4, 5, 6, 8], [0, 1, 3], [0, 1, 4, 5, 6, 7, 9]])
TDM = [[0 for j in range(len(D))] for i in range(len(CV))]
len(TDM), len(TDM[0])
(15274, 10)
for f in D:
    i = D.index(f)
    
    d = kobill.open(f).read()
    d = re.sub(r'^\s|\s$', '', re.sub(r'\s+', ' ', d))
    
    for t in word_tokenize(d)+ngram(d, 2)+ngram(d, 3)+\
             ma.morphs(d)+['/'.join(t) for t in ma.pos(d)]:
        j = CV.index(t)
        TDM[j][i] = 1
Q = '대통령 국민 국회'

rst = list()

for q in Q.split(): # |Q|
    k = CV.index(q)
    rst.append([i for i, v in enumerate(TDM[k]) if v == 1])
    
# TDM => Inverted Index
Array =! Linked List
rst = [data|p]    --->    [data|p]    ----->     [data|p]
[d1, d2, ..., d10]
[d1->d3] => [(i|다음주소),(i|다음주소),(i|다음주소)]
              0          1          2
Dictionary = dict()
Posting = list()
for f in D:
    i = D.index(f)
    
    d = kobill.open(f).read()
    d = re.sub(r'^\s|\s$', '', re.sub(r'\s+', ' ', d))
    
    for t in word_tokenize(d)+ngram(d, 2)+ngram(d, 3)+\
             ma.morphs(d)+['/'.join(t) for t in ma.pos(d)]:
        j = CV.index(t)
        
        if t not in Dictionary:
            Dictionary[t] = len(Posting)
            Posting.append((i,-1))
        else:
            pos = Dictionary[t]
            Dictionary[t] = len(Posting)
            Posting.append((i,pos))
Dictionary             Posting
단어1:주소               0:_(몇번째문서,다음주소)_
단어2:주소               1:__________
단어3:주소               2:새로운문서,다음주소-1
단어4:주소               3:새로운문서,다음주소2
단어5:3=(len(Posting))
Dictionary['국민']
44688
rst = list()
pos = Dictionary['대통령']
while pos != -1:
    i, pos = Posting[pos]
    rst.append(i)
list(set(rst))
[0, 3, 4, 5, 6, 8]
from struct import pack, unpack
pack('i', -1)
b'\xff\xff\xff\xff'
Dictionary = dict()
# Posting = list()
fp = open('posting.dat', 'wb')

for f in D:
    i = D.index(f)
    
    d = kobill.open(f).read()
    d = re.sub(r'^\s|\s$', '', re.sub(r'\s+', ' ', d))
    
    local = Counter(word_tokenize(d)+ngram(d, 2)+ngram(d, 3)+\
             ma.morphs(d)+['/'.join(t) for t in ma.pos(d)])
    # 문서1개에 대해서, 단어:빈도, 단어:빈도....    
    for k, v in local.items():
        if k not in Dictionary:
            Dictionary[k] = fp.tell()
            fp.write(pack('iii', i, v, -1))
        else:
            pos = Dictionary[k]
            Dictionary[k] = fp.tell()
            fp.write(pack('iii', i, v, pos))
fp.close()
!ls -al *.dat # 리눅스,유닉스,맥
!dir \w *.dat # 윈도우
-rw-r--r--  1 gon0121  staff  427020  4 23 10:29 posting.dat
Dictionary              File
                 fp:0
단어1:0                1,10,-1
                 fp:12
단어2:12               2,1,-1
pos = Dictionary['대통령']

fp = open('posting.dat', 'rb')
while pos != -1:
    fp.seek(pos)
    i,freq,npos = unpack('iii', fp.read(12))
    pos = npos
    print(i,freq)
fp.close()
8 2
6 3
5 1
4 1
3 6
0 6
from os import listdir

def fileids(path):
    return [path+('/' if path[-1] != '/' else '')+f
            for f in listdir(path) if re.search(r'[.]txt$', f)]

len(fileids('./news'))
51
Collection = fileids('./news')
def preprocessing(d):
    d = d.lower()
    d = re.sub(r'[a-z0-9\-_.]{3,}@[a-z0-9\-_.]{3,}(?:[.]?[a-z]{2})+', ' ', d)
    d = re.sub(r'[‘’ⓒ\'\"“”…=□*◆:]', ' ', d)
    d = re.sub(r'\s+', ' ', d)
    d = re.sub(r'^\s|\s$', '', d)
    return d

# with open(Collection[4], 'r', encoding='utf8') as fp:
#     print(preprocessing(fp.read()))
Dictionary = dict()
pfp = open('posting.dat', 'wb')

for f in Collection:
    # 문서 당 작업
    with open(f, 'r', encoding='utf8') as fp:
        text = preprocessing(fp.read())
    
    # k:v => Mapping
    localMap = dict()
    for t in word_tokenize(text):
        if t not in localMap:
            localMap[t] = 0
        localMap[t] += 1
        
    for s in sent_tokenize(text): # 메모리 오류
        for t in ma.morphs(s):
            if t not in localMap:
                localMap[t] = 0
            localMap[t] += 1
        
    for t in ngram(text):
        if t not in localMap:
            localMap[t] = 0
        localMap[t] += 1
        
    # Reducing
    i = Collection.index(f)
    for k, v in localMap.items():
        if k not in Dictionary:
            Dictionary[k] = pfp.tell()
            info = (i, v, -1)
            pfp.write(pack('iii', *info))
        else:
            info = (i, v, Dictionary[k])
            pfp.write(pack('iii', *info))
            Dictionary[k] = pfp.tell()
            
pfp.close()
with open('posting.dat', 'rb') as fp:
    pos = Dictionary['기자']
    while pos != -1:
        fp.seek(pos)
        i, freq, npos = unpack('iii', fp.read(12))
        pos = npos
        print(f'{i}. {Collection[i]}, 빈도:{freq}')
50. ./news/0011689239.txt, 빈도:2
49. ./news/0007437289.txt, 빈도:30
48. ./news/0003350025.txt, 빈도:2
47. ./news/0005700595.txt, 빈도:1
46. ./news/0012452115.txt, 빈도:14
45. ./news/0004319090.txt, 빈도:1
44. ./news/0012452262.txt, 빈도:5
25. ./news/0005162312.txt, 빈도:2
```

```python
입력 (10개 컬럼) -> 모델 -> output
입력 -> 전처리 
import seaborn as sns
from sklearn.datasets import load_wine, load_breast_cancer
data = load_wine(as_frame=True)
wine = data.frame
wine.info()
wine.boxplot(figsize=(11,6))
sns.pairplot(wine, hue='target')
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.neighbors import KNeighborsClassifier
pipe = Pipeline([('std', StandardScaler()),('knn', KNeighborsClassifier())])
from sklearn.model_selection import train_test_split
X_train,X_test,y_train,y_test = train_test_split(wine.iloc[:,:-1], wine.target)
pipe.fit(X_train,y_train)
knn = KNeighborsClassifier()
knn.fit(X_train,y_train)
knn.score(X_test,y_test)
pipe.score(X_test,y_test)
 
from sklearn import set_config
set_config(display='diagram')
from sklearn.model_selection import GridSearchCV
 
grid = GridSearchCV(KNeighborsClassifier(), param_grid={'n_neighbors':[5,6,7]})
grid.fit(X_train,y_train)
import pandas as pd
pd.DataFrame(grid.cv_results_).T
pipe2 = Pipeline([('pre',StandardScaler()), ('clf',KNeighborsClassifier())])
pipe2.get_params()
{'memory': None,
 'steps': [('pre', StandardScaler()), ('clf', KNeighborsClassifier())],
 'verbose': False,
 'pre': StandardScaler(),
 'clf': KNeighborsClassifier(),
 'pre__copy': True,
 'pre__with_mean': True,
 'pre__with_std': True,
 'clf__algorithm': 'auto',
 'clf__leaf_size': 30,
 'clf__metric': 'minkowski',
 'clf__metric_params': None,
 'clf__n_jobs': None,
 'clf__n_neighbors': 5,
 'clf__p': 2,
 'clf__weights': 'uniform'}
from sklearn.preprocessing import MinMaxScaler
from sklearn.linear_model import LogisticRegression
grid2 = GridSearchCV(pipe2, [{'clf': [KNeighborsClassifier(), LogisticRegression()]},
                             {'clf__n_neighbors':[5,6,7], 'pre':[MinMaxScaler(), StandardScaler()]}])
grid2.fit(X_train,y_train)
  GridSearchCV?i
estimator: Pipeline

 StandardScaler?

 KNeighborsClassifier?
pd.DataFrame(grid2.cv_results_).T
0	1	2	3	4	5	6	7
mean_fit_time	0.004439	0.007573	0.003459	0.004586	0.003197	0.00338	0.00399	0.00379
std_fit_time	0.000849	0.002788	0.001488	0.002076	0.000396	0.001027	0.001263	0.001596
mean_score_time	0.003868	0.002001	0.002782	0.002389	0.003391	0.003596	0.002592	0.003192
std_score_time	0.001177	0.000892	0.000395	0.000482	0.001352	0.001616	0.000488	0.000399
param_clf	KNeighborsClassifier()	LogisticRegression()	NaN	NaN	NaN	NaN	NaN	NaN
param_clf__n_neighbors	NaN	NaN	5	5	6	6	7	7
param_pre	NaN	NaN	MinMaxScaler()	StandardScaler()	MinMaxScaler()	StandardScaler()	MinMaxScaler()	StandardScaler()
params	{'clf': KNeighborsClassifier()}	{'clf': LogisticRegression()}	{'clf__n_neighbors': 5, 'pre': MinMaxScaler()}	{'clf__n_neighbors': 5, 'pre': StandardScaler()}	{'clf__n_neighbors': 6, 'pre': MinMaxScaler()}	{'clf__n_neighbors': 6, 'pre': StandardScaler()}	{'clf__n_neighbors': 7, 'pre': MinMaxScaler()}	{'clf__n_neighbors': 7, 'pre': StandardScaler()}
split0_test_score	0.925926	0.962963	1.0	0.925926	1.0	0.962963	1.0	1.0
split1_test_score	1.0	0.962963	1.0	1.0	1.0	1.0	1.0	1.0
split2_test_score	1.0	1.0	1.0	1.0	1.0	1.0	1.0	1.0
split3_test_score	1.0	1.0	1.0	1.0	1.0	1.0	1.0	1.0
split4_test_score	0.923077	0.923077	0.923077	0.923077	0.923077	0.923077	0.884615	0.884615
mean_test_score	0.969801	0.969801	0.984615	0.969801	0.984615	0.977208	0.976923	0.976923
std_test_score	0.036998	0.028638	0.030769	0.036998	0.030769	0.030632	0.046154	0.046154
rank_test_score	6	6	1	6	1	3	4	4
 
```